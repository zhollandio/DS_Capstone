{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd4887-ab1a-46b2-9450-766b64eedda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import gc\n",
    "import openpyxl\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== EMISSIONS 2003 DATASET - EXPLORATORY DATA ANALYSIS ===\\n\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel('emissions_2003.xlsx')\n",
    "    \n",
    "    print(\"Successfully loaded full dataset into memory!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    full_dataset_loaded = True\n",
    "    \n",
    "except MemoryError:\n",
    "    print(\"Memory error - switching to chunk-based processing\")\n",
    "    full_dataset_loaded = False\n",
    "except Exception as e:\n",
    "    print(f\"Error loading full dataset: {e}\")\n",
    "    print(\"Switching to chunk-based processing\")\n",
    "    full_dataset_loaded = False\n",
    "\n",
    "if full_dataset_loaded:\n",
    "\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "    print()\n",
    "    \n",
    "    print(\"Data types and non-null counts:\")\n",
    "    print(df.info())\n",
    "    print()\n",
    "    \n",
    " \n",
    "    missing_data = pd.DataFrame({\n",
    "        'Missing Count': df.isnull().sum(),\n",
    "        'Missing Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "    })\n",
    "    missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "    \n",
    "    if not missing_data.empty:\n",
    "        print(\"Missing values by column:\")\n",
    "        print(missing_data)\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    print()\n",
    "    \n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates}\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    print()\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(\"Statistical summary for numeric columns:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "        print()\n",
    "    \n",
    "    print(\"EMISSIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    emission_cols = ['ECO2', 'ECO', 'ECH4', 'EPM2.5']\n",
    "    available_emission_cols = [col for col in emission_cols if col in df.columns]\n",
    "    \n",
    "    if available_emission_cols:\n",
    "        print(\"Emission variables summary:\")\n",
    "        print(df[available_emission_cols].describe())\n",
    "        print()\n",
    "        \n",
    "        for col in available_emission_cols:\n",
    "            zero_count = (df[col] == 0).sum()\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            print(f\"{col}: {zero_count} zero values, {negative_count} negative values\")\n",
    "        print()\n",
    "    \n",
    "    print(\"GEOGRAPHICAL DISTRIBUTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'longitude' in df.columns and 'latitude' in df.columns:\n",
    "        print(\"Geographical bounds:\")\n",
    "        print(f\"Longitude range: {df['longitude'].min():.4f} to {df['longitude'].max():.4f}\")\n",
    "        print(f\"Latitude range: {df['latitude'].min():.4f} to {df['latitude'].max():.4f}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"TEMPORAL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'year' in df.columns:\n",
    "        print(\"Year distribution:\")\n",
    "        print(df['year'].value_counts().sort_index())\n",
    "        print()\n",
    "    \n",
    "    if 'doy' in df.columns:\n",
    "        print(\"Day of year statistics:\")\n",
    "        print(f\"DOY range: {df['doy'].min()} to {df['doy'].max()}\")\n",
    "        print(f\"Mean DOY: {df['doy'].mean():.1f}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"CATEGORICAL VARIABLES ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    categorical_vars = ['covertype', 'fuelcode', 'fuel_moisture_class', 'burn_source', 'burnday_source']\n",
    "    available_categorical = [col for col in categorical_vars if col in df.columns]\n",
    "    \n",
    "    for col in available_categorical:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col.upper()} distribution:\")\n",
    "            value_counts = df[col].value_counts()\n",
    "            print(value_counts.head(10))  # Show top 10 categories\n",
    "            if len(value_counts) > 10:\n",
    "                print(f\"... and {len(value_counts) - 10} more categories\")\n",
    "    print()\n",
    "    \n",
    "    print(\"FIRE CHARACTERISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    fire_vars = ['area_burned', 'prefire_fuel', 'consumed_fuel', 'cwd_frac', 'duff_frac']\n",
    "    available_fire_vars = [col for col in fire_vars if col in df.columns]\n",
    "    \n",
    "    if available_fire_vars:\n",
    "        print(\"Fire characteristics summary:\")\n",
    "        print(df[available_fire_vars].describe())\n",
    "        print()\n",
    "    \n",
    "    print(\"OUTLIER DETECTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    def detect_outliers_iqr(data, column):\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "        return len(outliers)\n",
    "    \n",
    "    key_vars = ['area_burned', 'ECO2', 'ECO', 'ECH4', 'EPM2.5']\n",
    "    available_key_vars = [col for col in key_vars if col in df.columns]\n",
    "    \n",
    "    print(\"Outlier count (using IQR method):\")\n",
    "    for col in available_key_vars:\n",
    "        outlier_count = detect_outliers_iqr(df, col)\n",
    "        outlier_pct = (outlier_count / len(df)) * 100\n",
    "        print(f\"{col}: {outlier_count} outliers ({outlier_pct:.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(\"Highly correlated variable pairs (|r| > 0.7):\")\n",
    "            for var1, var2, corr in high_corr_pairs:\n",
    "                print(f\"{var1} - {var2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"No highly correlated variable pairs found (|r| > 0.7)\")\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    \n",
    "    file_size = os.path.getsize('emissions_2003.xlsx') / (1024**2)\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    header_df = pd.read_excel('emissions_2003.xlsx', nrows=0, engine='openpyxl')\n",
    "    sample_df = pd.read_excel('emissions_2003.xlsx', nrows=1000, engine='openpyxl')\n",
    "    \n",
    "    print(f\"Number of columns: {len(header_df.columns)}\")\n",
    "    print(f\"Column names: {list(header_df.columns)}\")\n",
    "    print(f\"\\nSample data types:\")\n",
    "    print(sample_df.dtypes)\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(sample_df.head(3))\n",
    "    \n",
    "    dtype_dict = {}\n",
    "    float32_cols = ['longitude', 'latitude', 'area_burned', 'prefire_fuel', 'consumed_fuel', \n",
    "                    'ECO2', 'ECO', 'ECH4', 'EPM2.5', 'cwd_frac', 'duff_frac']\n",
    "    int16_cols = ['year', 'doy', 'grid10k', 'covertype', 'fuelcode', 'fuel_moisture_class']\n",
    "    int8_cols = ['burn_source', 'burnday_source', 'BSEV_flag']\n",
    "    \n",
    "    for col in header_df.columns:\n",
    "        if col in float32_cols:\n",
    "            dtype_dict[col] = 'float32'\n",
    "        elif col in int16_cols:\n",
    "            dtype_dict[col] = 'int16'\n",
    "        elif col in int8_cols:\n",
    "            dtype_dict[col] = 'int8'\n",
    "        else:\n",
    "            dtype_dict[col] = 'object'\n",
    "    \n",
    "    try:\n",
    "        wb = openpyxl.load_workbook('emissions_2003.xlsx', read_only=True)\n",
    "        sheet = wb.active\n",
    "        total_rows = sheet.max_row - 1 if sheet.max_row is not None else float('inf')\n",
    "        wb.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine row count, will process until end: {e}\")\n",
    "        total_rows = float('inf')\n",
    "    \n",
    "    chunk_size = 25000\n",
    "    \n",
    "    chunk_stats = []\n",
    "    missing_stats = {}\n",
    "    categorical_stats = {}\n",
    "    numeric_stats = {}\n",
    "    \n",
    "    for col in header_df.columns:\n",
    "        missing_stats[col] = 0\n",
    "        if dtype_dict.get(col) == 'object':\n",
    "            categorical_stats[col] = {}\n",
    "        elif dtype_dict.get(col) in ['float32', 'int16', 'int8']:\n",
    "            numeric_stats[col] = {\n",
    "                'count': 0, 'sum': 0, 'sum_sq': 0, 'min': float('inf'), 'max': float('-inf'),\n",
    "                'zero_count': 0, 'negative_count': 0\n",
    "            }\n",
    "    \n",
    "    print(f\"\\nProcessing Excel data in chunks of {chunk_size:,} rows...\")\n",
    "    \n",
    "    chunk_count = 0\n",
    "    total_processed = 0\n",
    "    start_row = 1\n",
    "    \n",
    "    while start_row < total_rows or total_rows == float('inf'):\n",
    "        try:\n",
    "            chunk = pd.read_excel('emissions_2003.xlsx', \n",
    "                                 skiprows=range(1, start_row + 1),\n",
    "                                 nrows=chunk_size,\n",
    "                                 engine='openpyxl',\n",
    "                                 dtype=dtype_dict)\n",
    "            \n",
    "            if len(chunk) == 0:\n",
    "                break\n",
    "                \n",
    "            chunk_count += 1\n",
    "            rows_in_chunk = len(chunk)\n",
    "            total_processed += rows_in_chunk\n",
    "            \n",
    "            chunk_stats.append({\n",
    "                'chunk': chunk_count,\n",
    "                'rows': rows_in_chunk,\n",
    "                'memory_mb': chunk.memory_usage(deep=True).sum() / (1024**2)\n",
    "            })\n",
    "            \n",
    "            for col in chunk.columns:\n",
    "                missing_stats[col] += chunk[col].isnull().sum()\n",
    "            \n",
    "            for col in chunk.columns:\n",
    "                if dtype_dict.get(col) == 'object':\n",
    "                    chunk_counts = chunk[col].value_counts()\n",
    "                    for value, count in chunk_counts.items():\n",
    "                        if pd.notna(value):\n",
    "                            if value in categorical_stats[col]:\n",
    "                                categorical_stats[col][value] += count\n",
    "                            else:\n",
    "                                categorical_stats[col][value] = count\n",
    "            \n",
    "            for col in chunk.columns:\n",
    "                if col in numeric_stats:\n",
    "                    valid_data = chunk[col].dropna()\n",
    "                    if len(valid_data) > 0:\n",
    "                        numeric_stats[col]['count'] += len(valid_data)\n",
    "                        numeric_stats[col]['sum'] += valid_data.sum()\n",
    "                        numeric_stats[col]['sum_sq'] += (valid_data ** 2).sum()\n",
    "                        numeric_stats[col]['min'] = min(numeric_stats[col]['min'], valid_data.min())\n",
    "                        numeric_stats[col]['max'] = max(numeric_stats[col]['max'], valid_data.max())\n",
    "                        numeric_stats[col]['zero_count'] += (valid_data == 0).sum()\n",
    "                        numeric_stats[col]['negative_count'] += (valid_data < 0).sum()\n",
    "            \n",
    "            print(f\"Processed chunk {chunk_count}: rows {start_row:,}-{start_row + rows_in_chunk - 1:,} ({rows_in_chunk:,} rows)\")\n",
    "            \n",
    "            start_row += chunk_size\n",
    "            \n",
    "            if total_rows == float('inf') and rows_in_chunk < chunk_size:\n",
    "                break\n",
    "            \n",
    "            del chunk\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk starting at row {start_row}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTotal rows processed: {total_processed:,}\")\n",
    "    \n",
    "    print(\"\\nDATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': list(missing_stats.keys()),\n",
    "        'Missing_Count': list(missing_stats.values()),\n",
    "        'Missing_Percentage': [(count/total_processed)*100 for count in missing_stats.values()]\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if not missing_df.empty:\n",
    "        print(\"Missing values by column:\")\n",
    "        print(missing_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    print()\n",
    "    \n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numeric_cols = [col for col, dtype in dtype_dict.items() if dtype in ['float32', 'int16', 'int8']]\n",
    "    categorical_cols = [col for col, dtype in dtype_dict.items() if dtype == 'object']\n",
    "    \n",
    "    print(f\"Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Statistical summary for numeric columns:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Column':<15} {'Count':<10} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12} {'Zeros':<8} {'Negative':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for col, stats in numeric_stats.items():\n",
    "        if stats['count'] > 0:\n",
    "            mean = stats['sum'] / stats['count']\n",
    "            variance = (stats['sum_sq'] / stats['count']) - (mean ** 2)\n",
    "            std = np.sqrt(max(0, variance))\n",
    "            \n",
    "            print(f\"{col:<15} {stats['count']:<10,} {mean:<12.2e} {std:<12.2e} {stats['min']:<12.2e} {stats['max']:<12.2e} {stats['zero_count']:<8,} {stats['negative_count']:<8,}\")\n",
    "    \n",
    "    print(\"\\nEMISSIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    emission_cols = ['ECO2', 'ECO', 'ECH4', 'EPM2.5']\n",
    "    available_emissions = [col for col in emission_cols if col in numeric_stats and numeric_stats[col]['count'] > 0]\n",
    "    \n",
    "    if available_emissions:\n",
    "        print(\"Emission Variables Analysis:\")\n",
    "        for col in available_emissions:\n",
    "            stats = numeric_stats[col]\n",
    "            if stats['count'] > 0:\n",
    "                mean = stats['sum'] / stats['count']\n",
    "                print(f\"\\n{col}:\")\n",
    "                print(f\"  Total emissions: {stats['sum']:.2e}\")\n",
    "                print(f\"  Mean emissions: {mean:.2e}\")\n",
    "                print(f\"  Zero emissions: {stats['zero_count']:,} ({(stats['zero_count']/stats['count'])*100:.1f}%)\")\n",
    "                if stats['negative_count'] > 0:\n",
    "                    print(f\"  Negative emissions: {stats['negative_count']:,} (CHECK DATA QUALITY)\")\n",
    "    \n",
    "    print(\"\\nGEOGRAPHICAL DISTRIBUTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'longitude' in numeric_stats and 'latitude' in numeric_stats:\n",
    "        lon_stats = numeric_stats['longitude']\n",
    "        lat_stats = numeric_stats['latitude']\n",
    "        if lon_stats['count'] > 0 and lat_stats['count'] > 0:\n",
    "            print(\"Geographical bounds:\")\n",
    "            print(f\"Longitude range: {lon_stats['min']:.4f} to {lon_stats['max']:.4f}\")\n",
    "            print(f\"Latitude range: {lat_stats['min']:.4f} to {lat_stats['max']:.4f}\")\n",
    "    \n",
    "    print(\"\\nTEMPORAL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'doy' in numeric_stats:\n",
    "        doy_stats = numeric_stats['doy']\n",
    "        if doy_stats['count'] > 0:\n",
    "            mean_doy = doy_stats['sum'] / doy_stats['count']\n",
    "            print(\"Day of year statistics:\")\n",
    "            print(f\"DOY range: {doy_stats['min']} to {doy_stats['max']}\")\n",
    "            print(f\"Mean DOY: {mean_doy:.1f}\")\n",
    "    \n",
    "    print(\"\\nCATEGORICAL VARIABLES ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    categorical_vars = ['covertype', 'fuelcode', 'fuel_moisture_class', 'burn_source', 'burnday_source']\n",
    "    available_categorical = [col for col in categorical_vars if col in categorical_stats]\n",
    "    \n",
    "    for col in available_categorical:\n",
    "        if categorical_stats[col]:\n",
    "            sorted_counts = sorted(categorical_stats[col].items(), key=lambda x: x[1], reverse=True)\n",
    "            total_unique = len(sorted_counts)\n",
    "            print(f\"\\n{col.upper()} distribution:\")\n",
    "            print(f\"  Unique values: {total_unique}\")\n",
    "            print(\"  Top 10 categories:\")\n",
    "            for i, (value, count) in enumerate(sorted_counts[:10]):\n",
    "                percentage = (count/total_processed)*100\n",
    "                print(f\"    {value}: {count:,} ({percentage:.2f}%)\")\n",
    "            if total_unique > 10:\n",
    "                print(f\"    ... and {total_unique-10} more categories\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb43eb3-d59b-477c-bdbf-963c8c697867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
