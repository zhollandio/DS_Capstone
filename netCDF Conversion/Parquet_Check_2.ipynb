{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18991634-3021-43ab-ab44-9488ea9b0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "folder_path = r\"C:\\Users\\zscho\\OneDrive\\Documents\\Capstone\\Weather\"\n",
    "year = 2015\n",
    "expected_days = 366 if year % 4 == 0 else 365\n",
    "\n",
    "parquet_files = [f for f in os.listdir(folder_path) if f.endswith('.parquet') and f\"{year}.parquet\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd39cab-fdc5-4815-a714-8bf6c19c5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage and gaps\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    if 'day' in df.columns:\n",
    "        unique_days = df['day'].nunique()\n",
    "        if unique_days != expected_days:\n",
    "            print(f\"{file}: Has {unique_days} days, expected {expected_days}\")\n",
    "        \n",
    "        day_range = df['day'].max() - df['day'].min() + 1\n",
    "        if day_range != expected_days:\n",
    "            print(f\"{file}: Day range spans {day_range}, expected {expected_days}\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70f472-32a4-4163-8935-77813377a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate validation\n",
    "coord_ranges = {}\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    \n",
    "    lat_col = [col for col in df.columns if 'lat' in col.lower()]\n",
    "    lon_col = [col for col in df.columns if 'lon' in col.lower()]\n",
    "    \n",
    "    if lat_col:\n",
    "        lat_min, lat_max = df[lat_col[0]].min(), df[lat_col[0]].max()\n",
    "        if lat_min < -90 or lat_max > 90:\n",
    "            print(f\"{file}: Invalid latitude range {lat_min} to {lat_max}\")\n",
    "        coord_ranges[file] = {'lat_range': (lat_min, lat_max)}\n",
    "    \n",
    "    if lon_col:\n",
    "        lon_min, lon_max = df[lon_col[0]].min(), df[lon_col[0]].max()\n",
    "        if lon_min < -180 or lon_max > 180:\n",
    "            print(f\"{file}: Invalid longitude range {lon_min} to {lon_max}\")\n",
    "        if file in coord_ranges:\n",
    "            coord_ranges[file]['lon_range'] = (lon_min, lon_max)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbd4c7-8234-4e62-b124-15dd9a0f3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value validation\n",
    "weather_limits = {\n",
    "    'tmmn': (-80, 60),  # min temp celsius\n",
    "    'tmmx': (-60, 80),  # max temp celsius  \n",
    "    'pr': (0, 1000),    # precipitation mm\n",
    "    'rmin': (0, 100),   # min humidity %\n",
    "    'rmax': (0, 100),   # max humidity %\n",
    "    'vs': (0, 200),     # wind speed m/s\n",
    "    'srad': (0, 50),    # solar radiation MJ/m2/day\n",
    "    'vpd': (0, 10000)   # vapor pressure deficit Pa\n",
    "}\n",
    "\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    \n",
    "    for var, (min_val, max_val) in weather_limits.items():\n",
    "        if var in df.columns:\n",
    "            out_of_range = ((df[var] < min_val) | (df[var] > max_val)).sum()\n",
    "            if out_of_range > 0:\n",
    "                actual_min, actual_max = df[var].min(), df[var].max()\n",
    "                print(f\"{file} - {var}: {out_of_range} values outside range ({actual_min:.2f} to {actual_max:.2f})\")\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c60de2-3442-411e-b508-232de9046e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp relationship validation\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    \n",
    "    if 'tmmn' in df.columns and 'tmmx' in df.columns:\n",
    "        invalid_temp = (df['tmmx'] <= df['tmmn']).sum()\n",
    "        if invalid_temp > 0:\n",
    "            print(f\"{file}: {invalid_temp} records where max temp <= min temp\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacd3a6-690d-43f1-b93b-9e5034de2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values (no variance)\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].nunique() == 1 and df[col].notna().sum() > 0:\n",
    "            print(f\"{file} - {col}: All values are constant ({df[col].iloc[0]})\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343a8cb-7641-429e-8130-0694c362b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spikes in time series\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if 'day' in df.columns:\n",
    "        df_sorted = df.sort_values('day')\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col != 'day':\n",
    "                values = df_sorted[col].dropna()\n",
    "                if len(values) > 1:\n",
    "                    diff = values.diff().abs()\n",
    "                    mean_diff = diff.mean()\n",
    "                    spike_threshold = mean_diff * 10\n",
    "                    spikes = (diff > spike_threshold).sum()\n",
    "                    if spikes > 0:\n",
    "                        print(f\"{file} - {col}: {spikes} potential spikes detected\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51c7cf-0d43-44a9-ae42-3d0ab0ba5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-file coordinate consistency\n",
    "if coord_ranges:\n",
    "    lat_ranges = [ranges.get('lat_range') for ranges in coord_ranges.values() if 'lat_range' in ranges]\n",
    "    lon_ranges = [ranges.get('lon_range') for ranges in coord_ranges.values() if 'lon_range' in ranges]\n",
    "    \n",
    "    if lat_ranges and len(set(lat_ranges)) > 1:\n",
    "        print(\"Inconsistent latitude ranges across files:\")\n",
    "        for file, ranges in coord_ranges.items():\n",
    "            if 'lat_range' in ranges:\n",
    "                print(f\"  {file}: {ranges['lat_range']}\")\n",
    "    \n",
    "    if lon_ranges and len(set(lon_ranges)) > 1:\n",
    "        print(\"Inconsistent longitude ranges across files:\")\n",
    "        for file, ranges in coord_ranges.items():\n",
    "            if 'lon_range' in ranges:\n",
    "                print(f\"  {file}: {ranges['lon_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2d309-6a4e-4064-80d9-44985583c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file size check\n",
    "file_sizes = {}\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    file_sizes[file] = size_mb\n",
    "\n",
    "mean_size = np.mean(list(file_sizes.values()))\n",
    "std_size = np.std(list(file_sizes.values()))\n",
    "\n",
    "for file, size in file_sizes.items():\n",
    "    if abs(size - mean_size) > 3 * std_size:\n",
    "        print(f\"{file}: Unusual file size {size:.1f}MB (mean: {mean_size:.1f}MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b538c72-fc3d-4afc-a3af-c4b1a516ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccount \n",
    "record_counts = {}\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    record_counts[file] = len(df)\n",
    "    del df\n",
    "\n",
    "unique_counts = set(record_counts.values())\n",
    "if len(unique_counts) > 1:\n",
    "    print(\"Inconsistent record counts across files:\")\n",
    "    for file, count in record_counts.items():\n",
    "        print(f\"  {file}: {count:,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa55a1f-f5d8-4e0e-a249-f84f93deede7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8b7a3-3e30-43ad-bd77-490d9344279a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3a04a-0d7d-458c-b277-c9c9c1f38352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
