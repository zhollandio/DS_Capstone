{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94e466-2f0c-4ea9-908d-3c385cfe29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = r\"C:\\Users\\zscho\\OneDrive\\Documents\\Capstone\\Weather\"\n",
    "year = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42579c-bf96-4da6-b19c-4b0f1016ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of parquet files for x year\n",
    "parquet_files = [f for f in os.listdir(folder_path) if f.endswith('.parquet') and f\"{year}.parquet\" in f]\n",
    "print(f\"Found {len(parquet_files)} files for {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5cac5-bed9-444f-82a2-a54cabd2aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info check\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    print(f\"{file}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856673d-e45d-4bad-89c1-c42c139af4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing \n",
    "missing_summary = {}\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    missing_pct = (missing_count / (df.shape[0] * df.shape[1])) * 100\n",
    "    missing_summary[file] = {'count': missing_count, 'percent': missing_pct}\n",
    "    del df\n",
    "\n",
    "for file, stats in missing_summary.items():\n",
    "    if stats['count'] > 0:\n",
    "        print(f\"{file}: {stats['count']} missing ({stats['percent']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63938a8-5b18-40e4-ab7c-ea9c8624c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite values (yes we did find one)\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    inf_count = np.isinf(df[numeric_cols]).sum().sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"{file}: {inf_count} infinite values\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48fb39-d417-4b21-82f1-e4a739422e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        print(f\"{file}: {dup_count} duplicate rows\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93e2be-be10-49d8-aabe-e9d3cdbd02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        values = df[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            min_val, max_val = values.min(), values.max()\n",
    "            q1, q3 = values.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            outlier_threshold_low = q1 - 3 * iqr\n",
    "            outlier_threshold_high = q3 + 3 * iqr\n",
    "            \n",
    "            extreme_outliers = ((values < outlier_threshold_low) | (values > outlier_threshold_high)).sum()\n",
    "            if extreme_outliers > 0:\n",
    "                print(f\"{file} - {col}: {extreme_outliers} extreme outliers (range: {min_val:.2f} to {max_val:.2f})\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41abead-0d35-4d6f-b1b8-265d89d53cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency check\n",
    "dtypes_summary = {}\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder_path, file))\n",
    "    dtypes_summary[file] = df.dtypes.to_dict()\n",
    "    del df\n",
    "\n",
    "common_cols = set.intersection(*[set(dtypes.keys()) for dtypes in dtypes_summary.values()])\n",
    "for col in common_cols:\n",
    "    unique_types = set([dtypes_summary[file][col] for file in parquet_files])\n",
    "    if len(unique_types) > 1:\n",
    "        print(f\"Column {col} has inconsistent types across files: {unique_types}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
