{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bd2f0-4ac9-4c14-9c52-7f0f85c82630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "def convert_nc_to_parquet(nc_filepath, output_filepath=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Convert NetCDF file to Parquet format\n",
    "    \n",
    "    Parameters:\n",
    "    nc_filepath (str): Path to the .nc file\n",
    "    output_filepath (str): Path for output .parquet file (optional)\n",
    "    chunk_size (int): Number of rows to process at once for large files (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading NetCDF file: {nc_filepath}\")\n",
    "    ds = xr.open_dataset(nc_filepath)\n",
    "    \n",
    "    print(ds)\n",
    "\n",
    "    print(\"\\nConverting to DataFrame...\")\n",
    "    df = ds.to_dataframe()\n",
    "    \n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "    print(\"\\nDataFrame info:\")\n",
    "    df.info()\n",
    "    \n",
    "    if output_filepath is None:\n",
    "        nc_path = Path(nc_filepath)\n",
    "        output_filepath = nc_path.with_suffix('.parquet')\n",
    "    \n",
    "    print(f\"\\nSaving to Parquet: {output_filepath}\")\n",
    "    \n",
    "    if chunk_size is not None:\n",
    "        print(f\"Processing in chunks of {chunk_size:,} rows...\")\n",
    "        df_reset = df.reset_index()\n",
    "    \n",
    "        for i in range(0, len(df_reset), chunk_size):\n",
    "            chunk = df_reset.iloc[i:i+chunk_size]\n",
    "            chunk_filepath = str(output_filepath).replace('.parquet', f'_chunk_{i//chunk_size}.parquet')\n",
    "            chunk.to_parquet(chunk_filepath, index=False)\n",
    "            print(f\"Saved chunk {i//chunk_size + 1}: {chunk_filepath}\")\n",
    "    else:\n",
    "        df.to_parquet(output_filepath)\n",
    "        print(f\"Successfully saved to: {output_filepath}\")\n",
    "    \n",
    "    nc_size = Path(nc_filepath).stat().st_size / 1024**2\n",
    "    if chunk_size is None:\n",
    "        parquet_size = Path(output_filepath).stat().st_size / 1024**2\n",
    "        print(f\"\\nFile size comparison:\")\n",
    "        print(f\"Original .nc file: {nc_size:.2f} MB\")\n",
    "        print(f\"Parquet file: {parquet_size:.2f} MB\")\n",
    "        print(f\"Compression ratio: {nc_size/parquet_size:.2f}x\")\n",
    "    \n",
    "    return output_filepath\n",
    "\n",
    "nc_file = \"Weather_data_bi_2003.nc\"\n",
    "\n",
    "try:\n",
    "    parquet_file = convert_nc_to_parquet(nc_file)\n",
    "    print(f\"\\nConversion completed successfully!\")\n",
    "    \n",
    "except MemoryError:\n",
    "    print(\"Memory error - trying chunked approach...\")\n",
    "    parquet_file = convert_nc_to_parquet(nc_file, chunk_size=1000000)  # 1M rows per chunk\n",
    "\n",
    "\n",
    "try:\n",
    "    df_parquet = pd.read_parquet(parquet_file)\n",
    "    print(f\"Parquet file shape: {df_parquet.shape}\")\n",
    "    print(f\"Columns: {list(df_parquet.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_parquet.head())\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(df_parquet.dtypes)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading parquet file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d762f-9021-4ec3-8217-e2793f7c9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93d122-2fca-43bc-afda-798d1806be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nc_to_parquet(nc_filepath, output_filepath=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Convert NetCDF file to Parquet format\n",
    "    \n",
    "    Parameters:\n",
    "    nc_filepath (str): Path to the .nc file\n",
    "    output_filepath (str): Path for output .parquet file (optional)\n",
    "    chunk_size (int): Number of rows to process at once for large files (optional)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading NetCDF file: {nc_filepath}\")\n",
    "        ds = xr.open_dataset(nc_filepath)\n",
    "        \n",
    "        print(f\"Dataset structure for {Path(nc_filepath).name}:\")\n",
    "        print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Variables: {list(ds.data_vars)}\")\n",
    "        \n",
    "        print(\"Converting to DataFrame\")\n",
    "        df = ds.to_dataframe()\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "        \n",
    "        if output_filepath is None:\n",
    "            nc_path = Path(nc_filepath)\n",
    "            output_filepath = nc_path.with_suffix('.parquet')\n",
    "        \n",
    "        print(f\"Saving to Parquet: {output_filepath}\")\n",
    "        if chunk_size is not None:\n",
    "            print(f\"Processing in chunks of {chunk_size:,} rows...\")\n",
    "            df_reset = df.reset_index()\n",
    "            \n",
    "            for i in range(0, len(df_reset), chunk_size):\n",
    "                chunk = df_reset.iloc[i:i+chunk_size]\n",
    "                chunk_filepath = str(output_filepath).replace('.parquet', f'_chunk_{i//chunk_size}.parquet')\n",
    "                chunk.to_parquet(chunk_filepath, index=False)\n",
    "                print(f\"Saved chunk {i//chunk_size + 1}: {chunk_filepath}\")\n",
    "                \n",
    "                del chunk\n",
    "                gc.collect()\n",
    "        else:\n",
    "            df.to_parquet(output_filepath)\n",
    "            print(f\"Successfully saved to: {output_filepath}\")\n",
    "        \n",
    "        nc_size = Path(nc_filepath).stat().st_size / 1024**2\n",
    "        if chunk_size is None:\n",
    "            parquet_size = Path(output_filepath).stat().st_size / 1024**2\n",
    "            print(f\"File size comparison:\")\n",
    "            print(f\"Original .nc file: {nc_size:.2f} MB\")\n",
    "            print(f\"Parquet file: {parquet_size:.2f} MB\")\n",
    "            print(f\"Compression ratio: {nc_size/parquet_size:.2f}x\")\n",
    "        \n",
    "        del df\n",
    "        ds.close()\n",
    "        del ds\n",
    "        gc.collect()\n",
    "        \n",
    "        return output_filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {nc_filepath}: {str(e)}\")\n",
    "        try:\n",
    "            if 'ds' in locals():\n",
    "                ds.close()\n",
    "            if 'df' in locals():\n",
    "                del df\n",
    "            if 'ds' in locals():\n",
    "                del ds\n",
    "            gc.collect()\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "def process_all_weather_files(year=2015, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Process all weather data files for a given year\n",
    "    \n",
    "    Parameters:\n",
    "    year (int): Year to process (default 2003)\n",
    "    chunk_size (int): Chunk size for large files (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    weather_vars = [\n",
    "        'bi', 'etr', 'fm100', 'fm1000', 'metdata_elevationdata', \n",
    "        'pet', 'pr', 'rmax', 'rmin', 'sph', 'srad', 'th', \n",
    "        'tmmn', 'tmmx', 'vpd', 'vs'\n",
    "    ]\n",
    "    \n",
    "    successful_conversions = []\n",
    "    failed_conversions = []\n",
    "    \n",
    "    print(f\"Starting batch conversion for {len(weather_vars)} files from year {year}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, var in enumerate(weather_vars, 1):\n",
    "        print(f\"\\n[{i}/{len(weather_vars)}] Processing variable: {var}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        nc_file = f\"Weather_data_{var}_{year}.nc\"\n",
    "        if not Path(nc_file).exists():\n",
    "            print(f\"File not found: {nc_file}\")\n",
    "            failed_conversions.append((var, \"File not found\"))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            parquet_file = convert_nc_to_parquet(nc_file, chunk_size=chunk_size)\n",
    "            \n",
    "            if parquet_file:\n",
    "                print(f\"Successfully converted {var}\")\n",
    "                successful_conversions.append(var)\n",
    "            else:\n",
    "                print(f\"Failed to convert {var}\")\n",
    "                failed_conversions.append((var, \"Conversion returned None\"))\n",
    "                \n",
    "        except MemoryError:\n",
    "            print(f\"Memory error for {var} - trying chunked approach...\")\n",
    "            try:\n",
    "                parquet_file = convert_nc_to_parquet(nc_file, chunk_size=1000000)  # 1M rows per chunk\n",
    "                if parquet_file:\n",
    "                    print(f\"Successfully converted {var} (chunked)\")\n",
    "                    successful_conversions.append(var)\n",
    "                else:\n",
    "                    print(f\"Failed to convert {var} even with chunking\")\n",
    "                    failed_conversions.append((var, \"Failed even with chunking\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {var}: {str(e)}\")\n",
    "                failed_conversions.append((var, str(e)))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {var}: {str(e)}\")\n",
    "            failed_conversions.append((var, str(e)))\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            import psutil\n",
    "            memory_percent = psutil.virtual_memory().percent\n",
    "            print(f\"Current memory usage: {memory_percent:.1f}%\")\n",
    "        except ImportError:\n",
    "            pass\n",
    "    \n",
    "    print(f\"Successfully converted: {len(successful_conversions)}/{len(weather_vars)} files\")\n",
    "    \n",
    "    if successful_conversions:\n",
    "        print(f\"\\nSuccessful conversions:\")\n",
    "        for var in successful_conversions:\n",
    "            print(f\"   - {var}\")\n",
    "    \n",
    "    if failed_conversions:\n",
    "        print(f\"\\nFailed conversions:\")\n",
    "        for var, error in failed_conversions:\n",
    "            print(f\"   - {var}: {error}\")\n",
    "    \n",
    "    return successful_conversions, failed_conversions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    successful, failed = process_all_weather_files(year=2015)\n",
    "    \n",
    "    if successful:\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading sample Parquet file:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        sample_var = successful[0]\n",
    "        sample_file = f\"Weather_data_{sample_var}_2003.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df_sample = pd.read_parquet(sample_file)\n",
    "            print(f\"Sample file ({sample_var}) shape: {df_sample.shape}\")\n",
    "            print(f\"Columns: {list(df_sample.columns)}\")\n",
    "            print(\"\\nFirst few rows:\")\n",
    "            print(df_sample.head())\n",
    "            \n",
    "            del df_sample\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample parquet file: {e}\")\n",
    "    \n",
    "    print(\"\\nBatch processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af1dc5-5abc-4c17-beb3-9f83af91e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youll have to change this to your local \n",
    "folder_path = r\"C:\\Users\\zscho\\OneDrive\\Documents\\Capstone\\Weather\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.startswith(\"Weather_data_\") and filename.endswith(\".parquet\"):\n",
    "        new_name = filename.replace(\"Weather_data_\", \"\", 1)\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {filename} â†’ {new_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c630fa3-dc69-43f5-a8af-69304a663135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
