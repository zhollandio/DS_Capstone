{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initial Fire Identification (Attempt 1) - Gathering info and attempting indentification"
      ],
      "metadata": {
        "id": "rF2rcY6Tvl5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "def check_table_schema():\n",
        "    client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "    # table schema\n",
        "    table_ref = client.dataset(\"emission_db\").table(\"emission_2004\")\n",
        "    table = client.get_table(table_ref)\n",
        "\n",
        "    print(\"=== Table Schema ===\")\n",
        "    print(f\"Table: {table.full_table_id}\")\n",
        "    print(f\"Number of rows: {table.num_rows:,}\")\n",
        "    print(f\"Number of columns: {len(table.schema)}\")\n",
        "    print()\n",
        "\n",
        "    print(\"=== Column Names and Types ===\")\n",
        "    for field in table.schema:\n",
        "        print(f\"{field.name}: {field.field_type}\")\n",
        "    print()\n",
        "\n",
        "    # sample data\n",
        "    query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM `code-for-planet.emission_db.emission_2004`\n",
        "    LIMIT 3\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== Sample Data (first 3 rows) ===\")\n",
        "    df = client.query(query).to_dataframe()\n",
        "    print(df.head())\n",
        "    print()\n",
        "\n",
        "    # check key columns we need\n",
        "    required_cols = ['longitude', 'latitude', 'fire_date', 'year', 'doy']\n",
        "    available_cols = [field.name for field in table.schema]\n",
        "\n",
        "    print(\"=== Required Columns Status ===\")\n",
        "    for col in required_cols:\n",
        "        if col in available_cols:\n",
        "            print(f\"{col}: Available\")\n",
        "        else:\n",
        "            print(f\"✗ {col}: Missing\")\n",
        "            # anything sim\n",
        "            similar = [c for c in available_cols if col.lower() in c.lower() or c.lower() in col.lower()]\n",
        "            if similar:\n",
        "                print(f\"  Similar columns: {similar}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_table_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZfW9_hB58HM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752462669793,
          "user_tz": 240,
          "elapsed": 5012,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "3a14c4b2-e78c-4ef3-9513-f51c2589a336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Table Schema ===\n",
            "Table: code-for-planet:emission_db.emission_2004\n",
            "Number of rows: 239,663\n",
            "Number of columns: 37\n",
            "\n",
            "=== Column Names and Types ===\n",
            "id: INTEGER\n",
            "year: INTEGER\n",
            "doy: INTEGER\n",
            "longitude: FLOAT\n",
            "latitude: FLOAT\n",
            "grid10k: INTEGER\n",
            "covertype: INTEGER\n",
            "fuelcode: INTEGER\n",
            "area_burned: FLOAT\n",
            "prefire_fuel: FLOAT\n",
            "consumed_fuel: FLOAT\n",
            "ECO2: FLOAT\n",
            "ECO: FLOAT\n",
            "ECH4: FLOAT\n",
            "EPM2_5: FLOAT\n",
            "cwd_frac: FLOAT\n",
            "duff_frac: FLOAT\n",
            "fuel_moisture_class: INTEGER\n",
            "burn_source: INTEGER\n",
            "burnday_source: INTEGER\n",
            "BSEV: INTEGER\n",
            "BSEV_flag: INTEGER\n",
            "fire_date: DATE\n",
            "bi_value: FLOAT\n",
            "fm100_value: FLOAT\n",
            "pet_value: FLOAT\n",
            "fm1000_value: FLOAT\n",
            "pr_value: FLOAT\n",
            "rmax_value: FLOAT\n",
            "rmin_value: FLOAT\n",
            "sph_value: FLOAT\n",
            "srad_value: FLOAT\n",
            "tmmn_value: FLOAT\n",
            "th_value: FLOAT\n",
            "tmmx_value: FLOAT\n",
            "vpd_value: FLOAT\n",
            "vs_value: FLOAT\n",
            "\n",
            "=== Sample Data (first 3 rows) ===\n",
            "      id  year  doy  longitude  latitude  grid10k  covertype  fuelcode  \\\n",
            "0  91593  2004    0  -113.3554   36.5108    62778          1         1   \n",
            "1  91594  2004    0  -113.3559   36.5130    62778          2         2   \n",
            "2  91725  2004    0  -114.0151   36.6305    64155          2         2   \n",
            "\n",
            "   area_burned  prefire_fuel  ...  pr_value  rmax_value  rmin_value  \\\n",
            "0      62500.0     18.045846  ...       0.0   86.400002        41.0   \n",
            "1      62500.0    418.304952  ...       0.0   86.400002        41.0   \n",
            "2      62500.0   1202.458608  ...       0.0   53.600002        31.5   \n",
            "\n",
            "   sph_value  srad_value  tmmn_value  th_value  tmmx_value  vpd_value  \\\n",
            "0    0.00284  103.400002  268.899994     359.0  277.899994       0.26   \n",
            "1    0.00284  103.400002  268.899994     359.0  277.899994       0.26   \n",
            "2    0.00244  101.400002  274.899994     359.0  280.299988       0.52   \n",
            "\n",
            "   vs_value  \n",
            "0       2.9  \n",
            "1       2.9  \n",
            "2       2.4  \n",
            "\n",
            "[3 rows x 37 columns]\n",
            "\n",
            "=== Required Columns Status ===\n",
            "✓ longitude: Available\n",
            "✓ latitude: Available\n",
            "✓ fire_date: Available\n",
            "✓ year: Available\n",
            "✓ doy: Available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class FireEventClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371  # earths radius in kms\n",
        "\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "        return R * c\n",
        "\n",
        "    def process_year_chunked(self, year, spatial_eps=0.02, temporal_days=7, min_samples=3, chunk_size=10000):\n",
        "        \"\"\"\n",
        "        Process fire events for a single year in chunks to manage memory\n",
        "\n",
        "        Parameters:\n",
        "        - year: Year to process\n",
        "        - spatial_eps: Spatial epsilon in degrees (~2.2km at equator)\n",
        "        - temporal_days: Temporal window for grouping (days)\n",
        "        - min_samples: Minimum samples for DBSCAN cluster\n",
        "        - chunk_size: Number of rows to process at once\n",
        "        \"\"\"\n",
        "\n",
        "        # build table name based on year\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        # total count & date range\n",
        "        count_query = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_rows,\n",
        "            MIN(fire_date) as min_date,\n",
        "            MAX(fire_date) as max_date\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Getting data overview for {year}...\")\n",
        "        overview = self.client.query(count_query).to_dataframe()\n",
        "        total_rows = overview['total_rows'].iloc[0]\n",
        "        min_date = pd.Timestamp(overview['min_date'].iloc[0])\n",
        "        max_date = pd.Timestamp(overview['max_date'].iloc[0])\n",
        "\n",
        "        print(f\"Total valid rows: {total_rows:,}\")\n",
        "        print(f\"Date range: {min_date.date()} to {max_date.date()}\")\n",
        "\n",
        "        if total_rows == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # process temporal chunks (month by month)\n",
        "        all_fire_events = []\n",
        "        event_id_offset = 0\n",
        "\n",
        "        # date ranges for processing\n",
        "        current_date = min_date\n",
        "\n",
        "        while current_date <= max_date:\n",
        "            # one month at a time\n",
        "            next_date = current_date + pd.DateOffset(months=1)\n",
        "            if next_date > max_date:\n",
        "                next_date = max_date + pd.DateOffset(days=1)\n",
        "\n",
        "            print(f\"\\nProcessing period: {current_date.date()} to {(next_date - pd.DateOffset(days=1)).date()}\")\n",
        "\n",
        "            # query for this time period\n",
        "            chunk_query = f\"\"\"\n",
        "            SELECT\n",
        "                id,\n",
        "                year,\n",
        "                doy,\n",
        "                longitude,\n",
        "                latitude,\n",
        "                fire_date,\n",
        "                grid10k,\n",
        "                covertype,\n",
        "                fuelcode,\n",
        "                area_burned,\n",
        "                consumed_fuel,\n",
        "                ECO2,\n",
        "                burn_source,\n",
        "                burnday_source\n",
        "            FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "            WHERE longitude IS NOT NULL\n",
        "            AND latitude IS NOT NULL\n",
        "            AND fire_date IS NOT NULL\n",
        "            AND fire_date >= '{current_date.date()}'\n",
        "            AND fire_date < '{next_date.date()}'\n",
        "            ORDER BY fire_date, longitude, latitude\n",
        "            \"\"\"\n",
        "\n",
        "            chunk_df = self.client.query(chunk_query).to_dataframe()\n",
        "\n",
        "            if len(chunk_df) == 0:\n",
        "                current_date = next_date\n",
        "                continue\n",
        "\n",
        "            print(f\"  Found {len(chunk_df)} records\")\n",
        "\n",
        "            # Convert fire_date to datetime\n",
        "            chunk_df['fire_date'] = pd.to_datetime(chunk_df['fire_date'])\n",
        "            chunk_df['day_of_year'] = chunk_df['fire_date'].dt.dayofyear\n",
        "\n",
        "            # chunk\n",
        "            try:\n",
        "                # clustering method\n",
        "                clustering_method = \"fixed\"\n",
        "\n",
        "                if clustering_method == \"fixed\":\n",
        "                    spatial_clusters = self._fixed_spatial_clustering(chunk_df, spatial_eps, min_samples)\n",
        "\n",
        "                chunk_fire_events = self._temporal_refinement(spatial_clusters, temporal_days)\n",
        "\n",
        "                # fire event IDs to be globally unique\n",
        "                if len(chunk_fire_events) > 0:\n",
        "                    max_event_id = chunk_fire_events['fire_event_id'].max()\n",
        "                    chunk_fire_events['fire_event_id'] += event_id_offset\n",
        "                    event_id_offset += max_event_id + 1\n",
        "\n",
        "                    all_fire_events.append(chunk_fire_events)\n",
        "                    print(f\"  Found {len(chunk_fire_events['fire_event_id'].unique())} fire events in this chunk\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing chunk: {e}\")\n",
        "                continue\n",
        "\n",
        "            current_date = next_date\n",
        "\n",
        "        # combine all chunks\n",
        "        if all_fire_events:\n",
        "            combined_events = pd.concat(all_fire_events, ignore_index=True)\n",
        "            print(f\"\\nTotal fire events found: {len(combined_events['fire_event_id'].unique())}\")\n",
        "            return combined_events\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "        \"\"\"\n",
        "        Process fire events for a single year\n",
        "\n",
        "        Parameters:\n",
        "        - year: Year to process\n",
        "        - spatial_eps: Spatial epsilon in degrees (~2.2km at equator)\n",
        "        - temporal_days: Temporal window for grouping (days)\n",
        "        - min_samples: Minimum samples for DBSCAN cluster\n",
        "        \"\"\"\n",
        "\n",
        "        # build table name based on year\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        # query data for the year\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id,\n",
        "            year,\n",
        "            doy,\n",
        "            longitude,\n",
        "            latitude,\n",
        "            fire_date,\n",
        "            grid10k,\n",
        "            covertype,\n",
        "            fuelcode,\n",
        "            area_burned,\n",
        "            consumed_fuel,\n",
        "            ECO2,\n",
        "            burn_source,\n",
        "            burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Processing year {year} from table {table_name}...\")\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"No data found for year {year}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"Found {len(df)} records for year {year}\")\n",
        "\n",
        "        # convert fire_date to datetime\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        # add temporal for clustering\n",
        "        df['day_of_year'] = df['fire_date'].dt.dayofyear\n",
        "\n",
        "        # clustering method\n",
        "        clustering_method = \"adaptive\"\n",
        "\n",
        "        if clustering_method == \"adaptive\":\n",
        "            spatial_clusters = self.adaptive_spatial_clustering(df, min_samples)\n",
        "        elif clustering_method == \"density\":\n",
        "            spatial_clusters = self.density_based_clustering(df, min_samples)\n",
        "        else:\n",
        "            spatial_clusters = self._fixed_spatial_clustering(df, spatial_eps, min_samples)\n",
        "\n",
        "        fire_events = self._temporal_refinement(spatial_clusters, temporal_days)\n",
        "\n",
        "        return fire_events\n",
        "\n",
        "    def _fixed_spatial_clustering(self, df, eps, min_samples):\n",
        "        \"\"\"Apply DBSCAN clustering with fixed distance threshold\"\"\"\n",
        "\n",
        "        # prep spatial features\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "\n",
        "        # DBSCAN\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='haversine')\n",
        "\n",
        "        # Convert to radians for haversine distance\n",
        "        coords_rad = np.radians(coords)\n",
        "        spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "\n",
        "        df['spatial_cluster'] = spatial_labels\n",
        "\n",
        "        print(f\"    Fixed spatial clustering found {len(set(spatial_labels)) - (1 if -1 in spatial_labels else 0)} clusters\")\n",
        "        print(f\"    Noise points: {sum(spatial_labels == -1)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def adaptive_spatial_clustering(self, df, min_samples=3):\n",
        "        \"\"\"\n",
        "        Adaptive clustering that handles fires of different sizes\n",
        "        Uses multiple distance thresholds and merges appropriately\n",
        "        \"\"\"\n",
        "        from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "        # radians for haversine distance\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        # Try multiple distance thresholds\n",
        "        distance_thresholds = [0.01, 0.02, 0.05, 0.1]  # 1km, 2km, 5km, 10km\n",
        "\n",
        "        best_clustering = None\n",
        "        best_score = -1\n",
        "\n",
        "        for eps in distance_thresholds:\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='haversine')\n",
        "            labels = dbscan.fit_predict(coords_rad)\n",
        "\n",
        "            # Calc silhouette score (higher is better)\n",
        "            if len(set(labels)) > 1:\n",
        "                from sklearn.metrics import silhouette_score\n",
        "                # non-noise points for scoring\n",
        "                mask = labels != -1\n",
        "                if np.sum(mask) > min_samples:\n",
        "                    score = silhouette_score(coords_rad[mask], labels[mask], metric='haversine')\n",
        "                    print(f\"Distance threshold {eps*111:.1f}km: {len(set(labels))-1} clusters, score: {score:.3f}\")\n",
        "\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_clustering = labels\n",
        "\n",
        "        if best_clustering is not None:\n",
        "            df['spatial_cluster'] = best_clustering\n",
        "            print(f\"Best clustering selected with score: {best_score:.3f}\")\n",
        "        else:\n",
        "            # fallback to default DBSCAN\n",
        "            dbscan = DBSCAN(eps=0.02, min_samples=min_samples, metric='haversine')\n",
        "            df['spatial_cluster'] = dbscan.fit_predict(coords_rad)\n",
        "            print(\"Using fallback clustering\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _temporal_refinement(self, df, temporal_days):\n",
        "        \"\"\"Refine spatial clusters by temporal proximity\"\"\"\n",
        "\n",
        "        fire_events = []\n",
        "        event_id = 0\n",
        "\n",
        "        # process each spatial cluster\n",
        "        for spatial_cluster in df['spatial_cluster'].unique():\n",
        "            if spatial_cluster == -1:  # Skip noise points\n",
        "                continue\n",
        "\n",
        "            cluster_data = df[df['spatial_cluster'] == spatial_cluster].copy()\n",
        "            cluster_data = cluster_data.sort_values('fire_date')\n",
        "\n",
        "            # Group by temporal prox within spatial cluster\n",
        "            cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(cluster_data)):\n",
        "                prev_date = cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > temporal_days:\n",
        "                    current_group += 1\n",
        "\n",
        "                cluster_data.iloc[i, cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            # assign fire event IDs\n",
        "            for temp_group in cluster_data['temp_group'].unique():\n",
        "                group_data = cluster_data[cluster_data['temp_group'] == temp_group].copy()\n",
        "                group_data['fire_event_id'] = event_id\n",
        "                fire_events.append(group_data)\n",
        "                event_id += 1\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            result_df = result_df.drop(['spatial_cluster', 'temp_group'], axis=1)\n",
        "            return result_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def analyze_fire_events(self, fire_events_df):\n",
        "        \"\"\"Analyze the identified fire events\"\"\"\n",
        "\n",
        "        if len(fire_events_df) == 0:\n",
        "            return {}\n",
        "\n",
        "        # calc fire event statistics\n",
        "        event_stats = fire_events_df.groupby('fire_event_id').agg({\n",
        "            'id': 'count',\n",
        "            'fire_date': ['min', 'max'],\n",
        "            'longitude': ['min', 'max'],\n",
        "            'latitude': ['min', 'max'],\n",
        "            'area_burned': 'sum',\n",
        "            'consumed_fuel': 'sum',\n",
        "            'ECO2': 'sum'\n",
        "        }).reset_index()\n",
        "\n",
        "        # flatten column names\n",
        "        event_stats.columns = ['fire_event_id', 'num_points', 'start_date', 'end_date',\n",
        "                              'min_lon', 'max_lon', 'min_lat', 'max_lat',\n",
        "                              'total_area_burned', 'total_consumed_fuel', 'total_ECO2']\n",
        "\n",
        "        # duration and spatial extent\n",
        "        event_stats['duration_days'] = (event_stats['end_date'] - event_stats['start_date']).dt.days + 1\n",
        "        event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "            lambda row: self.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                              row['max_lat'], row['max_lon']), axis=1\n",
        "        )\n",
        "\n",
        "        return event_stats\n",
        "\n",
        "    def save_results_to_csv(self, fire_events_df, filename):\n",
        "        \"\"\"Save results to CSV file instead of BigQuery\"\"\"\n",
        "        fire_events_df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "    def save_results(self, fire_events_df, output_table_name):\n",
        "        \"\"\"Save results back to BigQuery - COMMENTED OUT FOR SAFETY\"\"\"\n",
        "        print(\"WARNING: This method would write to BigQuery!\")\n",
        "        print(\"Uncomment the code below only when you're ready to save results\")\n",
        "        print(f\"Would save to table: {self.dataset_id}.{output_table_name}\")\n",
        "        # DO NOT UNCOMMENT THIS\n",
        "        # job_config = bigquery.LoadJobConfig(\n",
        "        #     write_disposition=\"WRITE_TRUNCATE\",\n",
        "        #     schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
        "        # )\n",
        "        #\n",
        "        # job = self.client.load_table_from_dataframe(\n",
        "        #     fire_events_df,\n",
        "        #     f\"{self.dataset_id}.{output_table_name}\",\n",
        "        #     job_config=job_config\n",
        "        # )\n",
        "        #\n",
        "        # job.result()  # Wait for the job to complete\n",
        "        # print(f\"Results saved to {self.dataset_id}.{output_table_name}\")\n",
        "\n",
        "#  2004\n",
        "def main():\n",
        "    # init\n",
        "    clustering = FireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    # process 2004\n",
        "    year = 2004\n",
        "    fire_events = clustering.process_year_chunked(\n",
        "        year=year,\n",
        "        spatial_eps=0.02,  # ~2.2km at equator\n",
        "        temporal_days=7,   # 7-day temporal window\n",
        "        min_samples=3,     # Minimum 3 points per cluster\n",
        "        chunk_size=10000   # Process 10k rows at a time\n",
        "    )\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        stats = clustering.analyze_fire_events(fire_events)\n",
        "        print(f\"\\nYear {year} - Found {len(stats)} fire events\")\n",
        "        print(f\"Average duration: {stats['duration_days'].mean():.1f} days\")\n",
        "        print(f\"Average spatial extent: {stats['spatial_extent_km'].mean():.1f} km\")\n",
        "        print(f\"Total data points in events: {len(fire_events)}\")\n",
        "\n",
        "        # save to CSV\n",
        "        clustering.save_results_to_csv(fire_events, f\"fire_events_{year}.csv\")\n",
        "        clustering.save_results_to_csv(stats, f\"fire_event_stats_{year}.csv\")\n",
        "\n",
        "        # show\n",
        "        print(\"\\nSample fire events:\")\n",
        "        print(stats[['fire_event_id', 'num_points', 'duration_days', 'spatial_extent_km',\n",
        "                    'total_area_burned', 'total_ECO2']].head(10))\n",
        "    else:\n",
        "        print(f\"No fire events found for year {year}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5YPVC_I64dC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752462939155,
          "user_tz": 240,
          "elapsed": 176266,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7d102d23-6c24-40f8-a40e-c8f189a44126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting data overview for 2004...\n",
            "Total valid rows: 239,663\n",
            "Date range: 2003-12-31 to 2004-12-31\n",
            "\n",
            "Processing period: 2003-12-31 to 2004-01-30\n",
            "  Found 9778 records\n",
            "    Fixed spatial clustering found 11 clusters\n",
            "    Noise points: 4\n",
            "  Found 13 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-01-31 to 2004-02-28\n",
            "  Found 11391 records\n",
            "    Fixed spatial clustering found 14 clusters\n",
            "    Noise points: 6\n",
            "  Found 15 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-02-29 to 2004-03-28\n",
            "  Found 46091 records\n",
            "    Fixed spatial clustering found 14 clusters\n",
            "    Noise points: 4\n",
            "  Found 15 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-03-29 to 2004-04-28\n",
            "  Found 30759 records\n",
            "    Fixed spatial clustering found 15 clusters\n",
            "    Noise points: 11\n",
            "  Found 19 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-04-29 to 2004-05-28\n",
            "  Found 14452 records\n",
            "    Fixed spatial clustering found 19 clusters\n",
            "    Noise points: 1\n",
            "  Found 22 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-05-29 to 2004-06-28\n",
            "  Found 19438 records\n",
            "    Fixed spatial clustering found 8 clusters\n",
            "    Noise points: 7\n",
            "  Found 11 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-06-29 to 2004-07-28\n",
            "  Found 28507 records\n",
            "    Fixed spatial clustering found 8 clusters\n",
            "    Noise points: 0\n",
            "  Found 8 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-07-29 to 2004-08-28\n",
            "  Found 22668 records\n",
            "    Fixed spatial clustering found 5 clusters\n",
            "    Noise points: 2\n",
            "  Found 5 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-08-29 to 2004-09-28\n",
            "  Found 26418 records\n",
            "    Fixed spatial clustering found 5 clusters\n",
            "    Noise points: 0\n",
            "  Found 6 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-09-29 to 2004-10-28\n",
            "  Found 12783 records\n",
            "    Fixed spatial clustering found 20 clusters\n",
            "    Noise points: 4\n",
            "  Found 24 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-10-29 to 2004-11-28\n",
            "  Found 6163 records\n",
            "    Fixed spatial clustering found 17 clusters\n",
            "    Noise points: 2\n",
            "  Found 22 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-11-29 to 2004-12-28\n",
            "  Found 10419 records\n",
            "    Fixed spatial clustering found 13 clusters\n",
            "    Noise points: 4\n",
            "  Found 14 fire events in this chunk\n",
            "\n",
            "Processing period: 2004-12-29 to 2004-12-31\n",
            "  Found 796 records\n",
            "    Fixed spatial clustering found 13 clusters\n",
            "    Noise points: 6\n",
            "  Found 13 fire events in this chunk\n",
            "\n",
            "Total fire events found: 187\n",
            "\n",
            "Year 2004 - Found 187 fire events\n",
            "Average duration: 10.7 days\n",
            "Average spatial extent: 457.8 km\n",
            "Total data points in events: 239612\n",
            "Results saved to fire_events_2004.csv\n",
            "Results saved to fire_event_stats_2004.csv\n",
            "\n",
            "Sample fire events:\n",
            "   fire_event_id  num_points  duration_days  spatial_extent_km  \\\n",
            "0              0         487             31         534.414109   \n",
            "1              1           9              4           1.623858   \n",
            "2              2          25              7           2.725034   \n",
            "3              3         181             22         380.048467   \n",
            "4              4          19              6          15.794448   \n",
            "5              5           3              3           2.132254   \n",
            "6              6          68              7         207.982248   \n",
            "7              7        8194             30        3070.653137   \n",
            "8              8           4              1           0.423207   \n",
            "9              9         100             11           4.639141   \n",
            "\n",
            "   total_area_burned    total_ECO2  \n",
            "0         23750000.0  1.122436e+06  \n",
            "1           250000.0  1.633096e+04  \n",
            "2          1187500.0  1.104507e+05  \n",
            "3          6812500.0  4.046785e+05  \n",
            "4           937500.0  3.109979e+03  \n",
            "5           187500.0  5.369692e+03  \n",
            "6          3062500.0  1.585062e+05  \n",
            "7        250375000.0  1.742083e+07  \n",
            "8                0.0  0.000000e+00  \n",
            "9          5000000.0  1.058146e+05  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# cwd\n",
        "print(\"Current working directory:\")\n",
        "print(os.getcwd())\n",
        "\n",
        "# ls\n",
        "print(\"\\nAll files in current directory:\")\n",
        "for file in os.listdir('.'):\n",
        "    print(f\"  {file}\")\n",
        "\n",
        "# Look for csv\n",
        "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "print(f\"\\nCSV files found ({len(csv_files)}):\")\n",
        "for csv_file in csv_files:\n",
        "    print(f\"  {csv_file}\")\n",
        "    size = os.path.getsize(csv_file)\n",
        "    print(f\"    Size: {size:,} bytes\")\n",
        "\n",
        "# show\n",
        "if csv_files:\n",
        "    print(f\"\\nPreview of {csv_files[0]}:\")\n",
        "    df = pd.read_csv(csv_files[0])\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXxnrLC58CjZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752463062383,
          "user_tz": 240,
          "elapsed": 78,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "46c3df89-e99a-475a-d64c-eeee38ef1d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory:\n",
            "/content\n",
            "\n",
            "All files in current directory:\n",
            "  fire_event_stats_2004.csv\n",
            "  fire_events_2004.csv\n",
            "\n",
            "CSV files found (2):\n",
            "  fire_event_stats_2004.csv\n",
            "    Size: 22,406 bytes\n",
            "  fire_events_2004.csv\n",
            "    Size: 21,909,220 bytes\n",
            "\n",
            "Preview of fire_event_stats_2004.csv:\n",
            "  Shape: (187, 13)\n",
            "   fire_event_id  num_points  start_date    end_date   min_lon   max_lon  \\\n",
            "0              0         487  2003-12-31  2004-01-30 -114.0156 -111.1802   \n",
            "1              1           9  2004-01-01  2004-01-04 -123.6575 -123.6529   \n",
            "2              2          25  2004-01-01  2004-01-07 -121.8666 -121.8453   \n",
            "\n",
            "   min_lat  max_lat  total_area_burned  total_consumed_fuel    total_ECO2  \\\n",
            "0  36.5108  40.7771         23750000.0        708644.127100  1.122436e+06   \n",
            "1  42.1500  42.1642           250000.0          9738.199809  1.633096e+04   \n",
            "2  42.4236  42.4424          1187500.0         71075.082111  1.104507e+05   \n",
            "\n",
            "   duration_days  spatial_extent_km  \n",
            "0             31         534.414109  \n",
            "1              4           1.623858  \n",
            "2              7           2.725034  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# init client\n",
        "client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "# load csvs\n",
        "fire_events = pd.read_csv('fire_events_2004.csv')\n",
        "fire_stats = pd.read_csv('fire_event_stats_2004.csv')\n",
        "\n",
        "# upload fire events table\n",
        "table_id = \"code-for-planet.emission_db.fire_events_2004\"\n",
        "job = client.load_table_from_dataframe(fire_events, table_id)\n",
        "job.result()\n",
        "print(f\"Loaded {len(fire_events)} rows into {table_id}\")\n",
        "\n",
        "# upload fire stats table\n",
        "table_id = \"code-for-planet.emission_db.fire_event_stats_2004\"\n",
        "job = client.load_table_from_dataframe(fire_stats, table_id)\n",
        "job.result()\n",
        "print(f\"Loaded {len(fire_stats)} rows into {table_id}\")\n",
        "\n",
        "print(\"Files successfully uploaded to BigQuery!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-UMxi7J8d7I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752463180872,
          "user_tz": 240,
          "elapsed": 7369,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "57cabe71-d45a-4cbe-8bbe-35483542c1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 239612 rows into code-for-planet.emission_db.fire_events_2004\n",
            "Loaded 187 rows into code-for-planet.emission_db.fire_event_stats_2004\n",
            "Files successfully uploaded to BigQuery!\n"
          ]
        }
      ]
    }
  ]
}