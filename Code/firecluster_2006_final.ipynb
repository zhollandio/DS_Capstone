{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "firecluster_2006_final"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from google.cloud import bigquery\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GeographicFireEventClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371  # earths radius in kms\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        return R * c\n",
        "\n",
        "    def km_to_radians(self, km):\n",
        "        \"\"\"Convert kilometers to radians for use with haversine metric\"\"\"\n",
        "        R = 6371\n",
        "        return km / R\n",
        "\n",
        "    def define_geographic_regions(self):\n",
        "        \"\"\"Define strictly non-overlapping geographic regions with clear boundaries\"\"\"\n",
        "        regions = {\n",
        "            'Pacific_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0},\n",
        "            'Mountain_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0},\n",
        "            'Great_Plains': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Texas': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Central': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.1},\n",
        "            'Midwest': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Southeast': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.1, 'lon_max': -75.0},\n",
        "            'Northeast': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0}\n",
        "        }\n",
        "        return regions\n",
        "\n",
        "    def get_region_parameters(self, region_name):\n",
        "        \"\"\"Get optimized parameters for each region based on fire patterns\"\"\"\n",
        "        region_params = {\n",
        "            'Pacific_West': {'spatial_eps_km': 0.5, 'temporal_days': 3, 'min_samples': 3},\n",
        "            'Mountain_West': {'spatial_eps_km': 0.5, 'temporal_days': 3, 'min_samples': 3},\n",
        "            'Great_Plains': {'spatial_eps_km': 0.3, 'temporal_days': 2, 'min_samples': 3},\n",
        "            'South_Texas': {'spatial_eps_km': 0.3, 'temporal_days': 2, 'min_samples': 3},\n",
        "            'South_Central': {'spatial_eps_km': 0.2, 'temporal_days': 1, 'min_samples': 3},\n",
        "            'Midwest': {'spatial_eps_km': 0.3, 'temporal_days': 2, 'min_samples': 3},\n",
        "            'Southeast': {'spatial_eps_km': 0.2, 'temporal_days': 1, 'min_samples': 3},\n",
        "            'Northeast': {'spatial_eps_km': 0.5, 'temporal_days': 3, 'min_samples': 3}\n",
        "        }\n",
        "        return region_params.get(region_name, {'spatial_eps_km': 0.5, 'temporal_days': 3, 'min_samples': 3})\n",
        "\n",
        "    def process_geographic_region(self, year, region_name, region_bounds):\n",
        "        \"\"\"Process fire events for a specific geographic region with region-optimized parameters\"\"\"\n",
        "\n",
        "        # region-specific parameters\n",
        "        params = self.get_region_parameters(region_name)\n",
        "        spatial_eps_km = params['spatial_eps_km']\n",
        "        temporal_days = params['temporal_days']\n",
        "        min_samples = params['min_samples']\n",
        "\n",
        "        # km to radians for DBSCAN\n",
        "        spatial_eps_rad = self.km_to_radians(spatial_eps_km)\n",
        "\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        print(f\"\\nProcessing region: {region_name}\")\n",
        "        print(f\"   Bounds: {region_bounds}\")\n",
        "        print(f\"   Parameters: eps={spatial_eps_km}km ({spatial_eps_rad:.6f} rad), temporal={temporal_days}d, min_samples={min_samples}\")\n",
        "\n",
        "        # query data for this geographic region - strict boundaries\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id, year, doy, longitude, latitude, fire_date,\n",
        "            grid10k, covertype, fuelcode, area_burned,\n",
        "            consumed_fuel, ECO2, burn_source, burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        AND latitude >= {region_bounds['lat_min']}\n",
        "        AND latitude < {region_bounds['lat_max']}\n",
        "        AND longitude >= {region_bounds['lon_min']}\n",
        "        AND longitude < {region_bounds['lon_max']}\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"   No data found for {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"   Found {len(df):,} records\")\n",
        "\n",
        "        # convert fire_date to datetime\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        # apply clustering to this region\n",
        "        clustered_df = self._cluster_region_data(df, spatial_eps_rad, temporal_days, min_samples)\n",
        "\n",
        "        if len(clustered_df) > 0:\n",
        "            # region identifier\n",
        "            clustered_df['region'] = region_name\n",
        "            unique_events = len(clustered_df['fire_event_id'].unique())\n",
        "            print(f\"   Found {unique_events} fire events in {region_name}\")\n",
        "            return clustered_df\n",
        "        else:\n",
        "            print(f\"   No fire events found in {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _cluster_region_data(self, df, spatial_eps_rad, temporal_days, min_samples):\n",
        "        \"\"\"Apply DBSCAN clustering to data from a single geographic region\"\"\"\n",
        "\n",
        "        # apply temporal pre-filtering to break up data into smaller temporal chunks\n",
        "        # prevents huge spatial clusters from forming across long time periods\n",
        "        df_sorted = df.sort_values('fire_date').copy()\n",
        "\n",
        "        # temporal groups (break if gap > temporal_days)\n",
        "        df_sorted['temporal_group'] = 0\n",
        "        current_group = 0\n",
        "\n",
        "        for i in range(1, len(df_sorted)):\n",
        "            prev_date = df_sorted.iloc[i-1]['fire_date']\n",
        "            curr_date = df_sorted.iloc[i]['fire_date']\n",
        "\n",
        "            if (curr_date - prev_date).days > temporal_days:\n",
        "                current_group += 1\n",
        "\n",
        "            df_sorted.iloc[i, df_sorted.columns.get_loc('temporal_group')] = current_group\n",
        "\n",
        "        # apply spatial clustering within each temporal group\n",
        "        fire_events = []\n",
        "        isolated_points = []\n",
        "        event_id = 0\n",
        "\n",
        "        for temp_group in df_sorted['temporal_group'].unique():\n",
        "            temp_group_data = df_sorted[df_sorted['temporal_group'] == temp_group].copy()\n",
        "\n",
        "            # handle small temporal groups differently\n",
        "            if len(temp_group_data) < min_samples:\n",
        "                # treat each point as individual fire event (small groups)\n",
        "                for idx, row in temp_group_data.iterrows():\n",
        "                    single_event = pd.DataFrame([row])\n",
        "                    single_event['fire_event_id'] = event_id\n",
        "                    fire_events.append(single_event)\n",
        "                    event_id += 1\n",
        "                continue\n",
        "\n",
        "            # spatial clustering\n",
        "            coords = temp_group_data[['longitude', 'latitude']].values\n",
        "            coords_rad = np.radians(coords)\n",
        "\n",
        "            dbscan = DBSCAN(eps=spatial_eps_rad, min_samples=min_samples, metric='haversine')\n",
        "            spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "            temp_group_data['spatial_cluster'] = spatial_labels\n",
        "\n",
        "            # spatial cluster within this temporal group becomes a fire event\n",
        "            for spatial_cluster in temp_group_data['spatial_cluster'].unique():\n",
        "                if spatial_cluster == -1:  # Handle noise points\n",
        "                    # convert noise points to individual events\n",
        "                    noise_data = temp_group_data[temp_group_data['spatial_cluster'] == spatial_cluster]\n",
        "                    for idx, row in noise_data.iterrows():\n",
        "                        single_event = pd.DataFrame([row]).drop(['spatial_cluster'], axis=1)\n",
        "                        single_event['fire_event_id'] = event_id\n",
        "                        fire_events.append(single_event)\n",
        "                        event_id += 1\n",
        "                    continue\n",
        "\n",
        "                cluster_data = temp_group_data[temp_group_data['spatial_cluster'] == spatial_cluster].copy()\n",
        "                cluster_data['fire_event_id'] = event_id\n",
        "                fire_events.append(cluster_data)\n",
        "                event_id += 1\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            # clean up\n",
        "            columns_to_drop = ['temporal_group', 'spatial_cluster']\n",
        "            columns_to_drop = [col for col in columns_to_drop if col in result_df.columns]\n",
        "            if columns_to_drop:\n",
        "                result_df = result_df.drop(columns_to_drop, axis=1)\n",
        "            return result_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_year_geographic(self, year):\n",
        "        \"\"\"Process entire year using geographic chunking approach with region-specific parameters\"\"\"\n",
        "\n",
        "        print(f\"GEOGRAPHIC FIRE CLUSTERING for {year}\")\n",
        "        print(\"Using region-specific optimized parameters (spatial eps in km, converted to radians):\")\n",
        "\n",
        "        regions = self.define_geographic_regions()\n",
        "\n",
        "        # params for each region\n",
        "        for region_name in regions.keys():\n",
        "            params = self.get_region_parameters(region_name)\n",
        "            spatial_eps_rad = self.km_to_radians(params['spatial_eps_km'])\n",
        "            print(f\"  {region_name}: eps={params['spatial_eps_km']}km ({spatial_eps_rad:.6f} rad), temporal={params['temporal_days']}d\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # total original data count for coverage calculation\n",
        "        total_query = f\"\"\"\n",
        "        SELECT COUNT(*) as total_count\n",
        "        FROM `{self.project_id}.{self.dataset_id}.emission_{year}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        total_result = self.client.query(total_query).to_dataframe()\n",
        "        original_count = total_result['total_count'].iloc[0]\n",
        "\n",
        "        all_regional_events = []\n",
        "        global_event_id = 0\n",
        "\n",
        "        for region_name, region_bounds in regions.items():\n",
        "            regional_events = self.process_geographic_region(\n",
        "                year, region_name, region_bounds\n",
        "            )\n",
        "\n",
        "            if len(regional_events) > 0:\n",
        "                # adjust event IDs to be globally unique\n",
        "                max_regional_id = regional_events['fire_event_id'].max()\n",
        "                regional_events['fire_event_id'] += global_event_id\n",
        "                global_event_id += max_regional_id + 1\n",
        "\n",
        "                all_regional_events.append(regional_events)\n",
        "\n",
        "        # combine all regions + check for duplicates\n",
        "        if all_regional_events:\n",
        "            combined_events = pd.concat(all_regional_events, ignore_index=True)\n",
        "\n",
        "            # remove duplicate records (same id appearing multiple times)\n",
        "            initial_count = len(combined_events)\n",
        "            combined_events = combined_events.drop_duplicates(subset=['id'], keep='first')\n",
        "            final_count = len(combined_events)\n",
        "\n",
        "            if initial_count != final_count:\n",
        "                print(f\"WARNING: Removed {initial_count - final_count} duplicate records\")\n",
        "\n",
        "            print(f\"\\nFINAL RESULTS:\")\n",
        "            print(f\"Total fire events: {len(combined_events['fire_event_id'].unique())}\")\n",
        "            print(f\"Total data points: {len(combined_events):,}\")\n",
        "            print(f\"Original data points: {original_count:,}\")\n",
        "            print(f\"Coverage: {len(combined_events)/original_count*100:.1f}% of original data\")\n",
        "\n",
        "            if len(combined_events) > original_count:\n",
        "                print(f\"WARNING: More data points than original - investigating...\")\n",
        "\n",
        "                # check regional totals\n",
        "                print(\"Regional data point totals:\")\n",
        "                for events in all_regional_events:\n",
        "                    region = events['region'].iloc[0]\n",
        "                    print(f\"  {region}: {len(events):,} points\")\n",
        "\n",
        "            return combined_events\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def analyze_fire_events(self, fire_events_df):\n",
        "        \"\"\"Analyze the identified fire events\"\"\"\n",
        "        if len(fire_events_df) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # calc fire event stats\n",
        "        event_stats = fire_events_df.groupby('fire_event_id').agg({\n",
        "            'id': 'count',\n",
        "            'fire_date': ['min', 'max'],\n",
        "            'longitude': ['min', 'max'],\n",
        "            'latitude': ['min', 'max'],\n",
        "            'area_burned': 'sum',\n",
        "            'consumed_fuel': 'sum',\n",
        "            'ECO2': 'sum',\n",
        "            'region': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        # flatten column names\n",
        "        event_stats.columns = ['fire_event_id', 'num_points', 'start_date', 'end_date',\n",
        "                              'min_lon', 'max_lon', 'min_lat', 'max_lat',\n",
        "                              'total_area_burned', 'total_consumed_fuel', 'total_ECO2', 'region']\n",
        "\n",
        "        # calc duration and spatial extent\n",
        "        event_stats['duration_days'] = (event_stats['end_date'] - event_stats['start_date']).dt.days + 1\n",
        "        event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "            lambda row: self.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                              row['max_lat'], row['max_lon']), axis=1\n",
        "        )\n",
        "\n",
        "        return event_stats\n",
        "\n",
        "    def apply_realistic_fire_limits(self, fire_events_df):\n",
        "        \"\"\"Applying better limits based on actual wildfire behavior\"\"\"\n",
        "        if len(fire_events_df) == 0:\n",
        "            return fire_events_df\n",
        "\n",
        "        print(f\"\\nApplying realistic fire event limits:\")\n",
        "        print(f\"  Spatial limit: 80km (auto-split), 120km (hard fail)\")\n",
        "        print(f\"  Temporal limit: 10 days (split in time)\")\n",
        "\n",
        "        # calc current event statistics\n",
        "        event_stats = self.analyze_fire_events(fire_events_df)\n",
        "\n",
        "        # problematic events\n",
        "        spatial_moderate = event_stats[event_stats['spatial_extent_km'] > 80]['fire_event_id'].tolist()\n",
        "        spatial_extreme = event_stats[event_stats['spatial_extent_km'] > 120]['fire_event_id'].tolist()\n",
        "        temporal_long = event_stats[event_stats['duration_days'] > 10]['fire_event_id'].tolist()\n",
        "\n",
        "        print(f\"  Events >80km (auto-split): {len(spatial_moderate)}\")\n",
        "        print(f\"  Events >120km (hard fail): {len(spatial_extreme)}\")\n",
        "        print(f\"  Events >10 days (time-split): {len(temporal_long)}\")\n",
        "\n",
        "        # hard fail- Drop events >120km (cannot be single fires, basing this off historical data)\n",
        "        if spatial_extreme:\n",
        "            print(f\"\\nHARD FAIL: Dropping {len(spatial_extreme)} events >120km (not realistic single fires)\")\n",
        "            fire_events_df = fire_events_df[~fire_events_df['fire_event_id'].isin(spatial_extreme)]\n",
        "\n",
        "        # problematic events\n",
        "        remaining_events = fire_events_df['fire_event_id'].unique()\n",
        "        spatial_moderate = [e for e in spatial_moderate if e in remaining_events and e not in spatial_extreme]\n",
        "        temporal_long = [e for e in temporal_long if e in remaining_events and e not in spatial_extreme]\n",
        "\n",
        "        # process events that need fixing\n",
        "        events_to_fix = list(set(spatial_moderate + temporal_long))\n",
        "\n",
        "        if not events_to_fix:\n",
        "            print(f\"No events require fixing\")\n",
        "            return fire_events_df\n",
        "\n",
        "        print(f\"\\nFixing {len(events_to_fix)} events...\")\n",
        "\n",
        "        # separate good events from events needing fixes\n",
        "        good_events = fire_events_df[~fire_events_df['fire_event_id'].isin(events_to_fix)].copy()\n",
        "        problem_events = fire_events_df[fire_events_df['fire_event_id'].isin(events_to_fix)].copy()\n",
        "\n",
        "        # new event ID starting point\n",
        "        max_event_id = fire_events_df['fire_event_id'].max() + 1\n",
        "\n",
        "        # fix problematic event\n",
        "        fixed_events = []\n",
        "\n",
        "        for i, event_id in enumerate(events_to_fix):\n",
        "            event_data = problem_events[problem_events['fire_event_id'] == event_id].copy()\n",
        "            region = event_data['region'].iloc[0]\n",
        "\n",
        "            # event stats\n",
        "            event_stat = event_stats[event_stats['fire_event_id'] == event_id].iloc[0]\n",
        "            spatial_extent = event_stat['spatial_extent_km']\n",
        "            duration = event_stat['duration_days']\n",
        "\n",
        "            # progress indicator (every 100 events - didnt want to flood logs)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"  Progress: {i}/{len(events_to_fix)} events processed...\")\n",
        "\n",
        "            # fix strategy\n",
        "            if spatial_extent > 80:\n",
        "                # use tight spatial clustering\n",
        "                tight_spatial_eps = self.km_to_radians(0.1)  # 100 meters\n",
        "                tight_temporal_days = 1\n",
        "                tight_min_samples = 2\n",
        "\n",
        "            elif duration > 10:\n",
        "                # use tighter temporal windows\n",
        "                tight_spatial_eps = self.km_to_radians(0.5)\n",
        "                tight_temporal_days = 2\n",
        "                tight_min_samples = 2\n",
        "\n",
        "            else:\n",
        "                # shouldnt happen, but handle\n",
        "                tight_spatial_eps = self.km_to_radians(0.2)\n",
        "                tight_temporal_days = 1\n",
        "                tight_min_samples = 2\n",
        "\n",
        "            # recluster\n",
        "            fixed_data = self._cluster_region_data(event_data, tight_spatial_eps, tight_temporal_days, tight_min_samples)\n",
        "\n",
        "            if len(fixed_data) > 0:\n",
        "                # reassign id\n",
        "                unique_events = fixed_data['fire_event_id'].unique()\n",
        "                for j, old_id in enumerate(unique_events):\n",
        "                    fixed_data.loc[fixed_data['fire_event_id'] == old_id, 'fire_event_id'] = max_event_id + j\n",
        "                max_event_id += len(unique_events)\n",
        "\n",
        "                fixed_events.append(fixed_data)\n",
        "            else:\n",
        "                # convert to individual events\n",
        "                for idx, row in event_data.iterrows():\n",
        "                    single_event = pd.DataFrame([row])\n",
        "                    single_event['fire_event_id'] = max_event_id\n",
        "                    fixed_events.append(single_event)\n",
        "                    max_event_id += 1\n",
        "\n",
        "        # combine\n",
        "        if fixed_events:\n",
        "            all_fixed = pd.concat(fixed_events, ignore_index=True)\n",
        "            final_events = pd.concat([good_events, all_fixed], ignore_index=True)\n",
        "        else:\n",
        "            final_events = good_events\n",
        "\n",
        "        print(f\"Realistic limits applied: {len(fire_events_df['fire_event_id'].unique())} → {len(final_events['fire_event_id'].unique())} events\")\n",
        "\n",
        "        return final_events\n",
        "\n",
        "    def save_results_to_csv(self, fire_events_df, filename):\n",
        "        \"\"\"Save results to CSV file\"\"\"\n",
        "        fire_events_df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "\n",
        "    # init cluster\n",
        "    clustering = GeographicFireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    # process 2006\n",
        "    year = 2006\n",
        "    fire_events = clustering.process_year_geographic(year=year)\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        # apply realistic fire limits\n",
        "        fire_events = clustering.apply_realistic_fire_limits(fire_events)\n",
        "\n",
        "        stats = clustering.analyze_fire_events(fire_events)\n",
        "        print(f\"\\nFINAL CLUSTERING RESULTS:\")\n",
        "        print(f\"Fire events found: {len(stats)}\")\n",
        "        print(f\"Average duration: {stats['duration_days'].mean():.1f} days\")\n",
        "        print(f\"Average spatial extent: {stats['spatial_extent_km'].mean():.1f} km\")\n",
        "        print(f\"Max spatial extent: {stats['spatial_extent_km'].max():.1f} km\")\n",
        "\n",
        "        # events exceeding realistic limits\n",
        "        large_events = stats[stats['spatial_extent_km'] > 80]\n",
        "        very_large_events = stats[stats['spatial_extent_km'] > 120]\n",
        "        long_events = stats[stats['duration_days'] > 10]\n",
        "\n",
        "        print(f\"\\nQUALITY CHECK (Scientifically-grounded limits):\")\n",
        "        print(f\"Events >80km: {len(large_events)} (auto-split threshold)\")\n",
        "        print(f\"Events >120km: {len(very_large_events)} (hard fail - should be 0)\")\n",
        "        print(f\"Events >10 days: {len(long_events)} (temporal split threshold)\")\n",
        "\n",
        "        if len(large_events) > 0:\n",
        "            print(f\"\\nDETAILED ANALYSIS OF LARGE EVENTS:\")\n",
        "            for idx, event in large_events.iterrows():\n",
        "                print(f\"\\nEvent {event['fire_event_id']} in {event['region']}:\")\n",
        "                print(f\"  Spatial extent: {event['spatial_extent_km']:.1f} km\")\n",
        "                print(f\"  Duration: {event['duration_days']} days\")\n",
        "                print(f\"  Points: {event['num_points']}\")\n",
        "                print(f\"  Date range: {event['start_date']} to {event['end_date']}\")\n",
        "                print(f\"  Lat range: {event['min_lat']:.3f} to {event['max_lat']:.3f}\")\n",
        "                print(f\"  Lon range: {event['min_lon']:.3f} to {event['max_lon']:.3f}\")\n",
        "\n",
        "                # approximate distance components\n",
        "                lat_dist = abs(event['max_lat'] - event['min_lat']) * 111  # ~111 km per degree\n",
        "                lon_dist = abs(event['max_lon'] - event['min_lon']) * 111 * np.cos(np.radians((event['max_lat'] + event['min_lat'])/2))\n",
        "                print(f\"  Approximate N-S span: {lat_dist:.1f} km\")\n",
        "                print(f\"  Approximate E-W span: {lon_dist:.1f} km\")\n",
        "\n",
        "                # flag based on thresholds\n",
        "                if event['spatial_extent_km'] > 120:\n",
        "                    print(f\"  STATUS: HARD FAIL - Drop this event (>120km)\")\n",
        "                elif event['spatial_extent_km'] > 80:\n",
        "                    print(f\"  STATUS: AUTO-SPLIT - Re-cluster with tight spatial limits\")\n",
        "                elif event['duration_days'] > 10:\n",
        "                    print(f\"  STATUS: TIME-SPLIT - Re-cluster with tight temporal limits\")\n",
        "\n",
        "            large_by_region = large_events.groupby('region')['fire_event_id'].count()\n",
        "            print(f\"\\nLarge events by region:\")\n",
        "            print(large_by_region)\n",
        "\n",
        "        # save\n",
        "        clustering.save_results_to_csv(fire_events, f\"fire_events_{year}_final.csv\")\n",
        "        clustering.save_results_to_csv(stats, f\"fire_event_stats_{year}_final.csv\")\n",
        "\n",
        "        print(f\"\\nResults saved:\")\n",
        "        print(f\"- fire_events_{year}_final.csv\")\n",
        "        print(f\"- fire_event_stats_{year}_final.csv\")\n",
        "\n",
        "        # sample\n",
        "        print(f\"\\nResults by region:\")\n",
        "        region_summary = stats.groupby('region').agg({\n",
        "            'fire_event_id': 'count',\n",
        "            'spatial_extent_km': 'mean',\n",
        "            'duration_days': 'mean'\n",
        "        }).round(1)\n",
        "        print(region_summary)\n",
        "\n",
        "    else:\n",
        "        print(\"No fire events found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIkzgL6J5aO-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752626890094,
          "user_tz": 240,
          "elapsed": 835428,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d67de8b7-7918-4e69-cc1b-66f03c5b92e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEOGRAPHIC FIRE CLUSTERING\n",
            "Key improvements made over a1 and other previous attempts:\n",
            "1. DBSCAN eps parameter correctly converted from km to radians\n",
            "2. Temporal filtering applied BEFORE spatial clustering to prevent chaining\n",
            "3. Much tighter spatial clustering distances (0.2-0.5 km)\n",
            "4. Reduced min_samples from 5 to 3 for better coverage\n",
            "5. Convert noise points and small groups to individual events\n",
            "6. Conducted better research on what realistic limits are could be to break up events:\n",
            "   - >80km: Auto-split (re-cluster with tight spatial limits)\n",
            "   - >120km: Hard fail (drop - cannot be single fire)\n",
            "   - >10 days: Time-split (re-cluster with tight temporal limits)\n",
            "\n",
            "GEOGRAPHIC FIRE CLUSTERING for 2006\n",
            "Using region-specific optimized parameters (spatial eps in km, converted to radians):\n",
            "  Pacific_West: eps=0.5km (0.000078 rad), temporal=3d\n",
            "  Mountain_West: eps=0.5km (0.000078 rad), temporal=3d\n",
            "  Great_Plains: eps=0.3km (0.000047 rad), temporal=2d\n",
            "  South_Texas: eps=0.3km (0.000047 rad), temporal=2d\n",
            "  South_Central: eps=0.2km (0.000031 rad), temporal=1d\n",
            "  Midwest: eps=0.3km (0.000047 rad), temporal=2d\n",
            "  Southeast: eps=0.2km (0.000031 rad), temporal=1d\n",
            "  Northeast: eps=0.5km (0.000078 rad), temporal=3d\n",
            "================================================================================\n",
            "\n",
            "Processing region: Pacific_West\n",
            "   Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0}\n",
            "   Parameters: eps=0.5km (0.000078 rad), temporal=3d, min_samples=3\n",
            "   Found 250,952 records\n",
            "   Found 1813 fire events in Pacific_West\n",
            "\n",
            "Processing region: Mountain_West\n",
            "   Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0}\n",
            "   Parameters: eps=0.5km (0.000078 rad), temporal=3d, min_samples=3\n",
            "   Found 162,847 records\n",
            "   Found 1833 fire events in Mountain_West\n",
            "\n",
            "Processing region: Great_Plains\n",
            "   Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0}\n",
            "   Parameters: eps=0.3km (0.000047 rad), temporal=2d, min_samples=3\n",
            "   Found 246,744 records\n",
            "   Found 18534 fire events in Great_Plains\n",
            "\n",
            "Processing region: South_Texas\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0}\n",
            "   Parameters: eps=0.3km (0.000047 rad), temporal=2d, min_samples=3\n",
            "   Found 20,945 records\n",
            "   Found 1679 fire events in South_Texas\n",
            "\n",
            "Processing region: South_Central\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.1}\n",
            "   Parameters: eps=0.2km (0.000031 rad), temporal=1d, min_samples=3\n",
            "   Found 51,887 records\n",
            "   Found 13688 fire events in South_Central\n",
            "\n",
            "Processing region: Midwest\n",
            "   Bounds: {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0}\n",
            "   Parameters: eps=0.3km (0.000047 rad), temporal=2d, min_samples=3\n",
            "   Found 8,706 records\n",
            "   Found 1503 fire events in Midwest\n",
            "\n",
            "Processing region: Southeast\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.1, 'lon_max': -75.0}\n",
            "   Parameters: eps=0.2km (0.000031 rad), temporal=1d, min_samples=3\n",
            "   Found 59,908 records\n",
            "   Found 8084 fire events in Southeast\n",
            "\n",
            "Processing region: Northeast\n",
            "   Bounds: {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0}\n",
            "   Parameters: eps=0.5km (0.000078 rad), temporal=3d, min_samples=3\n",
            "   Found 7,037 records\n",
            "   Found 1073 fire events in Northeast\n",
            "\n",
            "FINAL RESULTS:\n",
            "Total fire events: 48207\n",
            "Total data points: 809,026\n",
            "Original data points: 813,256\n",
            "Coverage: 99.5% of original data\n",
            "\n",
            "Applying realistic fire event limits:\n",
            "  Spatial limit: 80km (auto-split), 120km (hard fail)\n",
            "  Temporal limit: 10 days (split in time)\n",
            "  Events >80km (auto-split): 39\n",
            "  Events >120km (hard fail): 18\n",
            "  Events >10 days (time-split): 4839\n",
            "\n",
            "HARD FAIL: Dropping 18 events >120km (not realistic single fires)\n",
            "\n",
            "Fixing 4821 events...\n",
            "  Progress: 0/4821 events processed...\n",
            "  Progress: 100/4821 events processed...\n",
            "  Progress: 200/4821 events processed...\n",
            "  Progress: 300/4821 events processed...\n",
            "  Progress: 400/4821 events processed...\n",
            "  Progress: 500/4821 events processed...\n",
            "  Progress: 600/4821 events processed...\n",
            "  Progress: 700/4821 events processed...\n",
            "  Progress: 800/4821 events processed...\n",
            "  Progress: 900/4821 events processed...\n",
            "  Progress: 1000/4821 events processed...\n",
            "  Progress: 1100/4821 events processed...\n",
            "  Progress: 1200/4821 events processed...\n",
            "  Progress: 1300/4821 events processed...\n",
            "  Progress: 1400/4821 events processed...\n",
            "  Progress: 1500/4821 events processed...\n",
            "  Progress: 1600/4821 events processed...\n",
            "  Progress: 1700/4821 events processed...\n",
            "  Progress: 1800/4821 events processed...\n",
            "  Progress: 1900/4821 events processed...\n",
            "  Progress: 2000/4821 events processed...\n",
            "  Progress: 2100/4821 events processed...\n",
            "  Progress: 2200/4821 events processed...\n",
            "  Progress: 2300/4821 events processed...\n",
            "  Progress: 2400/4821 events processed...\n",
            "  Progress: 2500/4821 events processed...\n",
            "  Progress: 2600/4821 events processed...\n",
            "  Progress: 2700/4821 events processed...\n",
            "  Progress: 2800/4821 events processed...\n",
            "  Progress: 2900/4821 events processed...\n",
            "  Progress: 3000/4821 events processed...\n",
            "  Progress: 3100/4821 events processed...\n",
            "  Progress: 3200/4821 events processed...\n",
            "  Progress: 3300/4821 events processed...\n",
            "  Progress: 3400/4821 events processed...\n",
            "  Progress: 3500/4821 events processed...\n",
            "  Progress: 3600/4821 events processed...\n",
            "  Progress: 3700/4821 events processed...\n",
            "  Progress: 3800/4821 events processed...\n",
            "  Progress: 3900/4821 events processed...\n",
            "  Progress: 4000/4821 events processed...\n",
            "  Progress: 4100/4821 events processed...\n",
            "  Progress: 4200/4821 events processed...\n",
            "  Progress: 4300/4821 events processed...\n",
            "  Progress: 4400/4821 events processed...\n",
            "  Progress: 4500/4821 events processed...\n",
            "  Progress: 4600/4821 events processed...\n",
            "  Progress: 4700/4821 events processed...\n",
            "  Progress: 4800/4821 events processed...\n",
            "Realistic limits applied: 48189 → 64212 events\n",
            "\n",
            "FINAL CLUSTERING RESULTS:\n",
            "Fire events found: 64212\n",
            "Average duration: 1.9 days\n",
            "Average spatial extent: 1.0 km\n",
            "Max spatial extent: 70.1 km\n",
            "\n",
            "QUALITY CHECK (Scientifically-grounded limits):\n",
            "Events >80km: 0 (auto-split threshold)\n",
            "Events >120km: 0 (hard fail - should be 0)\n",
            "Events >10 days: 850 (temporal split threshold)\n",
            "Results saved to fire_events_2006_final.csv\n",
            "Results saved to fire_event_stats_2006_final.csv\n",
            "\n",
            "Results saved:\n",
            "- fire_events_2006_final.csv\n",
            "- fire_event_stats_2006_final.csv\n",
            "\n",
            "Results by region:\n",
            "               fire_event_id  spatial_extent_km  duration_days\n",
            "region                                                        \n",
            "Great_Plains           30207                1.4            2.2\n",
            "Midwest                 1580                0.4            1.4\n",
            "Mountain_West           2175                1.7            2.1\n",
            "Northeast               1158                0.4            2.1\n",
            "Pacific_West            2594                1.9            2.6\n",
            "South_Central          15512                0.5            1.5\n",
            "South_Texas             1819                0.5            2.3\n",
            "Southeast               9167                0.4            1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QC for clustering\n",
        "\n",
        "def test_fire_clustering_quality():\n",
        "    \"\"\"Comprehensive quality tests for the fire clustering results\"\"\"\n",
        "\n",
        "    print(\"FIRE CLUSTERING QUALITY TESTS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        fire_events = pd.read_csv(\"fire_events_2006_final.csv\")\n",
        "        stats = pd.read_csv(\"fire_event_stats_2006_final.csv\")\n",
        "        print(f\"Loaded results: {len(fire_events):,} data points, {len(stats):,} events\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Results files not found. Run main clustering first.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 1: SPATIAL REALISM\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    spatial_bins = [0, 1, 5, 10, 20, 50, 80, 120, 1000]\n",
        "    spatial_counts = pd.cut(stats['spatial_extent_km'], bins=spatial_bins, right=False).value_counts().sort_index()\n",
        "\n",
        "    print(\"Spatial extent distribution:\")\n",
        "    for interval, count in spatial_counts.items():\n",
        "        pct = count / len(stats) * 100\n",
        "        print(f\"  {interval}: {count:,} events ({pct:.1f}%)\")\n",
        "\n",
        "    mega_fires = stats[stats['spatial_extent_km'] > 80]\n",
        "    impossible_fires = stats[stats['spatial_extent_km'] > 120]\n",
        "\n",
        "    print(f\"\\nCRITICAL SPATIAL TESTS:\")\n",
        "    print(f\"  Events >80km: {len(mega_fires)} (should be 0)\")\n",
        "    print(f\"  Events >120km: {len(impossible_fires)} (MUST be 0)\")\n",
        "    print(f\"  Max extent: {stats['spatial_extent_km'].max():.1f} km\")\n",
        "    print(f\"  95th percentile: {stats['spatial_extent_km'].quantile(0.95):.1f} km\")\n",
        "\n",
        "    if len(impossible_fires) > 0:\n",
        "        print(\"FAILED: Found impossible fires >120km!\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"PASSED: No impossible mega-fires\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 2: TEMPORAL REALISM\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    duration_bins = [0, 1, 2, 3, 5, 7, 10, 15, 30, 365]\n",
        "    duration_counts = pd.cut(stats['duration_days'], bins=duration_bins, right=False).value_counts().sort_index()\n",
        "\n",
        "    print(\"Duration distribution:\")\n",
        "    for interval, count in duration_counts.items():\n",
        "        pct = count / len(stats) * 100\n",
        "        print(f\"  {interval}: {count:,} events ({pct:.1f}%)\")\n",
        "\n",
        "    very_long = stats[stats['duration_days'] > 30]\n",
        "    impossible_long = stats[stats['duration_days'] > 365]\n",
        "\n",
        "    print(f\"\\nCRITICAL TEMPORAL TESTS:\")\n",
        "    print(f\"  Events >30 days: {len(very_long)} (should be rare)\")\n",
        "    print(f\"  Events >365 days: {len(impossible_long)} (MUST be 0)\")\n",
        "    print(f\"  Max duration: {stats['duration_days'].max()} days\")\n",
        "    print(f\"  95th percentile: {stats['duration_days'].quantile(0.95):.1f} days\")\n",
        "\n",
        "    if len(impossible_long) > 0:\n",
        "        print(\"FAILED: Found impossible year-long fires!\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"PASSED: No impossible year-long fires\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 3: DATA INTEGRITY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    fire_events['fire_date'] = pd.to_datetime(fire_events['fire_date'])\n",
        "\n",
        "    actual_event_sizes = fire_events.groupby('fire_event_id').size()\n",
        "    reported_sizes = stats.set_index('fire_event_id')['num_points']\n",
        "\n",
        "    size_mismatch = (actual_event_sizes != reported_sizes).sum()\n",
        "    print(f\"Event size mismatches: {size_mismatch} (should be 0)\")\n",
        "\n",
        "    events_in_data = set(fire_events['fire_event_id'].unique())\n",
        "    events_in_stats = set(stats['fire_event_id'].unique())\n",
        "    orphaned = len(events_in_stats - events_in_data)\n",
        "    missing = len(events_in_data - events_in_stats)\n",
        "\n",
        "    print(f\"Orphaned events in stats: {orphaned} (should be 0)\")\n",
        "    print(f\"Missing events from stats: {missing} (should be 0)\")\n",
        "\n",
        "    null_coords = fire_events[['latitude', 'longitude']].isnull().any(axis=1).sum()\n",
        "    null_dates = fire_events['fire_date'].isnull().sum()\n",
        "\n",
        "    print(f\"Null coordinates: {null_coords} (should be 0)\")\n",
        "    print(f\"Null dates: {null_dates} (should be 0)\")\n",
        "\n",
        "    if size_mismatch == 0 and orphaned == 0 and missing == 0 and null_coords == 0 and null_dates == 0:\n",
        "        print(\"PASSED: Data integrity checks\")\n",
        "    else:\n",
        "        print(\"FAILED: Data integrity issues found\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 4: GEOGRAPHIC COVERAGE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    region_coverage = stats.groupby('region').agg({\n",
        "        'fire_event_id': 'count',\n",
        "        'spatial_extent_km': ['mean', 'max'],\n",
        "        'duration_days': ['mean', 'max'],\n",
        "        'num_points': 'sum'\n",
        "    }).round(2)\n",
        "\n",
        "    print(\"Regional summary:\")\n",
        "    print(region_coverage)\n",
        "\n",
        "    expected_regions = {\n",
        "        'Pacific_West', 'Mountain_West', 'Great_Plains', 'South_Texas',\n",
        "        'South_Central', 'Midwest', 'Southeast', 'Northeast'\n",
        "    }\n",
        "    actual_regions = set(stats['region'].unique())\n",
        "    missing_regions = expected_regions - actual_regions\n",
        "\n",
        "    if missing_regions:\n",
        "        print(f\"Missing regions: {missing_regions}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"PASSED: All regions represented\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 5: EDGE CASE VALIDATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    single_point_events = stats[stats['num_points'] == 1]\n",
        "    print(f\"Single-point events: {len(single_point_events):,} ({len(single_point_events)/len(stats)*100:.1f}%)\")\n",
        "\n",
        "    zero_duration = stats[stats['duration_days'] == 1]\n",
        "    print(f\"Same-day events: {len(zero_duration):,} ({len(zero_duration)/len(stats)*100:.1f}%)\")\n",
        "\n",
        "    zero_spatial = stats[stats['spatial_extent_km'] < 0.001]\n",
        "    print(f\"Point fires (<1m): {len(zero_spatial):,} ({len(zero_spatial)/len(stats)*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST 6: CLUSTERING EFFECTIVENESS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    total_points = len(fire_events)\n",
        "    total_events = len(stats)\n",
        "    avg_points_per_event = total_points / total_events\n",
        "\n",
        "    print(f\"Clustering efficiency:\")\n",
        "    print(f\"  Total points: {total_points:,}\")\n",
        "    print(f\"  Total events: {total_events:,}\")\n",
        "    print(f\"  Average points per event: {avg_points_per_event:.1f}\")\n",
        "    print(f\"  Reduction factor: {avg_points_per_event:.1f}x\")\n",
        "\n",
        "    event_sizes = stats['num_points'].value_counts().sort_index()\n",
        "    print(f\"\\nEvent size distribution:\")\n",
        "    for size in [1, 2, 3, 4, 5]:\n",
        "        if size in event_sizes.index:\n",
        "            count = event_sizes[size]\n",
        "            pct = count / len(stats) * 100\n",
        "            print(f\"  {size} points: {count:,} events ({pct:.1f}%)\")\n",
        "\n",
        "    large_events = stats[stats['num_points'] > 100]\n",
        "    print(f\"  >100 points: {len(large_events):,} events ({len(large_events)/len(stats)*100:.1f}%)\")\n",
        "\n",
        "    if avg_points_per_event > 1.5:\n",
        "        print(\"PASSED: Effective clustering achieved\")\n",
        "    else:\n",
        "        print(\"WARNING: Very little clustering - mostly individual points\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OVERALL TEST RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(f\"  • {len(stats):,} realistic fire events\")\n",
        "    print(f\"  • {stats['spatial_extent_km'].mean():.1f} km average spatial extent\")\n",
        "    print(f\"  • {stats['duration_days'].mean():.1f} days average duration\")\n",
        "    print(f\"  • {stats['spatial_extent_km'].max():.1f} km maximum extent (realistic)\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def test_different_year():\n",
        "    \"\"\"Test the system on a different year to ensure robustness\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ROBUSTNESS TEST: Different Year\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"Testing system on year 2005 (subset) to ensure robustness...\")\n",
        "\n",
        "    clustering = GeographicFireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    region_name = \"Southeast\"\n",
        "    region_bounds = {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.1, 'lon_max': -80.0}\n",
        "\n",
        "    try:\n",
        "        regional_events = clustering.process_geographic_region(\n",
        "            2005, region_name, region_bounds\n",
        "        )\n",
        "\n",
        "        if len(regional_events) > 0:\n",
        "            cleaned_events = clustering.apply_realistic_fire_limits(regional_events)\n",
        "            stats = clustering.analyze_fire_events(cleaned_events)\n",
        "\n",
        "            print(f\"2005 {region_name} test successful:\")\n",
        "            print(f\"  Events found: {len(stats)}\")\n",
        "            print(f\"  Max spatial extent: {stats['spatial_extent_km'].max():.1f} km\")\n",
        "            print(f\"  Max duration: {stats['duration_days'].max()} days\")\n",
        "            print(f\"  Average extent: {stats['spatial_extent_km'].mean():.1f} km\")\n",
        "\n",
        "            if stats['spatial_extent_km'].max() <= 80 and stats['duration_days'].max() <= 365:\n",
        "                print(\"PASSED: 2005 data processed correctly\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"FAILED: 2005 data has unrealistic events\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\"No events found for 2005 test region\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED: 2005 test error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting comprehensive fire clustering tests...\\n\")\n",
        "\n",
        "    main_tests_passed = test_fire_clustering_quality()\n",
        "\n",
        "    if main_tests_passed:\n",
        "        robustness_passed = test_different_year()\n",
        "\n",
        "        if robustness_passed:\n",
        "            print(\"\\nALL TESTS PASSED\")\n",
        "        else:\n",
        "            print(\"\\nMain tests passed but robustness test failed.\")\n",
        "    else:\n",
        "        print(\"\\nMain tests failed. System needs fixes before production.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhobtKQ4Ixh9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752626930356,
          "user_tz": 240,
          "elapsed": 40266,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c89e8893-95cb-45d2-b5d8-10957b8b849f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive fire clustering tests...\n",
            "\n",
            "FIRE CLUSTERING QUALITY TESTS\n",
            "==================================================\n",
            "Loaded results: 808,558 data points, 64,212 events\n",
            "\n",
            "==================================================\n",
            "TEST 1: SPATIAL REALISM\n",
            "==================================================\n",
            "Spatial extent distribution:\n",
            "  [0, 1): 50,600 events (78.8%)\n",
            "  [1, 5): 10,670 events (16.6%)\n",
            "  [5, 10): 1,748 events (2.7%)\n",
            "  [10, 20): 949 events (1.5%)\n",
            "  [20, 50): 228 events (0.4%)\n",
            "  [50, 80): 17 events (0.0%)\n",
            "  [80, 120): 0 events (0.0%)\n",
            "  [120, 1000): 0 events (0.0%)\n",
            "\n",
            "CRITICAL SPATIAL TESTS:\n",
            "  Events >80km: 0 (should be 0)\n",
            "  Events >120km: 0 (MUST be 0)\n",
            "  Max extent: 70.1 km\n",
            "  95th percentile: 4.7 km\n",
            "PASSED: No impossible mega-fires\n",
            "\n",
            "==================================================\n",
            "TEST 2: TEMPORAL REALISM\n",
            "==================================================\n",
            "Duration distribution:\n",
            "  [0, 1): 0 events (0.0%)\n",
            "  [1, 2): 50,856 events (79.2%)\n",
            "  [2, 3): 2,803 events (4.4%)\n",
            "  [3, 5): 4,418 events (6.9%)\n",
            "  [5, 7): 2,644 events (4.1%)\n",
            "  [7, 10): 2,315 events (3.6%)\n",
            "  [10, 15): 751 events (1.2%)\n",
            "  [15, 30): 332 events (0.5%)\n",
            "  [30, 365): 93 events (0.1%)\n",
            "\n",
            "CRITICAL TEMPORAL TESTS:\n",
            "  Events >30 days: 93 (should be rare)\n",
            "  Events >365 days: 0 (MUST be 0)\n",
            "  Max duration: 73 days\n",
            "  95th percentile: 7.0 days\n",
            "PASSED: No impossible year-long fires\n",
            "\n",
            "==================================================\n",
            "TEST 3: DATA INTEGRITY\n",
            "==================================================\n",
            "Event size mismatches: 0 (should be 0)\n",
            "Orphaned events in stats: 0 (should be 0)\n",
            "Missing events from stats: 0 (should be 0)\n",
            "Null coordinates: 0 (should be 0)\n",
            "Null dates: 0 (should be 0)\n",
            "PASSED: Data integrity checks\n",
            "\n",
            "==================================================\n",
            "TEST 4: GEOGRAPHIC COVERAGE\n",
            "==================================================\n",
            "Regional summary:\n",
            "              fire_event_id spatial_extent_km        duration_days      \\\n",
            "                      count              mean    max          mean max   \n",
            "region                                                                   \n",
            "Great_Plains          30207              1.42  65.56          2.17  73   \n",
            "Midwest                1580              0.40  20.95          1.41  42   \n",
            "Mountain_West          2175              1.75  66.37          2.13  53   \n",
            "Northeast              1158              0.44  14.00          2.10  59   \n",
            "Pacific_West           2594              1.94  64.71          2.58  54   \n",
            "South_Central         15512              0.46  70.09          1.48  68   \n",
            "South_Texas            1819              0.51  29.33          2.30  43   \n",
            "Southeast              9167              0.38  22.50          1.64  71   \n",
            "\n",
            "              num_points  \n",
            "                     sum  \n",
            "region                    \n",
            "Great_Plains      246744  \n",
            "Midwest             8706  \n",
            "Mountain_West     162847  \n",
            "Northeast           7037  \n",
            "Pacific_West      250952  \n",
            "South_Central      51419  \n",
            "South_Texas        20945  \n",
            "Southeast          59908  \n",
            "PASSED: All regions represented\n",
            "\n",
            "==================================================\n",
            "TEST 5: EDGE CASE VALIDATION\n",
            "==================================================\n",
            "Single-point events: 38,220 (59.5%)\n",
            "Same-day events: 50,856 (79.2%)\n",
            "Point fires (<1m): 38,277 (59.6%)\n",
            "\n",
            "==================================================\n",
            "TEST 6: CLUSTERING EFFECTIVENESS\n",
            "==================================================\n",
            "Clustering efficiency:\n",
            "  Total points: 808,558\n",
            "  Total events: 64,212\n",
            "  Average points per event: 12.6\n",
            "  Reduction factor: 12.6x\n",
            "\n",
            "Event size distribution:\n",
            "  1 points: 38,220 events (59.5%)\n",
            "  2 points: 2,196 events (3.4%)\n",
            "  3 points: 4,040 events (6.3%)\n",
            "  4 points: 4,627 events (7.2%)\n",
            "  5 points: 1,489 events (2.3%)\n",
            "  >100 points: 785 events (1.2%)\n",
            "PASSED: Effective clustering achieved\n",
            "\n",
            "==================================================\n",
            "OVERALL TEST RESULTS\n",
            "==================================================\n",
            "  • 64,212 realistic fire events\n",
            "  • 1.0 km average spatial extent\n",
            "  • 1.9 days average duration\n",
            "  • 70.1 km maximum extent (realistic)\n",
            "\n",
            "==================================================\n",
            "ROBUSTNESS TEST: Different Year\n",
            "==================================================\n",
            "Testing system on year 2005 (subset) to ensure robustness...\n",
            "\n",
            "Processing region: Southeast\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.1, 'lon_max': -80.0}\n",
            "   Parameters: eps=0.2km (0.000031 rad), temporal=1d, min_samples=3\n",
            "   Found 33,408 records\n",
            "   Found 3759 fire events in Southeast\n",
            "\n",
            "Applying realistic fire event limits:\n",
            "  Spatial limit: 80km (auto-split), 120km (hard fail)\n",
            "  Temporal limit: 10 days (split in time)\n",
            "  Events >80km (auto-split): 0\n",
            "  Events >120km (hard fail): 0\n",
            "  Events >10 days (time-split): 170\n",
            "\n",
            "Fixing 170 events...\n",
            "  Progress: 0/170 events processed...\n",
            "  Progress: 100/170 events processed...\n",
            "Realistic limits applied: 3759 → 4256 events\n",
            "2005 Southeast test successful:\n",
            "  Events found: 4256\n",
            "  Max spatial extent: 16.8 km\n",
            "  Max duration: 51 days\n",
            "  Average extent: 0.4 km\n",
            "PASSED: 2005 data processed correctly\n",
            "\n",
            "ALL TESTS PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bigquery_tables_from_csv(project_id=\"code-for-planet\", dataset_id=\"emission_db\", year=2006):\n",
        "    \"\"\"\n",
        "    Create BigQuery tables directly from the saved clustering CSV results\n",
        "    \"\"\"\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # load from csv\n",
        "    print(\"Loading clustering results from CSV...\")\n",
        "\n",
        "    try:\n",
        "        fire_events = pd.read_csv(f'fire_events_{year}_final.csv')\n",
        "        print(f\"Loaded {len(fire_events):,} clustered fire event records\")\n",
        "        print(f\"Covering {len(fire_events['fire_event_id'].unique())} unique fire events\")\n",
        "\n",
        "        # sample\n",
        "        print(f\"Columns in CSV: {list(fire_events.columns)}\")\n",
        "        print(f\"Sample data:\")\n",
        "        print(fire_events.head(3))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: fire_events_{year}_final.csv not found!\")\n",
        "        print(\"Please run the clustering algorithm first to generate this file.\")\n",
        "        return None\n",
        "\n",
        "    # create mapping table and join with original data\n",
        "    print(\"\\n Creating enhanced emission table with fire_event_id...\")\n",
        "\n",
        "    # mapping from clustered results\n",
        "    id_mapping = fire_events[['id', 'fire_event_id', 'region']].copy()\n",
        "\n",
        "    # mapping - BigQuery as a temp table\n",
        "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "    mapping_table_id = f\"{project_id}.{dataset_id}.temp_fire_event_mapping_{year}\"\n",
        "\n",
        "    job = client.load_table_from_dataframe(id_mapping, mapping_table_id, job_config=job_config)\n",
        "    job.result()\n",
        "    print(f\"Uploaded mapping table with {len(id_mapping):,} records\")\n",
        "\n",
        "    # creat table with fire_event_id\n",
        "    enhanced_table_name = f\"emission_{year}_with_fire_events\"\n",
        "    enhanced_table_id = f\"{project_id}.{dataset_id}.{enhanced_table_name}\"\n",
        "\n",
        "    create_enhanced_table_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{enhanced_table_id}` AS\n",
        "    SELECT\n",
        "        e.*,\n",
        "        COALESCE(m.fire_event_id, -1) as fire_event_id,\n",
        "        m.region as fire_region,\n",
        "        CASE\n",
        "            WHEN m.fire_event_id IS NOT NULL THEN 'clustered'\n",
        "            ELSE 'isolated'\n",
        "        END as clustering_status\n",
        "    FROM `{project_id}.{dataset_id}.emission_{year}` e\n",
        "    LEFT JOIN `{mapping_table_id}` m\n",
        "    ON e.id = m.id\n",
        "    \"\"\"\n",
        "\n",
        "    job = client.query(create_enhanced_table_query)\n",
        "    job.result()\n",
        "    print(f\"Created {enhanced_table_name} table\")\n",
        "\n",
        "    # add derived clustering metrics\n",
        "    print(\"\\nAdding derived clustering metrics...\")\n",
        "\n",
        "    add_metrics_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{enhanced_table_id}` AS\n",
        "    WITH fire_event_metrics AS (\n",
        "        SELECT\n",
        "            fire_event_id,\n",
        "            COUNT(*) as event_size_points,\n",
        "            DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1 as event_duration_days,\n",
        "            MIN(fire_date) as event_start_date,\n",
        "            MAX(fire_date) as event_end_date,\n",
        "            -- Calculate spatial extent using Haversine formula approximation\n",
        "            ST_DISTANCE(\n",
        "                ST_GEOGPOINT(MIN(longitude), MIN(latitude)),\n",
        "                ST_GEOGPOINT(MAX(longitude), MAX(latitude))\n",
        "            ) / 1000 as event_spatial_extent_km,\n",
        "            -- Calculate centroid\n",
        "            AVG(longitude) as event_centroid_lon,\n",
        "            AVG(latitude) as event_centroid_lat,\n",
        "            -- Calculate total emissions per event\n",
        "            SUM(COALESCE(ECO2, 0)) as event_total_ECO2,\n",
        "            SUM(COALESCE(area_burned, 0)) as event_total_area_burned,\n",
        "            -- Calculate event spread rate\n",
        "            CASE\n",
        "                WHEN DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1 > 1 THEN\n",
        "                    ST_DISTANCE(\n",
        "                        ST_GEOGPOINT(MIN(longitude), MIN(latitude)),\n",
        "                        ST_GEOGPOINT(MAX(longitude), MAX(latitude))\n",
        "                    ) / 1000 / (DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1)\n",
        "                ELSE 0\n",
        "            END as event_spread_rate_km_per_day\n",
        "        FROM `{enhanced_table_id}`\n",
        "        WHERE fire_event_id != -1\n",
        "        GROUP BY fire_event_id\n",
        "    ),\n",
        "    point_level_metrics AS (\n",
        "        SELECT\n",
        "            e.*,\n",
        "            -- Distance from each point to event centroid\n",
        "            CASE\n",
        "                WHEN e.fire_event_id != -1 THEN\n",
        "                    ST_DISTANCE(\n",
        "                        ST_GEOGPOINT(e.longitude, e.latitude),\n",
        "                        ST_GEOGPOINT(m.event_centroid_lon, m.event_centroid_lat)\n",
        "                    ) / 1000\n",
        "                ELSE NULL\n",
        "            END as distance_to_event_centroid_km,\n",
        "            -- Days from event start\n",
        "            CASE\n",
        "                WHEN e.fire_event_id != -1 THEN\n",
        "                    DATE_DIFF(e.fire_date, m.event_start_date, DAY)\n",
        "                ELSE NULL\n",
        "            END as days_from_event_start,\n",
        "            -- Add event-level metrics\n",
        "            m.event_size_points,\n",
        "            m.event_duration_days,\n",
        "            m.event_start_date,\n",
        "            m.event_end_date,\n",
        "            m.event_spatial_extent_km,\n",
        "            m.event_centroid_lon,\n",
        "            m.event_centroid_lat,\n",
        "            m.event_total_ECO2,\n",
        "            m.event_total_area_burned,\n",
        "            m.event_spread_rate_km_per_day,\n",
        "            -- Quality flags\n",
        "            CASE\n",
        "                WHEN e.fire_event_id = -1 THEN 'isolated'\n",
        "                WHEN m.event_spatial_extent_km > 80 THEN 'large_extent'\n",
        "                WHEN m.event_duration_days > 30 THEN 'long_duration'\n",
        "                WHEN m.event_spread_rate_km_per_day > 10 THEN 'fast_spread'\n",
        "                ELSE 'normal'\n",
        "            END as fire_event_quality_flag\n",
        "        FROM `{enhanced_table_id}` e\n",
        "        LEFT JOIN fire_event_metrics m ON e.fire_event_id = m.fire_event_id\n",
        "    )\n",
        "    SELECT * FROM point_level_metrics\n",
        "    \"\"\"\n",
        "\n",
        "    job = client.query(add_metrics_query)\n",
        "    job.result()\n",
        "    print(\"Added derived clustering metrics\")\n",
        "\n",
        "    # create summary statistics table\n",
        "    print(\"\\nCreating fire events summary table...\")\n",
        "\n",
        "    summary_table_name = f\"fire_events_{year}_summary\"\n",
        "    summary_table_id = f\"{project_id}.{dataset_id}.{summary_table_name}\"\n",
        "\n",
        "    create_summary_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{summary_table_id}` AS\n",
        "    SELECT\n",
        "        fire_event_id,\n",
        "        fire_region as region,\n",
        "        COUNT(*) as num_points,\n",
        "        MIN(fire_date) as start_date,\n",
        "        MAX(fire_date) as end_date,\n",
        "        DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1 as duration_days,\n",
        "        MIN(longitude) as min_lon,\n",
        "        MAX(longitude) as max_lon,\n",
        "        MIN(latitude) as min_lat,\n",
        "        MAX(latitude) as max_lat,\n",
        "        SUM(COALESCE(area_burned, 0)) as total_area_burned,\n",
        "        SUM(COALESCE(consumed_fuel, 0)) as total_consumed_fuel,\n",
        "        SUM(COALESCE(ECO2, 0)) as total_ECO2,\n",
        "        SUM(COALESCE(ECO, 0)) as total_ECO,\n",
        "        SUM(COALESCE(ECH4, 0)) as total_ECH4,\n",
        "        SUM(COALESCE(EPM2_5, 0)) as total_EPM2_5,\n",
        "        -- Use the derived metrics from the enhanced table\n",
        "        AVG(event_spatial_extent_km) as spatial_extent_km,\n",
        "        AVG(event_spread_rate_km_per_day) as spread_rate_km_per_day,\n",
        "        AVG(event_centroid_lon) as centroid_lon,\n",
        "        AVG(event_centroid_lat) as centroid_lat,\n",
        "        -- Quality flags summary\n",
        "        COUNT(CASE WHEN fire_event_quality_flag = 'large_extent' THEN 1 END) as large_extent_points,\n",
        "        COUNT(CASE WHEN fire_event_quality_flag = 'long_duration' THEN 1 END) as long_duration_points,\n",
        "        COUNT(CASE WHEN fire_event_quality_flag = 'fast_spread' THEN 1 END) as fast_spread_points,\n",
        "        CURRENT_TIMESTAMP() as created_timestamp\n",
        "    FROM `{enhanced_table_id}`\n",
        "    WHERE fire_event_id != -1\n",
        "    GROUP BY fire_event_id, fire_region\n",
        "    ORDER BY fire_event_id\n",
        "    \"\"\"\n",
        "\n",
        "    job = client.query(create_summary_query)\n",
        "    job.result()\n",
        "\n",
        "    # looking at coverage difference\n",
        "    print(\"\\nStep 5: Verifying results and investigating coverage...\")\n",
        "\n",
        "    # original emission table count\n",
        "    original_count_query = f\"\"\"\n",
        "    SELECT COUNT(*) as total_original\n",
        "    FROM `{project_id}.{dataset_id}.emission_{year}`\n",
        "    WHERE longitude IS NOT NULL\n",
        "    AND latitude IS NOT NULL\n",
        "    AND fire_date IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "    original_result = client.query(original_count_query).to_dataframe()\n",
        "    original_count = original_result['total_original'].iloc[0]\n",
        "\n",
        "    # csv counts\n",
        "    csv_count = len(fire_events)\n",
        "\n",
        "    # table results\n",
        "    verification_query = f\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(CASE WHEN fire_event_id != -1 THEN 1 END) as records_with_fire_event,\n",
        "        COUNT(DISTINCT fire_event_id) - 1 as unique_fire_events,\n",
        "        COUNT(CASE WHEN fire_event_id = -1 THEN 1 END) as unassigned_records\n",
        "    FROM `{enhanced_table_id}`\n",
        "    \"\"\"\n",
        "\n",
        "    verification_result = client.query(verification_query).to_dataframe()\n",
        "\n",
        "    print(\"VERIFICATION RESULTS:\")\n",
        "    print(f\"Original emission table (filtered): {original_count:,}\")\n",
        "    print(f\"CSV clustering results: {csv_count:,}\")\n",
        "    print(f\"Enhanced table total: {verification_result['total_records'].iloc[0]:,}\")\n",
        "    print(f\"Records assigned to fire events: {verification_result['records_with_fire_event'].iloc[0]:,}\")\n",
        "    print(f\"Unique fire events: {verification_result['unique_fire_events'].iloc[0]:,}\")\n",
        "    print(f\"Unassigned records (noise): {verification_result['unassigned_records'].iloc[0]:,}\")\n",
        "\n",
        "    # coverage %s\n",
        "    total_enhanced = verification_result['total_records'].iloc[0]\n",
        "    assigned = verification_result['records_with_fire_event'].iloc[0]\n",
        "    coverage_pct = (assigned / total_enhanced) * 100\n",
        "    csv_coverage_pct = (csv_count / original_count) * 100\n",
        "\n",
        "    print(f\"\\nCOVERAGE ANALYSIS:\")\n",
        "    print(f\"CSV coverage vs original: {csv_coverage_pct:.1f}%\")\n",
        "    print(f\"Enhanced table coverage: {coverage_pct:.1f}%\")\n",
        "    print(f\"Difference: {csv_coverage_pct - coverage_pct:.1f} percentage points\")\n",
        "\n",
        "    # potential issues (QC)\n",
        "    missing_records = original_count - total_enhanced\n",
        "    if missing_records > 0:\n",
        "        print(f\"\\nPOTENTIAL ISSUE: {missing_records:,} records missing from enhanced table\")\n",
        "        print(\"This could be due to:\")\n",
        "        print(\"- Different filtering criteria between clustering and BigQuery\")\n",
        "        print(\"- Data type conversion issues during join\")\n",
        "        print(\"- ID mismatches between CSV and original table\")\n",
        "\n",
        "        # ID mismatches\n",
        "        id_check_query = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(DISTINCT e.id) as original_ids,\n",
        "            COUNT(DISTINCT m.id) as mapping_ids,\n",
        "            COUNT(DISTINCT e.id) - COUNT(DISTINCT m.id) as missing_from_mapping\n",
        "        FROM `{project_id}.{dataset_id}.emission_{year}` e\n",
        "        LEFT JOIN `{mapping_table_id}` m ON e.id = m.id\n",
        "        WHERE e.longitude IS NOT NULL\n",
        "        AND e.latitude IS NOT NULL\n",
        "        AND e.fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        id_check_result = client.query(id_check_query).to_dataframe()\n",
        "        missing_ids = id_check_result['missing_from_mapping'].iloc[0]\n",
        "\n",
        "        if missing_ids > 0:\n",
        "            print(f\"- {missing_ids:,} IDs from original table not found in CSV mapping\")\n",
        "    else:\n",
        "        print(\"\\nTable counts match - coverage difference is within the enhanced table\")\n",
        "\n",
        "    # check if the difference is noise/isolated points\n",
        "    if abs(csv_coverage_pct - coverage_pct) < 1.0:\n",
        "        print(\"Coverage difference is small (<1%) - likely due to minor filtering differences\")\n",
        "    else:\n",
        "        print(\"Coverage difference is significant - investigation needed\")\n",
        "\n",
        "    # summary table count\n",
        "    count_query = f\"SELECT COUNT(*) as num_events FROM `{summary_table_id}`\"\n",
        "    count_result = client.query(count_query).to_dataframe()\n",
        "\n",
        "    # clean up temp table\n",
        "    print(\"\\nCleaning up temporary table...\")\n",
        "    cleanup_query = f\"DROP TABLE `{mapping_table_id}`\"\n",
        "    client.query(cleanup_query).result()\n",
        "    print(\"Temporary mapping table removed\")\n",
        "\n",
        "    # sample results\n",
        "    print(\"\\nSample of data...\")\n",
        "    sample_query = f\"\"\"\n",
        "    SELECT\n",
        "        id, fire_event_id, fire_region, clustering_status,\n",
        "        event_size_points, event_duration_days, event_spatial_extent_km,\n",
        "        fire_event_quality_flag, area_burned, ECO2\n",
        "    FROM `{enhanced_table_id}`\n",
        "    WHERE fire_event_id != -1\n",
        "    ORDER BY fire_event_id, fire_date\n",
        "    LIMIT 5\n",
        "    \"\"\"\n",
        "\n",
        "    sample_result = client.query(sample_query).to_dataframe()\n",
        "    print(sample_result)\n",
        "\n",
        "    # summary by region\n",
        "    print(\"\\nFire events by region:\")\n",
        "    region_query = f\"\"\"\n",
        "    SELECT\n",
        "        region,\n",
        "        COUNT(*) as num_events,\n",
        "        AVG(spatial_extent_km) as avg_extent_km,\n",
        "        AVG(duration_days) as avg_duration_days\n",
        "    FROM `{summary_table_id}`\n",
        "    GROUP BY region\n",
        "    ORDER BY num_events DESC\n",
        "    \"\"\"\n",
        "\n",
        "    region_result = client.query(region_query).to_dataframe()\n",
        "    print(region_result)\n",
        "\n",
        "    print(f\"\\nSUCCESS! Created BigQuery tables:\")\n",
        "    print(f\"Main table: {enhanced_table_name} ({total_enhanced:,} records)\")\n",
        "    print(f\"Summary table: {summary_table_name} ({count_result['num_events'].iloc[0]:,} fire events)\")\n",
        "\n",
        "    return enhanced_table_id, summary_table_id\n",
        "\n",
        "# main that uses csv\n",
        "def main():\n",
        "    print(\"CREATING BIGQUERY TABLES FROM EXISTING CSV RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        enhanced_table_id, summary_table_id = create_bigquery_tables_from_csv(\n",
        "            project_id=\"code-for-planet\",\n",
        "            dataset_id=\"emission_db\",\n",
        "            year=2006\n",
        "        )\n",
        "\n",
        "        print(\"\\nDone - Table Created.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"Make sure fire_events_2006_final.csv exists in your current directory\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccttUrrCVQS9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752626974690,
          "user_tz": 240,
          "elapsed": 44337,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "26f6bc01-0ce4-41e2-9441-a9f178bbcc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATING BIGQUERY TABLES FROM EXISTING CSV RESULTS\n",
            "======================================================================\n",
            "Loading clustering results from CSV...\n",
            "Loaded 808,558 clustered fire event records\n",
            "Covering 64212 unique fire events\n",
            "Columns in CSV: ['id', 'year', 'doy', 'longitude', 'latitude', 'fire_date', 'grid10k', 'covertype', 'fuelcode', 'area_burned', 'consumed_fuel', 'ECO2', 'burn_source', 'burnday_source', 'fire_event_id', 'region']\n",
            "Sample data:\n",
            "       id  year  doy  longitude  latitude   fire_date  grid10k  covertype  \\\n",
            "0  648899  2006    1  -121.4373   44.7098  2006-01-01   111137          3   \n",
            "1  648887  2006    1  -121.4295   44.7067  2006-01-01   111137          3   \n",
            "2  648892  2006    2  -121.4334   44.7082  2006-01-02   111137          3   \n",
            "\n",
            "   fuelcode  area_burned  consumed_fuel         ECO2  burn_source  \\\n",
            "0      1180      62500.0    1657.645804  2575.981579            3   \n",
            "1      1180      62500.0    1335.911902  2076.007096            3   \n",
            "2      1180      62500.0    1453.302180  2258.431588            3   \n",
            "\n",
            "   burnday_source  fire_event_id        region  \n",
            "0              77              0  Pacific_West  \n",
            "1              77              0  Pacific_West  \n",
            "2              77              0  Pacific_West  \n",
            "\n",
            " Creating enhanced emission table with fire_event_id...\n",
            "Uploaded mapping table with 808,558 records\n",
            "Created emission_2006_with_fire_events table\n",
            "\n",
            "Adding derived clustering metrics...\n",
            "Added derived clustering metrics\n",
            "\n",
            "Creating fire events summary table...\n",
            "\n",
            "Step 5: Verifying results and investigating coverage...\n",
            "VERIFICATION RESULTS:\n",
            "Original emission table (filtered): 813,256\n",
            "CSV clustering results: 808,558\n",
            "Enhanced table total: 813,256\n",
            "Records assigned to fire events: 808,558\n",
            "Unique fire events: 64,212\n",
            "Unassigned records (noise): 4,698\n",
            "\n",
            "COVERAGE ANALYSIS:\n",
            "CSV coverage vs original: 99.4%\n",
            "Enhanced table coverage: 99.4%\n",
            "Difference: 0.0 percentage points\n",
            "\n",
            "Table counts match - coverage difference is within the enhanced table\n",
            "Coverage difference is small (<1%) - likely due to minor filtering differences\n",
            "\n",
            "Cleaning up temporary table...\n",
            "Temporary mapping table removed\n",
            "\n",
            "Sample of data...\n",
            "       id  fire_event_id   fire_region clustering_status  event_size_points  \\\n",
            "0  648887              0  Pacific_West         clustered                  6   \n",
            "1  648899              0  Pacific_West         clustered                  6   \n",
            "2  648892              0  Pacific_West         clustered                  6   \n",
            "3  648901              0  Pacific_West         clustered                  6   \n",
            "4  648894              0  Pacific_West         clustered                  6   \n",
            "\n",
            "   event_duration_days  event_spatial_extent_km fire_event_quality_flag  \\\n",
            "0                    3                 0.780104                  normal   \n",
            "1                    3                 0.780104                  normal   \n",
            "2                    3                 0.780104                  normal   \n",
            "3                    3                 0.780104                  normal   \n",
            "4                    3                 0.780104                  normal   \n",
            "\n",
            "   area_burned         ECO2  \n",
            "0      62500.0  2076.007096  \n",
            "1      62500.0  2575.981579  \n",
            "2      62500.0  2258.431588  \n",
            "3      62500.0  2758.406071  \n",
            "4      62500.0  2258.431588  \n",
            "\n",
            "Fire events by region:\n",
            "          region  num_events  avg_extent_km  avg_duration_days\n",
            "0   Great_Plains       30207       1.420103           2.172410\n",
            "1  South_Central       15512       0.455057           1.484335\n",
            "2      Southeast        9167       0.377956           1.635322\n",
            "3   Pacific_West        2594       1.940831           2.577101\n",
            "4  Mountain_West        2175       1.747670           2.128276\n",
            "5    South_Texas        1819       0.506712           2.298516\n",
            "6        Midwest        1580       0.399626           1.414557\n",
            "7      Northeast        1158       0.444244           2.102763\n",
            "\n",
            "SUCCESS! Created BigQuery tables:\n",
            "Main table: emission_2006_with_fire_events (813,256 records)\n",
            "Summary table: fire_events_2006_summary (64,212 fire events)\n",
            "\n",
            "Done - Table Created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "# Check if clustered points have metrics populated\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  fire_event_id, fire_region, clustering_status,\n",
        "  event_size_points, event_duration_days, event_spatial_extent_km,\n",
        "  distance_to_event_centroid_km, days_from_event_start,\n",
        "  fire_event_quality_flag\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "WHERE fire_event_id != -1 AND fire_event_id IS NOT NULL\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "result = client.query(query).to_dataframe()\n",
        "print(\"Sample of clustered points:\")\n",
        "print(result)\n",
        "\n",
        "# Also check if we have any non-null metrics at all\n",
        "count_query = \"\"\"\n",
        "SELECT\n",
        "  COUNT(*) as total_rows,\n",
        "  COUNT(event_size_points) as rows_with_size,\n",
        "  COUNT(event_duration_days) as rows_with_duration,\n",
        "  COUNT(distance_to_event_centroid_km) as rows_with_distance\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "WHERE fire_event_id != -1\n",
        "\"\"\"\n",
        "\n",
        "count_result = client.query(count_query).to_dataframe()\n",
        "print(\"\\nMetrics population check:\")\n",
        "print(count_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8D-I-cYaVgS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752626978840,
          "user_tz": 240,
          "elapsed": 4153,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9cfa635f-866c-46ae-cf38-524d525584b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of clustered points:\n",
            "   fire_event_id   fire_region clustering_status  event_size_points  \\\n",
            "0              1  Pacific_West         clustered                  1   \n",
            "1              2  Pacific_West         clustered                  1   \n",
            "2              3  Pacific_West         clustered                  1   \n",
            "3              5  Pacific_West         clustered                  1   \n",
            "4              8  Pacific_West         clustered                  1   \n",
            "5              9  Pacific_West         clustered                  1   \n",
            "6             10  Pacific_West         clustered                  1   \n",
            "7             11  Pacific_West         clustered                  1   \n",
            "8             12  Pacific_West         clustered                  1   \n",
            "9             17  Pacific_West         clustered                  8   \n",
            "\n",
            "   event_duration_days  event_spatial_extent_km  \\\n",
            "0                    1                  0.00000   \n",
            "1                    1                  0.00000   \n",
            "2                    1                  0.00000   \n",
            "3                    1                  0.00000   \n",
            "4                    1                  0.00000   \n",
            "5                    1                  0.00000   \n",
            "6                    1                  0.00000   \n",
            "7                    1                  0.00000   \n",
            "8                    1                  0.00000   \n",
            "9                    3                  1.12993   \n",
            "\n",
            "   distance_to_event_centroid_km  days_from_event_start  \\\n",
            "0                       0.000000                      0   \n",
            "1                       0.000000                      0   \n",
            "2                       0.000000                      0   \n",
            "3                       0.000000                      0   \n",
            "4                       0.000000                      0   \n",
            "5                       0.000000                      0   \n",
            "6                       0.000000                      0   \n",
            "7                       0.000000                      0   \n",
            "8                       0.000000                      0   \n",
            "9                       0.178488                      2   \n",
            "\n",
            "  fire_event_quality_flag  \n",
            "0                  normal  \n",
            "1                  normal  \n",
            "2                  normal  \n",
            "3                  normal  \n",
            "4                  normal  \n",
            "5                  normal  \n",
            "6                  normal  \n",
            "7                  normal  \n",
            "8                  normal  \n",
            "9                  normal  \n",
            "\n",
            "Metrics population check:\n",
            "   total_rows  rows_with_size  rows_with_duration  rows_with_distance\n",
            "0      808558          808558              808558              808558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "print(\"Converting isolated points to unique events...\")\n",
        "\n",
        "# simple approach - just update the fire_event_id and add the flag\n",
        "update_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE `code-for-planet.emission_db.emission_2006_with_fire_events` AS\n",
        "SELECT\n",
        "  -- All original columns except fire_event_id and clustering_status\n",
        "  * EXCEPT(fire_event_id, clustering_status),\n",
        "\n",
        "  -- Updated fire_event_id (convert -1 to unique IDs)\n",
        "  CASE\n",
        "    WHEN fire_event_id = -1 THEN\n",
        "      ROW_NUMBER() OVER (ORDER BY id) + 35488  -- Start after max existing ID\n",
        "    ELSE fire_event_id\n",
        "  END as fire_event_id,\n",
        "\n",
        "  -- Add isolated event flag\n",
        "  CASE WHEN fire_event_id = -1 THEN TRUE ELSE FALSE END as is_isolated_event,\n",
        "\n",
        "  -- Updated clustering status\n",
        "  CASE\n",
        "    WHEN fire_event_id = -1 THEN 'isolated_event'\n",
        "    ELSE clustering_status\n",
        "  END as clustering_status\n",
        "\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "\"\"\"\n",
        "\n",
        "job = client.query(update_query)\n",
        "job.result()\n",
        "print(\"Successfully converted isolated points to unique events!\")\n",
        "\n",
        "# recalculate all the derived metrics for the new events\n",
        "print(\"\\nRecalculating derived metrics for all events...\")\n",
        "\n",
        "recalc_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE `code-for-planet.emission_db.emission_2006_with_fire_events` AS\n",
        "WITH fire_event_metrics AS (\n",
        "  SELECT\n",
        "    fire_event_id,\n",
        "    COUNT(*) as event_size_points,\n",
        "    DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1 as event_duration_days,\n",
        "    MIN(fire_date) as event_start_date,\n",
        "    MAX(fire_date) as event_end_date,\n",
        "    ST_DISTANCE(\n",
        "      ST_GEOGPOINT(MIN(longitude), MIN(latitude)),\n",
        "      ST_GEOGPOINT(MAX(longitude), MAX(latitude))\n",
        "    ) / 1000 as event_spatial_extent_km,\n",
        "    AVG(longitude) as event_centroid_lon,\n",
        "    AVG(latitude) as event_centroid_lat,\n",
        "    SUM(COALESCE(ECO2, 0)) as event_total_ECO2,\n",
        "    SUM(COALESCE(area_burned, 0)) as event_total_area_burned,\n",
        "    CASE\n",
        "      WHEN DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1 > 1 THEN\n",
        "        ST_DISTANCE(\n",
        "          ST_GEOGPOINT(MIN(longitude), MIN(latitude)),\n",
        "          ST_GEOGPOINT(MAX(longitude), MAX(latitude))\n",
        "        ) / 1000 / (DATE_DIFF(MAX(fire_date), MIN(fire_date), DAY) + 1)\n",
        "      ELSE 0\n",
        "    END as event_spread_rate_km_per_day\n",
        "  FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "  GROUP BY fire_event_id\n",
        ")\n",
        "SELECT\n",
        "  -- Original emission data and basic clustering results\n",
        "  e.* EXCEPT(distance_to_event_centroid_km, days_from_event_start,\n",
        "             event_size_points, event_duration_days, event_start_date, event_end_date,\n",
        "             event_spatial_extent_km, event_centroid_lon, event_centroid_lat,\n",
        "             event_total_ECO2, event_total_area_burned, event_spread_rate_km_per_day,\n",
        "             fire_event_quality_flag),\n",
        "\n",
        "  -- Recalculated point-level metrics\n",
        "  CASE\n",
        "    WHEN e.is_isolated_event THEN 0.0\n",
        "    ELSE ST_DISTANCE(\n",
        "      ST_GEOGPOINT(e.longitude, e.latitude),\n",
        "      ST_GEOGPOINT(m.event_centroid_lon, m.event_centroid_lat)\n",
        "    ) / 1000\n",
        "  END as distance_to_event_centroid_km,\n",
        "\n",
        "  CASE\n",
        "    WHEN e.is_isolated_event THEN 0\n",
        "    ELSE DATE_DIFF(e.fire_date, m.event_start_date, DAY)\n",
        "  END as days_from_event_start,\n",
        "\n",
        "  -- Event-level metrics\n",
        "  m.event_size_points,\n",
        "  m.event_duration_days,\n",
        "  m.event_start_date,\n",
        "  m.event_end_date,\n",
        "  m.event_spatial_extent_km,\n",
        "  m.event_centroid_lon,\n",
        "  m.event_centroid_lat,\n",
        "  m.event_total_ECO2,\n",
        "  m.event_total_area_burned,\n",
        "  m.event_spread_rate_km_per_day,\n",
        "\n",
        "  -- Quality flags\n",
        "  CASE\n",
        "    WHEN e.is_isolated_event THEN 'isolated_fire'\n",
        "    WHEN m.event_spatial_extent_km > 80 THEN 'large_extent'\n",
        "    WHEN m.event_duration_days > 30 THEN 'long_duration'\n",
        "    WHEN m.event_spread_rate_km_per_day > 10 THEN 'fast_spread'\n",
        "    ELSE 'normal'\n",
        "  END as fire_event_quality_flag\n",
        "\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events` e\n",
        "LEFT JOIN fire_event_metrics m ON e.fire_event_id = m.fire_event_id\n",
        "\"\"\"\n",
        "\n",
        "# recalc\n",
        "job = client.query(recalc_query)\n",
        "job.result()\n",
        "print(\"Recalculated all derived metrics!\")\n",
        "\n",
        "# verify results\n",
        "print(\"\\nVerifying final results...\")\n",
        "verify_query = \"\"\"\n",
        "SELECT\n",
        "  is_isolated_event,\n",
        "  clustering_status,\n",
        "  COUNT(*) as count,\n",
        "  COUNT(DISTINCT fire_event_id) as unique_events,\n",
        "  MIN(fire_event_id) as min_id,\n",
        "  MAX(fire_event_id) as max_id\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "GROUP BY is_isolated_event, clustering_status\n",
        "ORDER BY is_isolated_event\n",
        "\"\"\"\n",
        "\n",
        "result = client.query(verify_query).to_dataframe()\n",
        "print(\"Final event summary:\")\n",
        "print(result)\n",
        "\n",
        "# check - should be 0 isolated points (-1 values)\n",
        "final_check = \"\"\"\n",
        "SELECT\n",
        "  COUNT(*) as total_records,\n",
        "  COUNT(DISTINCT fire_event_id) as total_unique_events,\n",
        "  COUNT(CASE WHEN fire_event_id = -1 THEN 1 END) as remaining_minus_one_values\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "\"\"\"\n",
        "\n",
        "final_result = client.query(final_check).to_dataframe()\n",
        "print(\"\\nFinal summary:\")\n",
        "print(final_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvBHFREEb7S3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752627010279,
          "user_tz": 240,
          "elapsed": 31441,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c7c915ee-1df9-428a-d08c-1e4cd4c76ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting isolated points to unique events...\n",
            "Successfully converted isolated points to unique events!\n",
            "\n",
            "Recalculating derived metrics for all events...\n",
            "Recalculated all derived metrics!\n",
            "\n",
            "Verifying final results...\n",
            "Final event summary:\n",
            "   is_isolated_event clustering_status   count  unique_events  min_id  max_id\n",
            "0              False         clustered  808558          64212       0   69050\n",
            "1               True    isolated_event    4698           4698   55897  816755\n",
            "\n",
            "Final summary:\n",
            "   total_records  total_unique_events  remaining_minus_one_values\n",
            "0         813256                68781                           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "print(\"Fixing NULL fire_regions for isolated events using original region definitions...\")\n",
        "\n",
        "# update isolated events with fire_regions based on exact original boundaries\n",
        "fix_regions_query = \"\"\"\n",
        "UPDATE `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "SET fire_region = CASE\n",
        "  WHEN latitude >= 32.0 AND latitude < 49.0 AND longitude >= -125.0 AND longitude < -115.0 THEN 'Pacific_West'\n",
        "  WHEN latitude >= 32.0 AND latitude < 49.0 AND longitude >= -115.0 AND longitude < -105.0 THEN 'Mountain_West'\n",
        "  WHEN latitude >= 32.0 AND latitude < 49.0 AND longitude >= -105.0 AND longitude < -95.0 THEN 'Great_Plains'\n",
        "  WHEN latitude >= 25.0 AND latitude < 32.0 AND longitude >= -105.0 AND longitude < -95.0 THEN 'South_Texas'\n",
        "  WHEN latitude >= 25.0 AND latitude < 37.0 AND longitude >= -95.0 AND longitude < -85.1 THEN 'South_Central'\n",
        "  WHEN latitude >= 37.0 AND latitude < 49.0 AND longitude >= -95.0 AND longitude < -85.0 THEN 'Midwest'\n",
        "  WHEN latitude >= 25.0 AND latitude < 37.0 AND longitude >= -85.1 AND longitude < -75.0 THEN 'Southeast'\n",
        "  WHEN latitude >= 37.0 AND latitude < 49.0 AND longitude >= -85.0 AND longitude < -67.0 THEN 'Northeast'\n",
        "  ELSE 'Outside_Regions'\n",
        "END\n",
        "WHERE is_isolated_event = TRUE AND fire_region IS NULL\n",
        "\"\"\"\n",
        "\n",
        "job = client.query(fix_regions_query)\n",
        "job.result()\n",
        "\n",
        "# verify the fix\n",
        "verify_query = \"\"\"\n",
        "SELECT\n",
        "  fire_region,\n",
        "  COUNT(*) as isolated_events\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "WHERE is_isolated_event = TRUE\n",
        "GROUP BY fire_region\n",
        "ORDER BY isolated_events DESC\n",
        "\"\"\"\n",
        "\n",
        "result = client.query(verify_query).to_dataframe()\n",
        "print(\"Fixed! Isolated events by region:\")\n",
        "print(result)\n",
        "\n",
        "# double-check for any remaining NULLs\n",
        "null_check_query = \"\"\"\n",
        "SELECT\n",
        "  COUNT(*) as total_isolated,\n",
        "  COUNT(fire_region) as non_null_regions\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "WHERE is_isolated_event = TRUE\n",
        "\"\"\"\n",
        "\n",
        "null_check = client.query(null_check_query).to_dataframe()\n",
        "print(\"\\nFinal NULL check:\")\n",
        "print(null_check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf0n9HrOeYNm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752627024338,
          "user_tz": 240,
          "elapsed": 14061,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d607cbc4-065e-4ca3-d3a2-cfaeed9f472f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing NULL fire_regions for isolated events using original region definitions...\n",
            "Fixed! Isolated events by region:\n",
            "       fire_region  isolated_events\n",
            "0  Outside_Regions             4230\n",
            "1    South_Central              468\n",
            "\n",
            "Final NULL check:\n",
            "   total_isolated  non_null_regions\n",
            "0            4698              4698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "# NULL check\n",
        "null_check_query = \"\"\"\n",
        "SELECT *\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "WHERE\n",
        "  id IS NULL OR year IS NULL OR doy IS NULL OR longitude IS NULL OR latitude IS NULL OR\n",
        "  grid10k IS NULL OR covertype IS NULL OR fuelcode IS NULL OR area_burned IS NULL OR\n",
        "  prefire_fuel IS NULL OR consumed_fuel IS NULL OR ECO2 IS NULL OR ECO IS NULL OR\n",
        "  ECH4 IS NULL OR EPM2_5 IS NULL OR cwd_frac IS NULL OR duff_frac IS NULL OR\n",
        "  fuel_moisture_class IS NULL OR burn_source IS NULL OR burnday_source IS NULL OR\n",
        "  BSEV IS NULL OR BSEV_flag IS NULL OR fire_date IS NULL OR bi_value IS NULL OR\n",
        "  fm100_value IS NULL OR pet_value IS NULL OR fm1000_value IS NULL OR pr_value IS NULL OR\n",
        "  rmax_value IS NULL OR rmin_value IS NULL OR sph_value IS NULL OR srad_value IS NULL OR\n",
        "  tmmn_value IS NULL OR th_value IS NULL OR tmmx_value IS NULL OR vpd_value IS NULL OR\n",
        "  vs_value IS NULL OR fire_event_id IS NULL OR fire_region IS NULL OR\n",
        "  clustering_status IS NULL OR is_isolated_event IS NULL OR\n",
        "  distance_to_event_centroid_km IS NULL OR days_from_event_start IS NULL OR\n",
        "  event_size_points IS NULL OR event_duration_days IS NULL OR event_start_date IS NULL OR\n",
        "  event_end_date IS NULL OR event_spatial_extent_km IS NULL OR event_centroid_lon IS NULL OR\n",
        "  event_centroid_lat IS NULL OR event_total_ECO2 IS NULL OR event_total_area_burned IS NULL OR\n",
        "  event_spread_rate_km_per_day IS NULL OR fire_event_quality_flag IS NULL\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "null_result = client.query(null_check_query).to_dataframe()\n",
        "print(f\"Found {len(null_result)} rows with NULL values\")\n",
        "if len(null_result) > 0:\n",
        "    print(\"\\nFirst few rows with NULLs:\")\n",
        "    print(null_result)\n",
        "else:\n",
        "    print(\"No NULL values found in any columns!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TTZsX6UfLqz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752627026257,
          "user_tz": 240,
          "elapsed": 1921,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "3b70b0d9-2a8b-483d-f42d-ffb8fb956d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 rows with NULL values\n",
            "\n",
            "First few rows with NULLs:\n",
            "       id  year  doy  longitude  latitude  grid10k  covertype  fuelcode  \\\n",
            "0  595555  2006  255  -120.4958   49.0878   131902          0         0   \n",
            "1  595556  2006  255  -120.4925   49.0884   131902          0         0   \n",
            "2  595558  2006  253  -120.5097   49.0877   131902          0         0   \n",
            "3  595559  2006  256  -120.5065   49.0883   131902          0         0   \n",
            "4  595560  2006  256  -120.5032   49.0889   131902          0         0   \n",
            "\n",
            "   area_burned  prefire_fuel  ...  event_duration_days  event_start_date  \\\n",
            "0          0.0           0.0  ...                    1        2006-09-12   \n",
            "1          0.0           0.0  ...                    1        2006-09-12   \n",
            "2      62500.0           0.0  ...                    1        2006-09-10   \n",
            "3      62500.0           0.0  ...                    1        2006-09-13   \n",
            "4      62500.0           0.0  ...                    1        2006-09-13   \n",
            "\n",
            "   event_end_date  event_spatial_extent_km  event_centroid_lon  \\\n",
            "0      2006-09-12                      0.0           -120.4958   \n",
            "1      2006-09-12                      0.0           -120.4925   \n",
            "2      2006-09-10                      0.0           -120.5097   \n",
            "3      2006-09-13                      0.0           -120.5065   \n",
            "4      2006-09-13                      0.0           -120.5032   \n",
            "\n",
            "   event_centroid_lat  event_total_ECO2  event_total_area_burned  \\\n",
            "0             49.0878               0.0                      0.0   \n",
            "1             49.0884               0.0                      0.0   \n",
            "2             49.0877               0.0                  62500.0   \n",
            "3             49.0883               0.0                  62500.0   \n",
            "4             49.0889               0.0                  62500.0   \n",
            "\n",
            "   event_spread_rate_km_per_day  fire_event_quality_flag  \n",
            "0                           0.0            isolated_fire  \n",
            "1                           0.0            isolated_fire  \n",
            "2                           0.0            isolated_fire  \n",
            "3                           0.0            isolated_fire  \n",
            "4                           0.0            isolated_fire  \n",
            "\n",
            "[5 rows x 54 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=\"code-for-planet\")\n",
        "\n",
        "# actual fire_event_id distribution\n",
        "id_distribution_query = \"\"\"\n",
        "SELECT\n",
        "  MIN(fire_event_id) as min_id,\n",
        "  MAX(fire_event_id) as max_id,\n",
        "  COUNT(DISTINCT fire_event_id) as unique_events,\n",
        "  COUNT(*) as total_records\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "\"\"\"\n",
        "\n",
        "distribution = client.query(id_distribution_query).to_dataframe()\n",
        "print(\"Fire Event ID Distribution:\")\n",
        "print(distribution)\n",
        "\n",
        "# check for gaps in the sequence\n",
        "gap_check_query = \"\"\"\n",
        "WITH consecutive_ids AS (\n",
        "  SELECT DISTINCT fire_event_id\n",
        "  FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "  ORDER BY fire_event_id\n",
        "),\n",
        "gaps AS (\n",
        "  SELECT\n",
        "    fire_event_id,\n",
        "    LAG(fire_event_id) OVER (ORDER BY fire_event_id) as prev_id,\n",
        "    fire_event_id - LAG(fire_event_id) OVER (ORDER BY fire_event_id) as gap\n",
        "  FROM consecutive_ids\n",
        ")\n",
        "SELECT fire_event_id, prev_id, gap\n",
        "FROM gaps\n",
        "WHERE gap > 1\n",
        "ORDER BY fire_event_id\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "gaps = client.query(gap_check_query).to_dataframe()\n",
        "print(\"\\nFirst 10 gaps in fire_event_id sequence:\")\n",
        "print(gaps)\n",
        "\n",
        "lowest_ids_query = \"\"\"\n",
        "SELECT DISTINCT fire_event_id\n",
        "FROM `code-for-planet.emission_db.emission_2006_with_fire_events`\n",
        "ORDER BY fire_event_id\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "lowest = client.query(lowest_ids_query).to_dataframe()\n",
        "print(\"\\nLowest 20 fire_event_ids:\")\n",
        "print(lowest['fire_event_id'].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlCdJJeqf0XV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752627032759,
          "user_tz": 240,
          "elapsed": 6504,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "07476569-2abf-4faa-a837-2de9c357e6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fire Event ID Distribution:\n",
            "   min_id  max_id  unique_events  total_records\n",
            "0       0  816755          68781         813256\n",
            "\n",
            "First 10 gaps in fire_event_id sequence:\n",
            "   fire_event_id  prev_id  gap\n",
            "0             16       13    3\n",
            "1             21       19    2\n",
            "2             49       47    2\n",
            "3            699      696    3\n",
            "4            701      699    2\n",
            "5            708      706    2\n",
            "6            711      708    3\n",
            "7            727      725    2\n",
            "8            730      728    2\n",
            "9            733      731    2\n",
            "\n",
            "Lowest 20 fire_event_ids:\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22]\n"
          ]
        }
      ]
    }
  ]
}