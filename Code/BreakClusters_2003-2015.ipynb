{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from google.cloud import bigquery\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class HierarchicalFireEventClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371  # earthss radius in kilometers\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        return R * c\n",
        "\n",
        "    def define_geographic_regions(self):\n",
        "        \"\"\"Define geographic regions\"\"\"\n",
        "        regions = {\n",
        "            'Pacific_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0},\n",
        "            'Mountain_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0},\n",
        "            'Great_Plains': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Texas': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Central': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Midwest': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Southeast': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.0, 'lon_max': -75.0},\n",
        "            'Northeast': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0},\n",
        "            'Florida': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.0, 'lon_max': -80.0}\n",
        "        }\n",
        "        return regions\n",
        "\n",
        "    def get_region_parameters(self, region_name):\n",
        "        \"\"\"Get parameters with proper radians conversion\"\"\"\n",
        "\n",
        "        cropland_regions = ['Great_Plains', 'Midwest', 'South_Central']\n",
        "\n",
        "        region_params_km = {\n",
        "            'Pacific_West': {'spatial_km': 5.0, 'temporal_days': 5, 'min_samples': 5},\n",
        "            'Mountain_West': {'spatial_km': 5.0, 'temporal_days': 5, 'min_samples': 5},\n",
        "            'Great_Plains': {'spatial_km': 3.0, 'temporal_days': 5, 'min_samples': 6},      # Reduced from 8\n",
        "            'South_Texas': {'spatial_km': 3.0, 'temporal_days': 5, 'min_samples': 5},\n",
        "            'South_Central': {'spatial_km': 2.0, 'temporal_days': 5, 'min_samples': 7},     # Reduced from 10\n",
        "            'Midwest': {'spatial_km': 3.0, 'temporal_days': 5, 'min_samples': 6},          # Reduced from 8\n",
        "            'Southeast': {'spatial_km': 2.0, 'temporal_days': 5, 'min_samples': 5},\n",
        "            'Northeast': {'spatial_km': 5.0, 'temporal_days': 5, 'min_samples': 5},\n",
        "            'Florida': {'spatial_km': 2.0, 'temporal_days': 5, 'min_samples': 5}\n",
        "        }\n",
        "\n",
        "        params_km = region_params_km.get(region_name,\n",
        "                                       {'spatial_km': 5.0, 'temporal_days': 3, 'min_samples': 5})\n",
        "\n",
        "        # convert to radians\n",
        "        spatial_degrees = params_km['spatial_km'] / 111.0\n",
        "        spatial_radians = np.radians(spatial_degrees)\n",
        "\n",
        "        return {\n",
        "            'spatial_eps_radians': spatial_radians,\n",
        "            'spatial_km': params_km['spatial_km'],\n",
        "            'temporal_days': params_km['temporal_days'],\n",
        "            'min_samples': params_km['min_samples']\n",
        "        }\n",
        "\n",
        "    def _get_cluster_extent(self, cluster_data):\n",
        "        \"\"\"Calculate spatial extent of cluster in km\"\"\"\n",
        "        min_lat = cluster_data['latitude'].min()\n",
        "        max_lat = cluster_data['latitude'].max()\n",
        "        min_lon = cluster_data['longitude'].min()\n",
        "        max_lon = cluster_data['longitude'].max()\n",
        "\n",
        "        return self.haversine_distance(min_lat, min_lon, max_lat, max_lon)\n",
        "\n",
        "    def _break_up_large_cluster(self, cluster_data, max_size_km=25):\n",
        "        \"\"\"Try to break up large clusters using tighter clustering\"\"\"\n",
        "\n",
        "        initial_extent = self._get_cluster_extent(cluster_data)\n",
        "        print(f\"       Attempting to break up {initial_extent:.1f}km cluster...\")\n",
        "\n",
        "        # try progressively tighter clustering\n",
        "        sub_clusters = []\n",
        "\n",
        "        # Tighter spatial clustering\n",
        "        coords = cluster_data[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        #  tighter spatial\n",
        "        tight_spatial_eps = np.radians(2.0 / 111.0)  # 2km radius instead of 1km\n",
        "\n",
        "        dbscan_tight = DBSCAN(\n",
        "            eps=tight_spatial_eps,\n",
        "            min_samples=4,  # lower min_samples for sub-clustering\n",
        "            metric='haversine'\n",
        "        )\n",
        "\n",
        "        tight_labels = dbscan_tight.fit_predict(coords_rad)\n",
        "        cluster_data['tight_spatial_cluster'] = tight_labels\n",
        "\n",
        "        # temporal refinement on each tight spatial cluster\n",
        "        event_id = 0\n",
        "        for tight_cluster in cluster_data['tight_spatial_cluster'].unique():\n",
        "            if tight_cluster == -1:  # skip noise\n",
        "                continue\n",
        "\n",
        "            tight_cluster_data = cluster_data[cluster_data['tight_spatial_cluster'] == tight_cluster].copy()\n",
        "\n",
        "            # apply temporal splitting with 3-day gaps\n",
        "            tight_cluster_data = tight_cluster_data.sort_values('fire_date')\n",
        "            tight_cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(tight_cluster_data)):\n",
        "                prev_date = tight_cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = tight_cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > 5:  # 5-day gap instead of 3\n",
        "                    current_group += 1\n",
        "\n",
        "                tight_cluster_data.iloc[i, tight_cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            # create sub-events\n",
        "            for temp_group in tight_cluster_data['temp_group'].unique():\n",
        "                sub_event_data = tight_cluster_data[tight_cluster_data['temp_group'] == temp_group].copy()\n",
        "\n",
        "                # check if sub-event is reasonable size\n",
        "                sub_extent = self._get_cluster_extent(sub_event_data)\n",
        "                if sub_extent <= max_size_km:\n",
        "                    sub_event_data['fire_event_id'] = event_id\n",
        "                    sub_event_data = sub_event_data.drop(['tight_spatial_cluster', 'temp_group'], axis=1)\n",
        "                    sub_clusters.append(sub_event_data)\n",
        "                    event_id += 1\n",
        "                else:\n",
        "                    print(f\"         Sub-cluster still too large ({sub_extent:.1f}km), skipping\")\n",
        "\n",
        "        if sub_clusters:\n",
        "            result = pd.concat(sub_clusters, ignore_index=True)\n",
        "            n_sub_events = len(result['fire_event_id'].unique())\n",
        "            print(f\"         Successfully broke into {n_sub_events} sub-events\")\n",
        "            return result\n",
        "        else:\n",
        "            print(f\"         Could not break up cluster, treating as noise\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _cluster_region_data_hierarchical(self, df, params):\n",
        "        \"\"\"Apply hierarchical clustering with breakup logic\"\"\"\n",
        "\n",
        "        # initial spatial clustering\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        dbscan = DBSCAN(\n",
        "            eps=params['spatial_eps_radians'],\n",
        "            min_samples=params['min_samples'],\n",
        "            metric='haversine'\n",
        "        )\n",
        "        spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "        df['spatial_cluster'] = spatial_labels\n",
        "\n",
        "        n_spatial_clusters = len(set(spatial_labels)) - (1 if -1 in spatial_labels else 0)\n",
        "        print(f\"     Initial spatial clusters: {n_spatial_clusters}\")\n",
        "\n",
        "        # process each spatial cluster\n",
        "        fire_events = []\n",
        "        event_id = 0\n",
        "\n",
        "        for spatial_cluster in df['spatial_cluster'].unique():\n",
        "            if spatial_cluster == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_data = df[df['spatial_cluster'] == spatial_cluster].copy()\n",
        "            cluster_data = cluster_data.sort_values('fire_date')\n",
        "\n",
        "            # apply temporal splitting\n",
        "            cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(cluster_data)):\n",
        "                prev_date = cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > params['temporal_days']:\n",
        "                    current_group += 1\n",
        "\n",
        "                cluster_data.iloc[i, cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            # process each temporal group\n",
        "            for temp_group in cluster_data['temp_group'].unique():\n",
        "                group_data = cluster_data[cluster_data['temp_group'] == temp_group].copy()\n",
        "                extent_km = self._get_cluster_extent(group_data)\n",
        "\n",
        "                # apply hierarchical logic\n",
        "                if extent_km <= 25:\n",
        "                    # Size OK - accept as is?\n",
        "                    group_data['fire_event_id'] = event_id\n",
        "                    group_data = group_data.drop(['spatial_cluster', 'temp_group'], axis=1)\n",
        "                    fire_events.append(group_data)\n",
        "                    event_id += 1\n",
        "\n",
        "                elif extent_km <= 75:  # increased from 50km\n",
        "                    # size concerning but salvageable - try to break up\n",
        "                    print(f\"     Large cluster detected: {extent_km:.1f}km\")\n",
        "                    sub_events = self._break_up_large_cluster(group_data, max_size_km=30)  # allow 30km sub-events\n",
        "\n",
        "                    if len(sub_events) > 0:\n",
        "                        # adjust event IDs\n",
        "                        max_sub_id = sub_events['fire_event_id'].max()\n",
        "                        sub_events['fire_event_id'] += event_id\n",
        "                        fire_events.append(sub_events)\n",
        "                        event_id += max_sub_id + 1\n",
        "                    # ff break up fails, cluster is discarded (treated as noise)\n",
        "\n",
        "                else:\n",
        "                    # size too large - hard reject\n",
        "                    print(f\"     Rejected oversized cluster: {extent_km:.1f}km (>75km hard limit)\")\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            print(f\"     Final fire events: {len(result_df['fire_event_id'].unique())}\")\n",
        "            return result_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_geographic_region(self, year, region_name, region_bounds):\n",
        "        \"\"\"Process fire events for a specific geographic region\"\"\"\n",
        "\n",
        "        params = self.get_region_parameters(region_name)\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        print(f\"\\nProcessing region: {region_name}\")\n",
        "        print(f\"   Bounds: {region_bounds}\")\n",
        "        print(f\"   Parameters: {params['spatial_km']:.1f}km, temporal={params['temporal_days']}d, min_samples={params['min_samples']}\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id, year, doy, longitude, latitude, fire_date,\n",
        "            grid10k, covertype, fuelcode, area_burned,\n",
        "            consumed_fuel, ECO2, burn_source, burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        AND latitude >= {region_bounds['lat_min']}\n",
        "        AND latitude < {region_bounds['lat_max']}\n",
        "        AND longitude >= {region_bounds['lon_min']}\n",
        "        AND longitude < {region_bounds['lon_max']}\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"   No data found for {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"   Found {len(df):,} records\")\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        clustered_df = self._cluster_region_data_hierarchical(df, params)\n",
        "\n",
        "        if len(clustered_df) > 0:\n",
        "            clustered_df['region'] = region_name\n",
        "            unique_events = len(clustered_df['fire_event_id'].unique())\n",
        "            print(f\"   Found {unique_events} fire events in {region_name}\")\n",
        "            return clustered_df\n",
        "        else:\n",
        "            print(f\"   No fire events found in {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_year_hierarchical(self, year):\n",
        "        \"\"\"Process entire year using hierarchical clustering\"\"\"\n",
        "\n",
        "        print(f\"HIERARCHICAL FIRE CLUSTERING for {year}\")\n",
        "        print(\"Using intelligent cluster breakup:\")\n",
        "        print(\"  - ≤25km: Accept as-is\")\n",
        "        print(\"  - 25-75km: Try to break up (↑ increased range)\")\n",
        "        print(\"  - >75km: Hard reject\")\n",
        "        print(\"  - 5-day temporal gaps (↑ increased)\")\n",
        "        print(\"  - Balanced min_samples (↓ reduced)\")\n",
        "\n",
        "        regions = self.define_geographic_regions()\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # total count\n",
        "        total_query = f\"\"\"\n",
        "        SELECT COUNT(*) as total_count\n",
        "        FROM `{self.project_id}.{self.dataset_id}.emission_{year}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        total_result = self.client.query(total_query).to_dataframe()\n",
        "        original_count = total_result['total_count'].iloc[0]\n",
        "\n",
        "        all_regional_events = []\n",
        "        global_event_id = 0\n",
        "\n",
        "        for region_name, region_bounds in regions.items():\n",
        "            regional_events = self.process_geographic_region(year, region_name, region_bounds)\n",
        "\n",
        "            if len(regional_events) > 0:\n",
        "                # adjust event IDs\n",
        "                max_regional_id = regional_events['fire_event_id'].max()\n",
        "                regional_events['fire_event_id'] += global_event_id\n",
        "                global_event_id += max_regional_id + 1\n",
        "\n",
        "                all_regional_events.append(regional_events)\n",
        "\n",
        "        if all_regional_events:\n",
        "            combined_events = pd.concat(all_regional_events, ignore_index=True)\n",
        "\n",
        "            # remove dups\n",
        "            initial_count = len(combined_events)\n",
        "            combined_events = combined_events.drop_duplicates(subset=['id'], keep='first')\n",
        "            final_count = len(combined_events)\n",
        "\n",
        "            if initial_count != final_count:\n",
        "                duplicate_count = initial_count - final_count\n",
        "                print(f\"WARNING: Removed {duplicate_count} duplicate records ({duplicate_count/initial_count*100:.1f}%)\")\n",
        "\n",
        "            print(f\"\\nHIERARCHICAL CLUSTERING RESULTS:\")\n",
        "            print(f\"Total fire events: {len(combined_events['fire_event_id'].unique())}\")\n",
        "            print(f\"Total data points: {len(combined_events):,}\")\n",
        "            print(f\"Original data points: {original_count:,}\")\n",
        "            print(f\"Coverage: {len(combined_events)/original_count*100:.1f}% of original data\")\n",
        "\n",
        "            return combined_events\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def validate_results(self, fire_events_df):\n",
        "        \"\"\"Validate hierarchical clustering results\"\"\"\n",
        "\n",
        "        if len(fire_events_df) == 0:\n",
        "            print(\"No fire events to validate\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # calc stats\n",
        "        event_stats = fire_events_df.groupby('fire_event_id').agg({\n",
        "            'fire_date': ['min', 'max'],\n",
        "            'longitude': ['min', 'max'],\n",
        "            'latitude': ['min', 'max'],\n",
        "            'id': 'count'\n",
        "        }).reset_index()\n",
        "\n",
        "        event_stats.columns = ['fire_event_id', 'start_date', 'end_date',\n",
        "                              'min_lon', 'max_lon', 'min_lat', 'max_lat', 'point_count']\n",
        "\n",
        "        event_stats['duration_days'] = (event_stats['end_date'] - event_stats['start_date']).dt.days\n",
        "        event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "            lambda row: self.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                              row['max_lat'], row['max_lon']), axis=1\n",
        "        )\n",
        "\n",
        "        # quality checks\n",
        "        flagged_size = event_stats[event_stats['spatial_extent_km'] > 30]\n",
        "        hard_fails = event_stats[event_stats['spatial_extent_km'] > 75]\n",
        "\n",
        "        print(f\"\\nVALIDATION RESULTS:\")\n",
        "        print(f\"Total events: {len(event_stats)}\")\n",
        "        print(f\"Average spatial extent: {event_stats['spatial_extent_km'].mean():.1f} km\")\n",
        "        print(f\"Average duration: {event_stats['duration_days'].mean():.1f} days\")\n",
        "        print(f\"Max spatial extent: {event_stats['spatial_extent_km'].max():.1f} km\")\n",
        "        print(f\"Max duration: {event_stats['duration_days'].max():.1f} days\")\n",
        "        print(f\"\\nQUALITY CHECK:\")\n",
        "        print(f\"Events >30km (flagged): {len(flagged_size)} ({len(flagged_size)/len(event_stats)*100:.1f}%)\")\n",
        "        print(f\"Events >75km (hard fails): {len(hard_fails)} (should be 0)\")\n",
        "\n",
        "        if len(flagged_size) > 0:\n",
        "            print(f\"\\nFLAGGED EVENTS (30-75km):\")\n",
        "            for _, row in flagged_size.head(10).iterrows():\n",
        "                print(f\"  Event {row['fire_event_id']}: {row['spatial_extent_km']:.1f}km, \"\n",
        "                      f\"{row['duration_days']}d, {row['point_count']} points\")\n",
        "\n",
        "        if len(hard_fails) > 0:\n",
        "            print(f\"\\nHARD FAILURES (>75km):\")\n",
        "            for _, row in hard_fails.iterrows():\n",
        "                print(f\"  Event {row['fire_event_id']}: {row['spatial_extent_km']:.1f}km, \"\n",
        "                      f\"{row['duration_days']}d, {row['point_count']} points\")\n",
        "\n",
        "        return event_stats\n",
        "\n",
        "    def save_results_to_csv(self, fire_events_df, filename):\n",
        "        \"\"\"Save results to CSV file\"\"\"\n",
        "        fire_events_df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "# use\n",
        "def main():\n",
        "    print(\"HIERARCHICAL FIRE CLUSTERING\")\n",
        "    print(\"Intelligently breaks up large clusters instead of discarding them\")\n",
        "    print(\"Based on the recommended thresholds:\")\n",
        "    print(\"  - Hard size cap: 50km\")\n",
        "    print(\"  - QA flag threshold: 25km\")\n",
        "    print(\"  - Temporal gap: 3 days\")\n",
        "    print(\"  - Min samples: 8-10 in cropland/grassland, 5 elsewhere\")\n",
        "    print()\n",
        "\n",
        "    clustering = HierarchicalFireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    year = 2004\n",
        "    fire_events = clustering.process_year_hierarchical(year=year)\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        stats = clustering.validate_results(fire_events)\n",
        "\n",
        "        clustering.save_results_to_csv(fire_events, f\"fire_events_{year}_hierarchical.csv\")\n",
        "        clustering.save_results_to_csv(stats, f\"fire_event_stats_{year}_hierarchical.csv\")\n",
        "\n",
        "        print(f\"\\nResults saved:\")\n",
        "        print(f\"- fire_events_{year}_hierarchical.csv\")\n",
        "        print(f\"- fire_event_stats_{year}_hierarchical.csv\")\n",
        "\n",
        "        if len(stats) > 0:\n",
        "            print(f\"\\nResults by region:\")\n",
        "            fire_events_with_stats = fire_events.merge(\n",
        "                stats[['fire_event_id', 'spatial_extent_km', 'duration_days']],\n",
        "                on='fire_event_id'\n",
        "            )\n",
        "            region_summary = fire_events_with_stats.groupby('region').agg({\n",
        "                'fire_event_id': 'nunique',\n",
        "                'spatial_extent_km': 'mean',\n",
        "                'duration_days': 'mean'\n",
        "            }).round(1)\n",
        "            print(region_summary)\n",
        "    else:\n",
        "        print(\"No fire events found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTWF76x6hy7G",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752506730052,
          "user_tz": 240,
          "elapsed": 217204,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "033ba187-44eb-482a-b32f-1bcb65ddf024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIERARCHICAL FIRE CLUSTERING\n",
            "Intelligently breaks up large clusters instead of discarding them\n",
            "Based on the recommended thresholds:\n",
            "  - Hard size cap: 50km\n",
            "  - QA flag threshold: 25km\n",
            "  - Temporal gap: 3 days\n",
            "  - Min samples: 8-10 in cropland/grassland, 5 elsewhere\n",
            "\n",
            "HIERARCHICAL FIRE CLUSTERING for 2004\n",
            "Using intelligent cluster breakup:\n",
            "  - ≤25km: Accept as-is\n",
            "  - 25-75km: Try to break up (↑ increased range)\n",
            "  - >75km: Hard reject\n",
            "  - 5-day temporal gaps (↑ increased)\n",
            "  - Balanced min_samples (↓ reduced)\n",
            "================================================================================\n",
            "\n",
            "Processing region: Pacific_West\n",
            "   Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0}\n",
            "   Parameters: 5.0km, temporal=5d, min_samples=5\n",
            "   Found 41,845 records\n",
            "     Initial spatial clusters: 418\n",
            "     Large cluster detected: 27.7km\n",
            "       Attempting to break up 27.7km cluster...\n",
            "         Successfully broke into 5 sub-events\n",
            "     Large cluster detected: 26.3km\n",
            "       Attempting to break up 26.3km cluster...\n",
            "         Successfully broke into 4 sub-events\n",
            "     Large cluster detected: 47.2km\n",
            "       Attempting to break up 47.2km cluster...\n",
            "         Successfully broke into 5 sub-events\n",
            "     Large cluster detected: 33.8km\n",
            "       Attempting to break up 33.8km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 37.5km\n",
            "       Attempting to break up 37.5km cluster...\n",
            "         Sub-cluster still too large (37.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 34.3km\n",
            "       Attempting to break up 34.3km cluster...\n",
            "         Sub-cluster still too large (34.3km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Final fire events: 686\n",
            "   Found 686 fire events in Pacific_West\n",
            "\n",
            "Processing region: Mountain_West\n",
            "   Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0}\n",
            "   Parameters: 5.0km, temporal=5d, min_samples=5\n",
            "   Found 38,643 records\n",
            "     Initial spatial clusters: 249\n",
            "     Large cluster detected: 57.9km\n",
            "       Attempting to break up 57.9km cluster...\n",
            "         Successfully broke into 10 sub-events\n",
            "     Large cluster detected: 28.6km\n",
            "       Attempting to break up 28.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 56.1km\n",
            "       Attempting to break up 56.1km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 29.5km\n",
            "       Attempting to break up 29.5km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 29.4km\n",
            "       Attempting to break up 29.4km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 44.1km\n",
            "       Attempting to break up 44.1km cluster...\n",
            "         Sub-cluster still too large (44.1km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 33.9km\n",
            "       Attempting to break up 33.9km cluster...\n",
            "         Sub-cluster still too large (33.9km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 26.0km\n",
            "       Attempting to break up 26.0km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Final fire events: 401\n",
            "   Found 401 fire events in Mountain_West\n",
            "\n",
            "Processing region: Great_Plains\n",
            "   Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0}\n",
            "   Parameters: 3.0km, temporal=5d, min_samples=6\n",
            "   Found 43,100 records\n",
            "     Initial spatial clusters: 428\n",
            "     Large cluster detected: 66.5km\n",
            "       Attempting to break up 66.5km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 169.7km (>75km hard limit)\n",
            "     Rejected oversized cluster: 100.0km (>75km hard limit)\n",
            "     Rejected oversized cluster: 86.6km (>75km hard limit)\n",
            "     Large cluster detected: 46.6km\n",
            "       Attempting to break up 46.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 110.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 125.8km (>75km hard limit)\n",
            "     Large cluster detected: 71.7km\n",
            "       Attempting to break up 71.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 108.5km (>75km hard limit)\n",
            "     Large cluster detected: 48.5km\n",
            "       Attempting to break up 48.5km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 252.9km (>75km hard limit)\n",
            "     Rejected oversized cluster: 264.0km (>75km hard limit)\n",
            "     Rejected oversized cluster: 78.2km (>75km hard limit)\n",
            "     Rejected oversized cluster: 165.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 239.7km (>75km hard limit)\n",
            "     Rejected oversized cluster: 134.8km (>75km hard limit)\n",
            "     Large cluster detected: 51.2km\n",
            "       Attempting to break up 51.2km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Rejected oversized cluster: 101.4km (>75km hard limit)\n",
            "     Large cluster detected: 69.3km\n",
            "       Attempting to break up 69.3km cluster...\n",
            "         Sub-cluster still too large (68.3km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 63.8km\n",
            "       Attempting to break up 63.8km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 45.6km\n",
            "       Attempting to break up 45.6km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 46.2km\n",
            "       Attempting to break up 46.2km cluster...\n",
            "         Sub-cluster still too large (46.2km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 28.3km\n",
            "       Attempting to break up 28.3km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 33.8km\n",
            "       Attempting to break up 33.8km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 47.6km\n",
            "       Attempting to break up 47.6km cluster...\n",
            "         Successfully broke into 5 sub-events\n",
            "     Large cluster detected: 71.0km\n",
            "       Attempting to break up 71.0km cluster...\n",
            "         Successfully broke into 6 sub-events\n",
            "     Large cluster detected: 26.2km\n",
            "       Attempting to break up 26.2km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 42.4km\n",
            "       Attempting to break up 42.4km cluster...\n",
            "         Sub-cluster still too large (41.3km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 87.6km (>75km hard limit)\n",
            "     Large cluster detected: 32.3km\n",
            "       Attempting to break up 32.3km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 31.8km\n",
            "       Attempting to break up 31.8km cluster...\n",
            "         Sub-cluster still too large (31.8km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 55.4km\n",
            "       Attempting to break up 55.4km cluster...\n",
            "         Successfully broke into 8 sub-events\n",
            "     Final fire events: 622\n",
            "   Found 622 fire events in Great_Plains\n",
            "\n",
            "Processing region: South_Texas\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0}\n",
            "   Parameters: 3.0km, temporal=5d, min_samples=5\n",
            "   Found 5,117 records\n",
            "     Initial spatial clusters: 92\n",
            "     Large cluster detected: 46.4km\n",
            "       Attempting to break up 46.4km cluster...\n",
            "         Sub-cluster still too large (40.7km), skipping\n",
            "         Successfully broke into 4 sub-events\n",
            "     Large cluster detected: 26.1km\n",
            "       Attempting to break up 26.1km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Final fire events: 148\n",
            "   Found 148 fire events in South_Texas\n",
            "\n",
            "Processing region: South_Central\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.0}\n",
            "   Parameters: 2.0km, temporal=5d, min_samples=7\n",
            "   Found 47,810 records\n",
            "     Initial spatial clusters: 308\n",
            "     Large cluster detected: 65.0km\n",
            "       Attempting to break up 65.0km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 29.6km\n",
            "       Attempting to break up 29.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 64.1km\n",
            "       Attempting to break up 64.1km cluster...\n",
            "         Sub-cluster still too large (30.4km), skipping\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 27.3km\n",
            "       Attempting to break up 27.3km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 60.9km\n",
            "       Attempting to break up 60.9km cluster...\n",
            "         Sub-cluster still too large (49.0km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 56.3km\n",
            "       Attempting to break up 56.3km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 73.7km\n",
            "       Attempting to break up 73.7km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 143.9km (>75km hard limit)\n",
            "     Rejected oversized cluster: 632.8km (>75km hard limit)\n",
            "     Rejected oversized cluster: 655.4km (>75km hard limit)\n",
            "     Large cluster detected: 52.5km\n",
            "       Attempting to break up 52.5km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Rejected oversized cluster: 724.8km (>75km hard limit)\n",
            "     Large cluster detected: 32.9km\n",
            "       Attempting to break up 32.9km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 172.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 99.4km (>75km hard limit)\n",
            "     Rejected oversized cluster: 754.7km (>75km hard limit)\n",
            "     Large cluster detected: 57.0km\n",
            "       Attempting to break up 57.0km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 639.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 143.5km (>75km hard limit)\n",
            "     Rejected oversized cluster: 573.0km (>75km hard limit)\n",
            "     Rejected oversized cluster: 817.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 173.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 569.5km (>75km hard limit)\n",
            "     Rejected oversized cluster: 82.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 115.8km (>75km hard limit)\n",
            "     Rejected oversized cluster: 211.7km (>75km hard limit)\n",
            "     Large cluster detected: 37.2km\n",
            "       Attempting to break up 37.2km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Rejected oversized cluster: 145.7km (>75km hard limit)\n",
            "     Rejected oversized cluster: 107.0km (>75km hard limit)\n",
            "     Large cluster detected: 27.2km\n",
            "       Attempting to break up 27.2km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 117.5km (>75km hard limit)\n",
            "     Rejected oversized cluster: 102.4km (>75km hard limit)\n",
            "     Large cluster detected: 51.1km\n",
            "       Attempting to break up 51.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 71.7km\n",
            "       Attempting to break up 71.7km cluster...\n",
            "         Sub-cluster still too large (71.7km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 97.9km (>75km hard limit)\n",
            "     Large cluster detected: 70.3km\n",
            "       Attempting to break up 70.3km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 89.7km (>75km hard limit)\n",
            "     Large cluster detected: 69.1km\n",
            "       Attempting to break up 69.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 273.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 308.7km (>75km hard limit)\n",
            "     Rejected oversized cluster: 361.3km (>75km hard limit)\n",
            "     Rejected oversized cluster: 209.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 173.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 249.3km (>75km hard limit)\n",
            "     Rejected oversized cluster: 75.1km (>75km hard limit)\n",
            "     Large cluster detected: 34.3km\n",
            "       Attempting to break up 34.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 120.0km (>75km hard limit)\n",
            "     Rejected oversized cluster: 81.3km (>75km hard limit)\n",
            "     Large cluster detected: 40.6km\n",
            "       Attempting to break up 40.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 60.9km\n",
            "       Attempting to break up 60.9km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 38.1km\n",
            "       Attempting to break up 38.1km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Rejected oversized cluster: 173.3km (>75km hard limit)\n",
            "     Rejected oversized cluster: 85.1km (>75km hard limit)\n",
            "     Large cluster detected: 36.9km\n",
            "       Attempting to break up 36.9km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 125.3km (>75km hard limit)\n",
            "     Large cluster detected: 35.8km\n",
            "       Attempting to break up 35.8km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 32.9km\n",
            "       Attempting to break up 32.9km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 27.5km\n",
            "       Attempting to break up 27.5km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 667.8km (>75km hard limit)\n",
            "     Rejected oversized cluster: 137.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 196.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 463.6km (>75km hard limit)\n",
            "     Rejected oversized cluster: 229.3km (>75km hard limit)\n",
            "     Rejected oversized cluster: 633.5km (>75km hard limit)\n",
            "     Large cluster detected: 25.3km\n",
            "       Attempting to break up 25.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 27.6km\n",
            "       Attempting to break up 27.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 54.1km\n",
            "       Attempting to break up 54.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 68.9km\n",
            "       Attempting to break up 68.9km cluster...\n",
            "         Sub-cluster still too large (68.9km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 60.4km\n",
            "       Attempting to break up 60.4km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 26.2km\n",
            "       Attempting to break up 26.2km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 68.0km\n",
            "       Attempting to break up 68.0km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 99.6km (>75km hard limit)\n",
            "     Large cluster detected: 63.1km\n",
            "       Attempting to break up 63.1km cluster...\n",
            "         Sub-cluster still too large (36.6km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 84.3km (>75km hard limit)\n",
            "     Rejected oversized cluster: 81.8km (>75km hard limit)\n",
            "     Rejected oversized cluster: 146.5km (>75km hard limit)\n",
            "     Rejected oversized cluster: 235.8km (>75km hard limit)\n",
            "     Rejected oversized cluster: 214.0km (>75km hard limit)\n",
            "     Large cluster detected: 28.3km\n",
            "       Attempting to break up 28.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 65.4km\n",
            "       Attempting to break up 65.4km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 90.2km (>75km hard limit)\n",
            "     Rejected oversized cluster: 80.3km (>75km hard limit)\n",
            "     Large cluster detected: 68.7km\n",
            "       Attempting to break up 68.7km cluster...\n",
            "         Sub-cluster still too large (55.3km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 195.7km (>75km hard limit)\n",
            "     Large cluster detected: 35.7km\n",
            "       Attempting to break up 35.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 35.9km\n",
            "       Attempting to break up 35.9km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 40.2km\n",
            "       Attempting to break up 40.2km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 45.6km\n",
            "       Attempting to break up 45.6km cluster...\n",
            "         Sub-cluster still too large (45.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 26.3km\n",
            "       Attempting to break up 26.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 40.4km\n",
            "       Attempting to break up 40.4km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 35.9km\n",
            "       Attempting to break up 35.9km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 29.7km\n",
            "       Attempting to break up 29.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 28.8km\n",
            "       Attempting to break up 28.8km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 52.6km\n",
            "       Attempting to break up 52.6km cluster...\n",
            "         Sub-cluster still too large (47.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 45.4km\n",
            "       Attempting to break up 45.4km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 37.2km\n",
            "       Attempting to break up 37.2km cluster...\n",
            "         Sub-cluster still too large (37.2km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 30.6km\n",
            "       Attempting to break up 30.6km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 90.8km (>75km hard limit)\n",
            "     Large cluster detected: 29.1km\n",
            "       Attempting to break up 29.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 26.7km\n",
            "       Attempting to break up 26.7km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 105.4km (>75km hard limit)\n",
            "     Large cluster detected: 45.0km\n",
            "       Attempting to break up 45.0km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 38.2km\n",
            "       Attempting to break up 38.2km cluster...\n",
            "         Sub-cluster still too large (38.2km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 27.7km\n",
            "       Attempting to break up 27.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 68.9km\n",
            "       Attempting to break up 68.9km cluster...\n",
            "         Sub-cluster still too large (68.9km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 39.4km\n",
            "       Attempting to break up 39.4km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 61.3km\n",
            "       Attempting to break up 61.3km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 72.3km\n",
            "       Attempting to break up 72.3km cluster...\n",
            "         Sub-cluster still too large (72.3km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 33.5km\n",
            "       Attempting to break up 33.5km cluster...\n",
            "         Sub-cluster still too large (33.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 544.3km (>75km hard limit)\n",
            "     Large cluster detected: 57.3km\n",
            "       Attempting to break up 57.3km cluster...\n",
            "         Sub-cluster still too large (34.4km), skipping\n",
            "         Sub-cluster still too large (35.8km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 192.0km (>75km hard limit)\n",
            "     Rejected oversized cluster: 518.5km (>75km hard limit)\n",
            "     Large cluster detected: 48.7km\n",
            "       Attempting to break up 48.7km cluster...\n",
            "         Sub-cluster still too large (48.7km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 109.9km (>75km hard limit)\n",
            "     Rejected oversized cluster: 85.9km (>75km hard limit)\n",
            "     Rejected oversized cluster: 113.6km (>75km hard limit)\n",
            "     Large cluster detected: 55.4km\n",
            "       Attempting to break up 55.4km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 181.6km (>75km hard limit)\n",
            "     Large cluster detected: 36.4km\n",
            "       Attempting to break up 36.4km cluster...\n",
            "         Sub-cluster still too large (36.4km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 71.7km\n",
            "       Attempting to break up 71.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 25.7km\n",
            "       Attempting to break up 25.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 43.6km\n",
            "       Attempting to break up 43.6km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 28.1km\n",
            "       Attempting to break up 28.1km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 38.5km\n",
            "       Attempting to break up 38.5km cluster...\n",
            "         Sub-cluster still too large (38.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 38.5km\n",
            "       Attempting to break up 38.5km cluster...\n",
            "         Sub-cluster still too large (38.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 47.4km\n",
            "       Attempting to break up 47.4km cluster...\n",
            "         Sub-cluster still too large (31.5km), skipping\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 26.6km\n",
            "       Attempting to break up 26.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 105.7km (>75km hard limit)\n",
            "     Large cluster detected: 53.2km\n",
            "       Attempting to break up 53.2km cluster...\n",
            "         Sub-cluster still too large (53.2km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 28.2km\n",
            "       Attempting to break up 28.2km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 31.6km\n",
            "       Attempting to break up 31.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 36.4km\n",
            "       Attempting to break up 36.4km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 43.5km\n",
            "       Attempting to break up 43.5km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 25.3km\n",
            "       Attempting to break up 25.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 106.2km (>75km hard limit)\n",
            "     Rejected oversized cluster: 182.5km (>75km hard limit)\n",
            "     Rejected oversized cluster: 95.0km (>75km hard limit)\n",
            "     Rejected oversized cluster: 87.6km (>75km hard limit)\n",
            "     Large cluster detected: 50.0km\n",
            "       Attempting to break up 50.0km cluster...\n",
            "         Sub-cluster still too large (50.0km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 37.5km\n",
            "       Attempting to break up 37.5km cluster...\n",
            "         Sub-cluster still too large (37.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 31.3km\n",
            "       Attempting to break up 31.3km cluster...\n",
            "         Sub-cluster still too large (31.3km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Final fire events: 776\n",
            "   Found 776 fire events in South_Central\n",
            "\n",
            "Processing region: Midwest\n",
            "   Bounds: {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0}\n",
            "   Parameters: 3.0km, temporal=5d, min_samples=6\n",
            "   Found 5,276 records\n",
            "     Initial spatial clusters: 106\n",
            "     Large cluster detected: 54.6km\n",
            "       Attempting to break up 54.6km cluster...\n",
            "         Sub-cluster still too large (54.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 41.2km\n",
            "       Attempting to break up 41.2km cluster...\n",
            "         Sub-cluster still too large (41.2km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 90.6km (>75km hard limit)\n",
            "     Large cluster detected: 59.6km\n",
            "       Attempting to break up 59.6km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Rejected oversized cluster: 241.0km (>75km hard limit)\n",
            "     Large cluster detected: 74.4km\n",
            "       Attempting to break up 74.4km cluster...\n",
            "         Sub-cluster still too large (74.4km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 30.6km\n",
            "       Attempting to break up 30.6km cluster...\n",
            "         Sub-cluster still too large (30.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 37.8km\n",
            "       Attempting to break up 37.8km cluster...\n",
            "         Sub-cluster still too large (37.8km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 46.8km\n",
            "       Attempting to break up 46.8km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 42.7km\n",
            "       Attempting to break up 42.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 27.1km\n",
            "       Attempting to break up 27.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 57.7km\n",
            "       Attempting to break up 57.7km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 28.8km\n",
            "       Attempting to break up 28.8km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 46.8km\n",
            "       Attempting to break up 46.8km cluster...\n",
            "         Sub-cluster still too large (46.8km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 62.7km\n",
            "       Attempting to break up 62.7km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 29.2km\n",
            "       Attempting to break up 29.2km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 30.5km\n",
            "       Attempting to break up 30.5km cluster...\n",
            "         Sub-cluster still too large (30.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 84.8km (>75km hard limit)\n",
            "     Large cluster detected: 37.7km\n",
            "       Attempting to break up 37.7km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Rejected oversized cluster: 257.8km (>75km hard limit)\n",
            "     Large cluster detected: 36.9km\n",
            "       Attempting to break up 36.9km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 37.5km\n",
            "       Attempting to break up 37.5km cluster...\n",
            "         Sub-cluster still too large (37.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 49.1km\n",
            "       Attempting to break up 49.1km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 31.5km\n",
            "       Attempting to break up 31.5km cluster...\n",
            "         Sub-cluster still too large (31.5km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Final fire events: 154\n",
            "   Found 154 fire events in Midwest\n",
            "\n",
            "Processing region: Southeast\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.0, 'lon_max': -75.0}\n",
            "   Parameters: 2.0km, temporal=5d, min_samples=5\n",
            "   Found 55,033 records\n",
            "     Initial spatial clusters: 452\n",
            "     Rejected oversized cluster: 144.2km (>75km hard limit)\n",
            "     Rejected oversized cluster: 113.8km (>75km hard limit)\n",
            "     Large cluster detected: 37.3km\n",
            "       Attempting to break up 37.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 43.0km\n",
            "       Attempting to break up 43.0km cluster...\n",
            "         Successfully broke into 8 sub-events\n",
            "     Large cluster detected: 44.3km\n",
            "       Attempting to break up 44.3km cluster...\n",
            "         Successfully broke into 8 sub-events\n",
            "     Large cluster detected: 25.0km\n",
            "       Attempting to break up 25.0km cluster...\n",
            "         Successfully broke into 5 sub-events\n",
            "     Large cluster detected: 26.4km\n",
            "       Attempting to break up 26.4km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 42.1km\n",
            "       Attempting to break up 42.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 41.0km\n",
            "       Attempting to break up 41.0km cluster...\n",
            "         Sub-cluster still too large (41.0km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 25.6km\n",
            "       Attempting to break up 25.6km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 30.0km\n",
            "       Attempting to break up 30.0km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 31.8km\n",
            "       Attempting to break up 31.8km cluster...\n",
            "         Sub-cluster still too large (31.8km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 38.9km\n",
            "       Attempting to break up 38.9km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 35.6km\n",
            "       Attempting to break up 35.6km cluster...\n",
            "         Successfully broke into 4 sub-events\n",
            "     Large cluster detected: 48.6km\n",
            "       Attempting to break up 48.6km cluster...\n",
            "         Sub-cluster still too large (48.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 31.7km\n",
            "       Attempting to break up 31.7km cluster...\n",
            "         Successfully broke into 4 sub-events\n",
            "     Large cluster detected: 25.6km\n",
            "       Attempting to break up 25.6km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 69.2km\n",
            "       Attempting to break up 69.2km cluster...\n",
            "         Sub-cluster still too large (53.9km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 26.5km\n",
            "       Attempting to break up 26.5km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 28.9km\n",
            "       Attempting to break up 28.9km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 66.4km\n",
            "       Attempting to break up 66.4km cluster...\n",
            "         Sub-cluster still too large (37.0km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 49.4km\n",
            "       Attempting to break up 49.4km cluster...\n",
            "         Sub-cluster still too large (48.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 26.0km\n",
            "       Attempting to break up 26.0km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 25.5km\n",
            "       Attempting to break up 25.5km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 26.7km\n",
            "       Attempting to break up 26.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Final fire events: 968\n",
            "   Found 968 fire events in Southeast\n",
            "\n",
            "Processing region: Northeast\n",
            "   Bounds: {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0}\n",
            "   Parameters: 5.0km, temporal=5d, min_samples=5\n",
            "   Found 2,614 records\n",
            "     Initial spatial clusters: 42\n",
            "     Large cluster detected: 28.5km\n",
            "       Attempting to break up 28.5km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 52.1km\n",
            "       Attempting to break up 52.1km cluster...\n",
            "         Sub-cluster still too large (30.4km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 37.3km\n",
            "       Attempting to break up 37.3km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Rejected oversized cluster: 154.5km (>75km hard limit)\n",
            "     Rejected oversized cluster: 113.9km (>75km hard limit)\n",
            "     Rejected oversized cluster: 100.8km (>75km hard limit)\n",
            "     Rejected oversized cluster: 98.1km (>75km hard limit)\n",
            "     Rejected oversized cluster: 128.2km (>75km hard limit)\n",
            "     Rejected oversized cluster: 113.7km (>75km hard limit)\n",
            "     Large cluster detected: 74.9km\n",
            "       Attempting to break up 74.9km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 74.0km\n",
            "       Attempting to break up 74.0km cluster...\n",
            "         Successfully broke into 4 sub-events\n",
            "     Large cluster detected: 60.2km\n",
            "       Attempting to break up 60.2km cluster...\n",
            "         Successfully broke into 5 sub-events\n",
            "     Large cluster detected: 55.6km\n",
            "       Attempting to break up 55.6km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 28.3km\n",
            "       Attempting to break up 28.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 53.8km\n",
            "       Attempting to break up 53.8km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 27.8km\n",
            "       Attempting to break up 27.8km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 39.8km\n",
            "       Attempting to break up 39.8km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Rejected oversized cluster: 75.7km (>75km hard limit)\n",
            "     Large cluster detected: 48.5km\n",
            "       Attempting to break up 48.5km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 26.1km\n",
            "       Attempting to break up 26.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 25.0km\n",
            "       Attempting to break up 25.0km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Final fire events: 105\n",
            "   Found 105 fire events in Northeast\n",
            "\n",
            "Processing region: Florida\n",
            "   Bounds: {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.0, 'lon_max': -80.0}\n",
            "   Parameters: 2.0km, temporal=5d, min_samples=5\n",
            "   Found 45,214 records\n",
            "     Initial spatial clusters: 249\n",
            "     Rejected oversized cluster: 144.2km (>75km hard limit)\n",
            "     Rejected oversized cluster: 113.8km (>75km hard limit)\n",
            "     Large cluster detected: 37.3km\n",
            "       Attempting to break up 37.3km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 43.0km\n",
            "       Attempting to break up 43.0km cluster...\n",
            "         Successfully broke into 8 sub-events\n",
            "     Large cluster detected: 44.3km\n",
            "       Attempting to break up 44.3km cluster...\n",
            "         Successfully broke into 8 sub-events\n",
            "     Large cluster detected: 25.0km\n",
            "       Attempting to break up 25.0km cluster...\n",
            "         Successfully broke into 5 sub-events\n",
            "     Large cluster detected: 26.4km\n",
            "       Attempting to break up 26.4km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 42.1km\n",
            "       Attempting to break up 42.1km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 41.0km\n",
            "       Attempting to break up 41.0km cluster...\n",
            "         Sub-cluster still too large (41.0km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 25.6km\n",
            "       Attempting to break up 25.6km cluster...\n",
            "         Successfully broke into 3 sub-events\n",
            "     Large cluster detected: 30.0km\n",
            "       Attempting to break up 30.0km cluster...\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 31.8km\n",
            "       Attempting to break up 31.8km cluster...\n",
            "         Sub-cluster still too large (31.8km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 38.9km\n",
            "       Attempting to break up 38.9km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 25.6km\n",
            "       Attempting to break up 25.6km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 69.2km\n",
            "       Attempting to break up 69.2km cluster...\n",
            "         Sub-cluster still too large (53.9km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 26.5km\n",
            "       Attempting to break up 26.5km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 28.9km\n",
            "       Attempting to break up 28.9km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 66.4km\n",
            "       Attempting to break up 66.4km cluster...\n",
            "         Sub-cluster still too large (37.0km), skipping\n",
            "         Successfully broke into 1 sub-events\n",
            "     Large cluster detected: 49.4km\n",
            "       Attempting to break up 49.4km cluster...\n",
            "         Sub-cluster still too large (48.6km), skipping\n",
            "         Could not break up cluster, treating as noise\n",
            "     Large cluster detected: 26.0km\n",
            "       Attempting to break up 26.0km cluster...\n",
            "         Successfully broke into 2 sub-events\n",
            "     Large cluster detected: 26.7km\n",
            "       Attempting to break up 26.7km cluster...\n",
            "         Successfully broke into 1 sub-events\n",
            "     Final fire events: 577\n",
            "   Found 577 fire events in Florida\n",
            "WARNING: Removed 18934 duplicate records (12.0%)\n",
            "\n",
            "HIERARCHICAL CLUSTERING RESULTS:\n",
            "Total fire events: 3860\n",
            "Total data points: 139,490\n",
            "Original data points: 239,663\n",
            "Coverage: 58.2% of original data\n",
            "\n",
            "VALIDATION RESULTS:\n",
            "Total events: 3860\n",
            "Average spatial extent: 3.0 km\n",
            "Average duration: 3.1 days\n",
            "Max spatial extent: 29.7 km\n",
            "Max duration: 68.0 days\n",
            "\n",
            "QUALITY CHECK:\n",
            "Events >30km (flagged): 0 (0.0%)\n",
            "Events >75km (hard fails): 0 (should be 0)\n",
            "Results saved to fire_events_2004_hierarchical.csv\n",
            "Results saved to fire_event_stats_2004_hierarchical.csv\n",
            "\n",
            "Results saved:\n",
            "- fire_events_2004_hierarchical.csv\n",
            "- fire_event_stats_2004_hierarchical.csv\n",
            "\n",
            "Results by region:\n",
            "               fire_event_id  spatial_extent_km  duration_days\n",
            "region                                                        \n",
            "Great_Plains             622               10.5            8.2\n",
            "Midwest                  154                8.4            5.1\n",
            "Mountain_West            401               12.6            7.8\n",
            "Northeast                105                7.0           10.5\n",
            "Pacific_West             686               10.4            6.3\n",
            "South_Central            776                9.0            8.8\n",
            "South_Texas              148                5.2            9.5\n",
            "Southeast                968                9.3           16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from google.cloud import bigquery\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DiagnosticFireClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "        self.data_loss_tracking = {}\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        return R * c\n",
        "\n",
        "    def get_region_parameters(self, region_name):\n",
        "        \"\"\"Get parameters with proper radians conversion\"\"\"\n",
        "\n",
        "        # much more permissive parameters to reduce data loss\n",
        "        region_params_km = {\n",
        "            'Pacific_West': {'spatial_km': 8.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'Mountain_West': {'spatial_km': 8.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'Great_Plains': {'spatial_km': 5.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'South_Texas': {'spatial_km': 5.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'South_Central': {'spatial_km': 3.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'Midwest': {'spatial_km': 5.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'Southeast': {'spatial_km': 3.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'Northeast': {'spatial_km': 8.0, 'temporal_days': 7, 'min_samples': 4},\n",
        "            'Florida': {'spatial_km': 3.0, 'temporal_days': 7, 'min_samples': 4}\n",
        "        }\n",
        "\n",
        "        params_km = region_params_km.get(region_name,\n",
        "                                       {'spatial_km': 5.0, 'temporal_days': 7, 'min_samples': 4})\n",
        "\n",
        "        # convert to radians\n",
        "        spatial_degrees = params_km['spatial_km'] / 111.0\n",
        "        spatial_radians = np.radians(spatial_degrees)\n",
        "\n",
        "        return {\n",
        "            'spatial_eps_radians': spatial_radians,\n",
        "            'spatial_km': params_km['spatial_km'],\n",
        "            'temporal_days': params_km['temporal_days'],\n",
        "            'min_samples': params_km['min_samples']\n",
        "        }\n",
        "\n",
        "    def _get_cluster_extent(self, cluster_data):\n",
        "        \"\"\"Calculate spatial extent of cluster in km\"\"\"\n",
        "        min_lat = cluster_data['latitude'].min()\n",
        "        max_lat = cluster_data['latitude'].max()\n",
        "        min_lon = cluster_data['longitude'].min()\n",
        "        max_lon = cluster_data['longitude'].max()\n",
        "\n",
        "        return self.haversine_distance(min_lat, min_lon, max_lat, max_lon)\n",
        "\n",
        "    def _track_data_loss(self, stage, region, original_count, remaining_count, notes=\"\"):\n",
        "        \"\"\"Track data loss at each stage\"\"\"\n",
        "        if region not in self.data_loss_tracking:\n",
        "            self.data_loss_tracking[region] = []\n",
        "\n",
        "        lost_count = original_count - remaining_count\n",
        "        lost_percent = (lost_count / original_count * 100) if original_count > 0 else 0\n",
        "\n",
        "        self.data_loss_tracking[region].append({\n",
        "            'stage': stage,\n",
        "            'original': original_count,\n",
        "            'remaining': remaining_count,\n",
        "            'lost': lost_count,\n",
        "            'lost_percent': lost_percent,\n",
        "            'notes': notes\n",
        "        })\n",
        "\n",
        "    def _simple_size_filter(self, group_data, max_size_km=100):\n",
        "        \"\"\"Simple size filter - just check if cluster is reasonable\"\"\"\n",
        "        extent_km = self._get_cluster_extent(group_data)\n",
        "\n",
        "        if extent_km <= max_size_km:\n",
        "            return group_data, True, f\"accepted ({extent_km:.1f}km)\"\n",
        "        else:\n",
        "            return pd.DataFrame(), False, f\"rejected ({extent_km:.1f}km > {max_size_km}km)\"\n",
        "\n",
        "    def _cluster_region_data_diagnostic(self, df, params, region_name):\n",
        "        \"\"\"Apply clustering with detailed diagnostics\"\"\"\n",
        "\n",
        "        original_count = len(df)\n",
        "        print(f\"     Starting with {original_count:,} records\")\n",
        "\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        dbscan = DBSCAN(\n",
        "            eps=params['spatial_eps_radians'],\n",
        "            min_samples=params['min_samples'],\n",
        "            metric='haversine'\n",
        "        )\n",
        "        spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "        df['spatial_cluster'] = spatial_labels\n",
        "\n",
        "        noise_count = np.sum(spatial_labels == -1)\n",
        "        clustered_count = original_count - noise_count\n",
        "        n_spatial_clusters = len(set(spatial_labels)) - (1 if -1 in spatial_labels else 0)\n",
        "\n",
        "        print(f\"     Initial clustering: {n_spatial_clusters} clusters, {noise_count:,} noise points\")\n",
        "        self._track_data_loss(\"initial_clustering\", region_name, original_count, clustered_count,\n",
        "                             f\"{n_spatial_clusters} clusters, {noise_count} noise\")\n",
        "\n",
        "        fire_events = []\n",
        "        event_id = 0\n",
        "        temporal_processed = 0\n",
        "        temporal_accepted = 0\n",
        "\n",
        "        for spatial_cluster in df['spatial_cluster'].unique():\n",
        "            if spatial_cluster == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_data = df[df['spatial_cluster'] == spatial_cluster].copy()\n",
        "            cluster_data = cluster_data.sort_values('fire_date')\n",
        "            temporal_processed += len(cluster_data)\n",
        "\n",
        "            cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(cluster_data)):\n",
        "                prev_date = cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > params['temporal_days']:\n",
        "                    current_group += 1\n",
        "\n",
        "                cluster_data.iloc[i, cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            for temp_group in cluster_data['temp_group'].unique():\n",
        "                group_data = cluster_data[cluster_data['temp_group'] == temp_group].copy()\n",
        "\n",
        "                filtered_data, accepted, notes = self._simple_size_filter(group_data, max_size_km=100)\n",
        "\n",
        "                if accepted:\n",
        "                    filtered_data['fire_event_id'] = event_id\n",
        "                    filtered_data = filtered_data.drop(['spatial_cluster', 'temp_group'], axis=1)\n",
        "                    fire_events.append(filtered_data)\n",
        "                    temporal_accepted += len(filtered_data)\n",
        "                    event_id += 1\n",
        "\n",
        "        print(f\"     Temporal processing: {temporal_processed:,} → {temporal_accepted:,} points\")\n",
        "        self._track_data_loss(\"temporal_and_size\", region_name, temporal_processed, temporal_accepted,\n",
        "                             f\"after temporal + size filtering\")\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            final_count = len(result_df)\n",
        "            n_events = len(result_df['fire_event_id'].unique())\n",
        "            print(f\"     Final result: {n_events} events, {final_count:,} points\")\n",
        "            self._track_data_loss(\"final\", region_name, original_count, final_count, f\"{n_events} events\")\n",
        "            return result_df\n",
        "        else:\n",
        "            print(f\"     Final result: 0 events, 0 points\")\n",
        "            self._track_data_loss(\"final\", region_name, original_count, 0, \"no events\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_region_diagnostic(self, year, region_name, region_bounds):\n",
        "        \"\"\"Process region with detailed diagnostics\"\"\"\n",
        "\n",
        "        params = self.get_region_parameters(region_name)\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PROCESSING REGION: {region_name}\")\n",
        "        print(f\"Bounds: {region_bounds}\")\n",
        "        print(f\"Parameters: {params['spatial_km']:.1f}km, {params['temporal_days']}d, min_samples={params['min_samples']}\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id, year, doy, longitude, latitude, fire_date,\n",
        "            grid10k, covertype, fuelcode, area_burned,\n",
        "            consumed_fuel, ECO2, burn_source, burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        AND latitude >= {region_bounds['lat_min']}\n",
        "        AND latitude < {region_bounds['lat_max']}\n",
        "        AND longitude >= {region_bounds['lon_min']}\n",
        "        AND longitude < {region_bounds['lon_max']}\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"NO DATA FOUND\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        clustered_df = self._cluster_region_data_diagnostic(df, params, region_name)\n",
        "\n",
        "        if len(clustered_df) > 0:\n",
        "            clustered_df['region'] = region_name\n",
        "            return clustered_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def run_diagnostic(self, year):\n",
        "        \"\"\"Run diagnostic on all regions\"\"\"\n",
        "\n",
        "        print(f\"DIAGNOSTIC FIRE CLUSTERING for {year}\")\n",
        "        print(\"Using more permissive parameters to identify data loss points\")\n",
        "\n",
        "        regions = {\n",
        "            'Pacific_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0},\n",
        "            'Mountain_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0},\n",
        "            'Great_Plains': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Texas': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Central': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Midwest': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Southeast': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.0, 'lon_max': -75.0},\n",
        "            'Northeast': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0},\n",
        "            'Florida': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.0, 'lon_max': -80.0}\n",
        "        }\n",
        "\n",
        "        total_query = f\"\"\"\n",
        "        SELECT COUNT(*) as total_count\n",
        "        FROM `{self.project_id}.{self.dataset_id}.emission_{year}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        total_result = self.client.query(total_query).to_dataframe()\n",
        "        original_count = total_result['total_count'].iloc[0]\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        for region_name, region_bounds in regions.items():\n",
        "            result = self.process_region_diagnostic(year, region_name, region_bounds)\n",
        "            if len(result) > 0:\n",
        "                all_results.append(result)\n",
        "\n",
        "        if all_results:\n",
        "            combined_events = pd.concat(all_results, ignore_index=True)\n",
        "            combined_events = combined_events.drop_duplicates(subset=['id'], keep='first')\n",
        "\n",
        "            final_count = len(combined_events)\n",
        "            coverage = final_count / original_count * 100\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"DIAGNOSTIC SUMMARY\")\n",
        "            print(f\"Total original records: {original_count:,}\")\n",
        "            print(f\"Total final records: {final_count:,}\")\n",
        "            print(f\"Overall coverage: {coverage:.1f}%\")\n",
        "\n",
        "            print(f\"\\nDATA LOSS ANALYSIS BY REGION:\")\n",
        "            for region, losses in self.data_loss_tracking.items():\n",
        "                print(f\"\\n{region}:\")\n",
        "                for loss in losses:\n",
        "                    print(f\"  {loss['stage']}: {loss['original']:,} → {loss['remaining']:,} \"\n",
        "                          f\"({loss['lost_percent']:.1f}% lost) [{loss['notes']}]\")\n",
        "\n",
        "            return combined_events\n",
        "        else:\n",
        "            print(\"No results found\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "def main():\n",
        "    print(\"DIAGNOSTIC FIRE CLUSTERING\")\n",
        "    print(\"Purpose: Identify exactly where we're losing data\")\n",
        "    print(\"Uses more permissive parameters to minimize data loss\")\n",
        "    print()\n",
        "\n",
        "    clustering = DiagnosticFireClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    year = 2004\n",
        "    fire_events = clustering.run_diagnostic(year=year)\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        print(f\"\\nSuccessfully processed {len(fire_events):,} data points\")\n",
        "        print(f\"Found {len(fire_events['fire_event_id'].unique())} fire events\")\n",
        "\n",
        "        if len(fire_events) > 0:\n",
        "            event_stats = fire_events.groupby('fire_event_id').agg({\n",
        "                'fire_date': ['min', 'max'],\n",
        "                'longitude': ['min', 'max'],\n",
        "                'latitude': ['min', 'max']\n",
        "            }).reset_index()\n",
        "\n",
        "            event_stats.columns = ['fire_event_id', 'start_date', 'end_date',\n",
        "                                  'min_lon', 'max_lon', 'min_lat', 'max_lat']\n",
        "\n",
        "            event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "                lambda row: clustering.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                                        row['max_lat'], row['max_lon']), axis=1\n",
        "            )\n",
        "\n",
        "            print(f\"Max spatial extent: {event_stats['spatial_extent_km'].max():.1f} km\")\n",
        "            print(f\"Average spatial extent: {event_stats['spatial_extent_km'].mean():.1f} km\")\n",
        "\n",
        "            large_events = event_stats[event_stats['spatial_extent_km'] > 100]\n",
        "            print(f\"Events >100km: {len(large_events)}\")\n",
        "    else:\n",
        "        print(\"No fire events found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5xTG5KCjAAY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1752507013272,
          "user_tz": 240,
          "elapsed": 178846,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a64b3140-a954-469a-f1e6-557b9e6ab6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIAGNOSTIC FIRE CLUSTERING\n",
            "Purpose: Identify exactly where we're losing data\n",
            "Uses more permissive parameters to minimize data loss\n",
            "\n",
            "DIAGNOSTIC FIRE CLUSTERING for 2004\n",
            "Using more permissive parameters to identify data loss points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Pacific_West\n",
            "Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0}\n",
            "Parameters: 8.0km, 7d, min_samples=4\n",
            "     Starting with 41,845 records\n",
            "     Initial clustering: 381 clusters, 257 noise points\n",
            "     Temporal processing: 41,588 → 40,973 points\n",
            "     Final result: 643 events, 40,973 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Mountain_West\n",
            "Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0}\n",
            "Parameters: 8.0km, 7d, min_samples=4\n",
            "     Starting with 38,643 records\n",
            "     Initial clustering: 250 clusters, 215 noise points\n",
            "     Temporal processing: 38,428 → 38,428 points\n",
            "     Final result: 402 events, 38,428 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Great_Plains\n",
            "Bounds: {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0}\n",
            "Parameters: 5.0km, 7d, min_samples=4\n",
            "     Starting with 43,100 records\n",
            "     Initial clustering: 380 clusters, 142 noise points\n",
            "     Temporal processing: 42,958 → 22,277 points\n",
            "     Final result: 528 events, 22,277 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: South_Texas\n",
            "Bounds: {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0}\n",
            "Parameters: 5.0km, 7d, min_samples=4\n",
            "     Starting with 5,117 records\n",
            "     Initial clustering: 88 clusters, 39 noise points\n",
            "     Temporal processing: 5,078 → 3,945 points\n",
            "     Final result: 148 events, 3,945 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: South_Central\n",
            "Bounds: {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.0}\n",
            "Parameters: 3.0km, 7d, min_samples=4\n",
            "     Starting with 47,810 records\n",
            "     Initial clustering: 248 clusters, 258 noise points\n",
            "     Temporal processing: 47,552 → 15,121 points\n",
            "     Final result: 626 events, 15,121 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Midwest\n",
            "Bounds: {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0}\n",
            "Parameters: 5.0km, 7d, min_samples=4\n",
            "     Starting with 5,276 records\n",
            "     Initial clustering: 114 clusters, 109 noise points\n",
            "     Temporal processing: 5,167 → 3,451 points\n",
            "     Final result: 167 events, 3,451 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Southeast\n",
            "Bounds: {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.0, 'lon_max': -75.0}\n",
            "Parameters: 3.0km, 7d, min_samples=4\n",
            "     Starting with 55,033 records\n",
            "     Initial clustering: 424 clusters, 834 noise points\n",
            "     Temporal processing: 54,199 → 38,155 points\n",
            "     Final result: 977 events, 38,155 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Northeast\n",
            "Bounds: {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0}\n",
            "Parameters: 8.0km, 7d, min_samples=4\n",
            "     Starting with 2,614 records\n",
            "     Initial clustering: 33 clusters, 91 noise points\n",
            "     Temporal processing: 2,523 → 1,389 points\n",
            "     Final result: 55 events, 1,389 points\n",
            "\n",
            "============================================================\n",
            "PROCESSING REGION: Florida\n",
            "Bounds: {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.0, 'lon_max': -80.0}\n",
            "Parameters: 3.0km, 7d, min_samples=4\n",
            "     Starting with 45,214 records\n",
            "     Initial clustering: 201 clusters, 232 noise points\n",
            "     Temporal processing: 44,982 → 28,938 points\n",
            "     Final result: 485 events, 28,938 points\n",
            "\n",
            "============================================================\n",
            "DIAGNOSTIC SUMMARY\n",
            "Total original records: 239,663\n",
            "Total final records: 163,739\n",
            "Overall coverage: 68.3%\n",
            "\n",
            "DATA LOSS ANALYSIS BY REGION:\n",
            "\n",
            "Pacific_West:\n",
            "  initial_clustering: 41,845 → 41,588 (0.6% lost) [381 clusters, 257 noise]\n",
            "  temporal_and_size: 41,588 → 40,973 (1.5% lost) [after temporal + size filtering]\n",
            "  final: 41,845 → 40,973 (2.1% lost) [643 events]\n",
            "\n",
            "Mountain_West:\n",
            "  initial_clustering: 38,643 → 38,428 (0.6% lost) [250 clusters, 215 noise]\n",
            "  temporal_and_size: 38,428 → 38,428 (0.0% lost) [after temporal + size filtering]\n",
            "  final: 38,643 → 38,428 (0.6% lost) [402 events]\n",
            "\n",
            "Great_Plains:\n",
            "  initial_clustering: 43,100 → 42,958 (0.3% lost) [380 clusters, 142 noise]\n",
            "  temporal_and_size: 42,958 → 22,277 (48.1% lost) [after temporal + size filtering]\n",
            "  final: 43,100 → 22,277 (48.3% lost) [528 events]\n",
            "\n",
            "South_Texas:\n",
            "  initial_clustering: 5,117 → 5,078 (0.8% lost) [88 clusters, 39 noise]\n",
            "  temporal_and_size: 5,078 → 3,945 (22.3% lost) [after temporal + size filtering]\n",
            "  final: 5,117 → 3,945 (22.9% lost) [148 events]\n",
            "\n",
            "South_Central:\n",
            "  initial_clustering: 47,810 → 47,552 (0.5% lost) [248 clusters, 258 noise]\n",
            "  temporal_and_size: 47,552 → 15,121 (68.2% lost) [after temporal + size filtering]\n",
            "  final: 47,810 → 15,121 (68.4% lost) [626 events]\n",
            "\n",
            "Midwest:\n",
            "  initial_clustering: 5,276 → 5,167 (2.1% lost) [114 clusters, 109 noise]\n",
            "  temporal_and_size: 5,167 → 3,451 (33.2% lost) [after temporal + size filtering]\n",
            "  final: 5,276 → 3,451 (34.6% lost) [167 events]\n",
            "\n",
            "Southeast:\n",
            "  initial_clustering: 55,033 → 54,199 (1.5% lost) [424 clusters, 834 noise]\n",
            "  temporal_and_size: 54,199 → 38,155 (29.6% lost) [after temporal + size filtering]\n",
            "  final: 55,033 → 38,155 (30.7% lost) [977 events]\n",
            "\n",
            "Northeast:\n",
            "  initial_clustering: 2,614 → 2,523 (3.5% lost) [33 clusters, 91 noise]\n",
            "  temporal_and_size: 2,523 → 1,389 (44.9% lost) [after temporal + size filtering]\n",
            "  final: 2,614 → 1,389 (46.9% lost) [55 events]\n",
            "\n",
            "Florida:\n",
            "  initial_clustering: 45,214 → 44,982 (0.5% lost) [201 clusters, 232 noise]\n",
            "  temporal_and_size: 44,982 → 28,938 (35.7% lost) [after temporal + size filtering]\n",
            "  final: 45,214 → 28,938 (36.0% lost) [485 events]\n",
            "\n",
            "Successfully processed 163,739 data points\n",
            "Found 977 fire events\n",
            "Max spatial extent: 5064.8 km\n",
            "Average spatial extent: 2443.8 km\n",
            "Events >100km: 643\n"
          ]
        }
      ]
    }
  ]
}