{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from google.cloud import bigquery\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GeographicFireEventClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        return R * c\n",
        "\n",
        "    def define_geographic_regions(self):\n",
        "        \"\"\"Define geographic regions to process separately\"\"\"\n",
        "        regions = {\n",
        "            'Pacific_West': {'lat_min': 32, 'lat_max': 49, 'lon_min': -125, 'lon_max': -115},\n",
        "            'Mountain_West': {'lat_min': 32, 'lat_max': 49, 'lon_min': -115, 'lon_max': -105},\n",
        "            'Great_Plains': {'lat_min': 32, 'lat_max': 49, 'lon_min': -105, 'lon_max': -95},\n",
        "            'Midwest': {'lat_min': 37, 'lat_max': 49, 'lon_min': -95, 'lon_max': -85},\n",
        "            'Southeast': {'lat_min': 25, 'lat_max': 37, 'lon_min': -95, 'lon_max': -75},\n",
        "            'Northeast': {'lat_min': 37, 'lat_max': 49, 'lon_min': -85, 'lon_max': -67},\n",
        "            'South_Texas': {'lat_min': 25, 'lat_max': 32, 'lon_min': -105, 'lon_max': -95},\n",
        "            'Florida': {'lat_min': 25, 'lat_max': 32, 'lon_min': -95, 'lon_max': -80}\n",
        "        }\n",
        "        return regions\n",
        "\n",
        "    def get_region_parameters(self, region_name):\n",
        "        \"\"\"Get optimized parameters for each region based on fire patterns\"\"\"\n",
        "        region_params = {\n",
        "            'Pacific_West': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Mountain_West': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Great_Plains': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},  # tighter\n",
        "            'Midwest': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},      # tighter\n",
        "            'Southeast': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5},    # tighter\n",
        "            'Northeast': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'South_Texas': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},  # tighter\n",
        "            'Florida': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5}       # tighter\n",
        "        }\n",
        "        return region_params.get(region_name, {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5})\n",
        "\n",
        "    def process_geographic_region(self, year, region_name, region_bounds):\n",
        "        \"\"\"Process fire events for a specific geographic region with region-optimized parameters\"\"\"\n",
        "\n",
        "        # region-specific parameters\n",
        "        params = self.get_region_parameters(region_name)\n",
        "        spatial_eps = params['spatial_eps']\n",
        "        temporal_days = params['temporal_days']\n",
        "        min_samples = params['min_samples']\n",
        "\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        print(f\"\\nProcessing region: {region_name}\")\n",
        "        print(f\"   Bounds: {region_bounds}\")\n",
        "        print(f\"   Parameters: eps={spatial_eps}° (~{spatial_eps*111:.1f}km), temporal={temporal_days}d, min_samples={min_samples}\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id, year, doy, longitude, latitude, fire_date,\n",
        "            grid10k, covertype, fuelcode, area_burned,\n",
        "            consumed_fuel, ECO2, burn_source, burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        AND latitude >= {region_bounds['lat_min']}\n",
        "        AND latitude < {region_bounds['lat_max']}\n",
        "        AND longitude >= {region_bounds['lon_min']}\n",
        "        AND longitude < {region_bounds['lon_max']}\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"   No data found for {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"   Found {len(df):,} records\")\n",
        "\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        clustered_df = self._cluster_region_data(df, spatial_eps, temporal_days, min_samples)\n",
        "\n",
        "        if len(clustered_df) > 0:\n",
        "            clustered_df['region'] = region_name\n",
        "            unique_events = len(clustered_df['fire_event_id'].unique())\n",
        "            print(f\"   Found {unique_events} fire events in {region_name}\")\n",
        "            return clustered_df\n",
        "        else:\n",
        "            print(f\"   No fire events found in {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _cluster_region_data(self, df, spatial_eps, temporal_days, min_samples):\n",
        "        \"\"\"Apply clustering to data from a single geographic region\"\"\"\n",
        "\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        dbscan = DBSCAN(eps=spatial_eps, min_samples=min_samples, metric='haversine')\n",
        "        spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "        df['spatial_cluster'] = spatial_labels\n",
        "\n",
        "        fire_events = []\n",
        "        event_id = 0\n",
        "\n",
        "        for spatial_cluster in df['spatial_cluster'].unique():\n",
        "            if spatial_cluster == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_data = df[df['spatial_cluster'] == spatial_cluster].copy()\n",
        "            cluster_data = cluster_data.sort_values('fire_date')\n",
        "\n",
        "            cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(cluster_data)):\n",
        "                prev_date = cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > temporal_days:\n",
        "                    current_group += 1\n",
        "\n",
        "                cluster_data.iloc[i, cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            for temp_group in cluster_data['temp_group'].unique():\n",
        "                group_data = cluster_data[cluster_data['temp_group'] == temp_group].copy()\n",
        "                group_data['fire_event_id'] = event_id\n",
        "                fire_events.append(group_data)\n",
        "                event_id += 1\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            result_df = result_df.drop(['spatial_cluster', 'temp_group'], axis=1)\n",
        "            return result_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_year_geographic(self, year):\n",
        "        \"\"\"Process entire year using geographic chunking approach with region-specific parameters\"\"\"\n",
        "\n",
        "        print(f\"GEOGRAPHIC CLUSTERING for {year}\")\n",
        "        print(\"Using region-specific optimized parameters:\")\n",
        "\n",
        "        regions = self.define_geographic_regions()\n",
        "\n",
        "        for region_name in regions.keys():\n",
        "            params = self.get_region_parameters(region_name)\n",
        "            print(f\"  {region_name}: eps={params['spatial_eps']}° (~{params['spatial_eps']*111:.1f}km), temporal={params['temporal_days']}d\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        regions = self.define_geographic_regions()\n",
        "        all_regional_events = []\n",
        "        global_event_id = 0\n",
        "\n",
        "        for region_name, region_bounds in regions.items():\n",
        "            regional_events = self.process_geographic_region(\n",
        "                year, region_name, region_bounds\n",
        "            )\n",
        "\n",
        "            if len(regional_events) > 0:\n",
        "                max_regional_id = regional_events['fire_event_id'].max()\n",
        "                regional_events['fire_event_id'] += global_event_id\n",
        "                global_event_id += max_regional_id + 1\n",
        "\n",
        "                all_regional_events.append(regional_events)\n",
        "\n",
        "        if all_regional_events:\n",
        "            combined_events = pd.concat(all_regional_events, ignore_index=True)\n",
        "\n",
        "            print(f\"\\nFINAL RESULTS:\")\n",
        "            print(f\"Total fire events: {len(combined_events['fire_event_id'].unique())}\")\n",
        "            print(f\"Total data points: {len(combined_events):,}\")\n",
        "            print(f\"Coverage: {len(combined_events)/239663*100:.1f}% of original data\")\n",
        "\n",
        "            return combined_events\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def analyze_fire_events(self, fire_events_df):\n",
        "        \"\"\"Analyze the identified fire events\"\"\"\n",
        "        if len(fire_events_df) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        event_stats = fire_events_df.groupby('fire_event_id').agg({\n",
        "            'id': 'count',\n",
        "            'fire_date': ['min', 'max'],\n",
        "            'longitude': ['min', 'max'],\n",
        "            'latitude': ['min', 'max'],\n",
        "            'area_burned': 'sum',\n",
        "            'consumed_fuel': 'sum',\n",
        "            'ECO2': 'sum',\n",
        "            'region': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        event_stats.columns = ['fire_event_id', 'num_points', 'start_date', 'end_date',\n",
        "                              'min_lon', 'max_lon', 'min_lat', 'max_lat',\n",
        "                              'total_area_burned', 'total_consumed_fuel', 'total_ECO2', 'region']\n",
        "\n",
        "        event_stats['duration_days'] = (event_stats['end_date'] - event_stats['start_date']).dt.days + 1\n",
        "        event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "            lambda row: self.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                              row['max_lat'], row['max_lon']), axis=1\n",
        "        )\n",
        "\n",
        "        return event_stats\n",
        "\n",
        "    def save_results_to_csv(self, fire_events_df, filename):\n",
        "        \"\"\"Save results to CSV file\"\"\"\n",
        "        fire_events_df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    print(\"GEOGRAPHIC FIRE CLUSTERING)\n",
        "    print(\"This approach processes data by geographic regions to prevent\")\n",
        "    print(\"continent-spanning 'mega-fires' caused by temporal chunking.\")\n",
        "    print()\n",
        "\n",
        "    clustering = GeographicFireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    year = 2004\n",
        "    fire_events = clustering.process_year_geographic(year=year)\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        stats = clustering.analyze_fire_events(fire_events)\n",
        "        print(f\"\\nGEOGRAPHIC CLUSTERING RESULTS:\")\n",
        "        print(f\"Fire events found: {len(stats)}\")\n",
        "        print(f\"Average duration: {stats['duration_days'].mean():.1f} days\")\n",
        "        print(f\"Average spatial extent: {stats['spatial_extent_km'].mean():.1f} km\")\n",
        "        print(f\"Max spatial extent: {stats['spatial_extent_km'].max():.1f} km\")\n",
        "\n",
        "        large_events = stats[stats['spatial_extent_km'] > 500]\n",
        "        very_large_events = stats[stats['spatial_extent_km'] > 1000]\n",
        "\n",
        "        print(f\"\\nQUALITY CHECK:\")\n",
        "        print(f\"Events >500km: {len(large_events)} (target: <5)\")\n",
        "        print(f\"Events >1000km: {len(very_large_events)} (target: 0)\")\n",
        "\n",
        "        if len(large_events) > 0:\n",
        "            print(f\"\\nRemaining large events by region:\")\n",
        "            large_by_region = large_events.groupby('region')['fire_event_id'].count()\n",
        "            print(large_by_region)\n",
        "\n",
        "        clustering.save_results_to_csv(fire_events, f\"fire_events_{year}_geographic_optimized.csv\")\n",
        "        clustering.save_results_to_csv(stats, f\"fire_event_stats_{year}_geographic_optimized.csv\")\n",
        "\n",
        "        print(f\"\\nResults saved:\")\n",
        "        print(f\"- fire_events_{year}_geographic_optimized.csv\")\n",
        "        print(f\"- fire_event_stats_{year}_geographic_optimized.csv\")\n",
        "\n",
        "        print(f\"\\nResults by region:\")\n",
        "        region_summary = stats.groupby('region').agg({\n",
        "            'fire_event_id': 'count',\n",
        "            'spatial_extent_km': 'mean',\n",
        "            'duration_days': 'mean'\n",
        "        }).round(1)\n",
        "        print(region_summary)\n",
        "\n",
        "    else:\n",
        "        print(\"No fire events found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "R0hYb0qyTwp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from google.cloud import bigquery\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GeographicFireEventClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        return R * c\n",
        "\n",
        "    def define_geographic_regions(self):\n",
        "        \"\"\"Define non-overlapping geographic regions to process separately\"\"\"\n",
        "        regions = {\n",
        "            'Pacific_West': {'lat_min': 32, 'lat_max': 49, 'lon_min': -125, 'lon_max': -115},\n",
        "            'Mountain_West': {'lat_min': 32, 'lat_max': 49, 'lon_min': -115, 'lon_max': -105},\n",
        "            'Great_Plains': {'lat_min': 32, 'lat_max': 49, 'lon_min': -105, 'lon_max': -95},\n",
        "            'South_Texas': {'lat_min': 25, 'lat_max': 32, 'lon_min': -105, 'lon_max': -95},\n",
        "            'South_Central': {'lat_min': 25, 'lat_max': 37, 'lon_min': -95, 'lon_max': -85},\n",
        "            'Midwest': {'lat_min': 37, 'lat_max': 49, 'lon_min': -95, 'lon_max': -85},\n",
        "            'Southeast': {'lat_min': 25, 'lat_max': 37, 'lon_min': -85, 'lon_max': -75},\n",
        "            'Northeast': {'lat_min': 37, 'lat_max': 49, 'lon_min': -85, 'lon_max': -67},\n",
        "            'Florida': {'lat_min': 25, 'lat_max': 32, 'lon_min': -85, 'lon_max': -80}\n",
        "        }\n",
        "        return regions\n",
        "\n",
        "    def get_region_parameters(self, region_name):\n",
        "        \"\"\"Get optimized parameters for each region based on fire patterns\"\"\"\n",
        "        region_params = {\n",
        "            'Pacific_West': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Mountain_West': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Great_Plains': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'South_Texas': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'South_Central': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'Midwest': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'Southeast': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5},\n",
        "            'Northeast': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Florida': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5}\n",
        "        }\n",
        "        return region_params.get(region_name, {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5})\n",
        "\n",
        "    def process_geographic_region(self, year, region_name, region_bounds):\n",
        "        \"\"\"Process fire events for a specific geographic region with region-optimized parameters\"\"\"\n",
        "\n",
        "        params = self.get_region_parameters(region_name)\n",
        "        spatial_eps = params['spatial_eps']\n",
        "        temporal_days = params['temporal_days']\n",
        "        min_samples = params['min_samples']\n",
        "\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        print(f\"\\nProcessing region: {region_name}\")\n",
        "        print(f\"   Bounds: {region_bounds}\")\n",
        "        print(f\"   Parameters: eps={spatial_eps}° (~{spatial_eps*111:.1f}km), temporal={temporal_days}d, min_samples={min_samples}\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id, year, doy, longitude, latitude, fire_date,\n",
        "            grid10k, covertype, fuelcode, area_burned,\n",
        "            consumed_fuel, ECO2, burn_source, burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        AND latitude >= {region_bounds['lat_min']}\n",
        "        AND latitude < {region_bounds['lat_max']}\n",
        "        AND longitude >= {region_bounds['lon_min']}\n",
        "        AND longitude < {region_bounds['lon_max']}\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"   No data found for {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"   Found {len(df):,} records\")\n",
        "\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        clustered_df = self._cluster_region_data(df, spatial_eps, temporal_days, min_samples)\n",
        "\n",
        "        if len(clustered_df) > 0:\n",
        "            clustered_df['region'] = region_name\n",
        "            unique_events = len(clustered_df['fire_event_id'].unique())\n",
        "            print(f\"   Found {unique_events} fire events in {region_name}\")\n",
        "            return clustered_df\n",
        "        else:\n",
        "            print(f\"   No fire events found in {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _cluster_region_data(self, df, spatial_eps, temporal_days, min_samples):\n",
        "        \"\"\"Apply DBSCAN clustering to data from a single geographic region\"\"\"\n",
        "\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        dbscan = DBSCAN(eps=spatial_eps, min_samples=min_samples, metric='haversine')\n",
        "        spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "        df['spatial_cluster'] = spatial_labels\n",
        "\n",
        "        fire_events = []\n",
        "        event_id = 0\n",
        "\n",
        "        for spatial_cluster in df['spatial_cluster'].unique():\n",
        "            if spatial_cluster == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_data = df[df['spatial_cluster'] == spatial_cluster].copy()\n",
        "            cluster_data = cluster_data.sort_values('fire_date')\n",
        "\n",
        "            cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(cluster_data)):\n",
        "                prev_date = cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > temporal_days:\n",
        "                    current_group += 1\n",
        "\n",
        "                cluster_data.iloc[i, cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            for temp_group in cluster_data['temp_group'].unique():\n",
        "                group_data = cluster_data[cluster_data['temp_group'] == temp_group].copy()\n",
        "                group_data['fire_event_id'] = event_id\n",
        "                fire_events.append(group_data)\n",
        "                event_id += 1\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            result_df = result_df.drop(['spatial_cluster', 'temp_group'], axis=1)\n",
        "            return result_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_year_geographic(self, year):\n",
        "        \"\"\"Process entire year using geographic chunking approach with region-specific parameters\"\"\"\n",
        "\n",
        "        print(f\"GEOGRAPHIC FIRE CLUSTERING for {year}\")\n",
        "        print(\"Using region-specific optimized parameters:\")\n",
        "\n",
        "        regions = self.define_geographic_regions()\n",
        "\n",
        "        for region_name in regions.keys():\n",
        "            params = self.get_region_parameters(region_name)\n",
        "            print(f\"  {region_name}: eps={params['spatial_eps']}° (~{params['spatial_eps']*111:.1f}km), temporal={params['temporal_days']}d\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        total_query = f\"\"\"\n",
        "        SELECT COUNT(*) as total_count\n",
        "        FROM `{self.project_id}.{self.dataset_id}.emission_{year}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        total_result = self.client.query(total_query).to_dataframe()\n",
        "        original_count = total_result['total_count'].iloc[0]\n",
        "\n",
        "        all_regional_events = []\n",
        "        global_event_id = 0\n",
        "\n",
        "        for region_name, region_bounds in regions.items():\n",
        "            regional_events = self.process_geographic_region(\n",
        "                year, region_name, region_bounds\n",
        "            )\n",
        "\n",
        "            if len(regional_events) > 0:\n",
        "                max_regional_id = regional_events['fire_event_id'].max()\n",
        "                regional_events['fire_event_id'] += global_event_id\n",
        "                global_event_id += max_regional_id + 1\n",
        "\n",
        "                all_regional_events.append(regional_events)\n",
        "\n",
        "        if all_regional_events:\n",
        "            combined_events = pd.concat(all_regional_events, ignore_index=True)\n",
        "\n",
        "            print(f\"\\nFINAL RESULTS:\")\n",
        "            print(f\"Total fire events: {len(combined_events['fire_event_id'].unique())}\")\n",
        "            print(f\"Total data points: {len(combined_events):,}\")\n",
        "            print(f\"Original data points: {original_count:,}\")\n",
        "            print(f\"Coverage: {len(combined_events)/original_count*100:.1f}% of original data\")\n",
        "\n",
        "            return combined_events\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def analyze_fire_events(self, fire_events_df):\n",
        "        \"\"\"Analyze the identified fire events\"\"\"\n",
        "        if len(fire_events_df) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        event_stats = fire_events_df.groupby('fire_event_id').agg({\n",
        "            'id': 'count',\n",
        "            'fire_date': ['min', 'max'],\n",
        "            'longitude': ['min', 'max'],\n",
        "            'latitude': ['min', 'max'],\n",
        "            'area_burned': 'sum',\n",
        "            'consumed_fuel': 'sum',\n",
        "            'ECO2': 'sum',\n",
        "            'region': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        event_stats.columns = ['fire_event_id', 'num_points', 'start_date', 'end_date',\n",
        "                              'min_lon', 'max_lon', 'min_lat', 'max_lat',\n",
        "                              'total_area_burned', 'total_consumed_fuel', 'total_ECO2', 'region']\n",
        "\n",
        "        event_stats['duration_days'] = (event_stats['end_date'] - event_stats['start_date']).dt.days + 1\n",
        "        event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "            lambda row: self.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                              row['max_lat'], row['max_lon']), axis=1\n",
        "        )\n",
        "\n",
        "        return event_stats\n",
        "\n",
        "    def save_results_to_csv(self, fire_events_df, filename):\n",
        "        \"\"\"Save results to CSV file\"\"\"\n",
        "        fire_events_df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    print(\"GEOGRAPHIC FIRE CLUSTERING)\n",
        "    print(\"This approach processes data by geographic regions to prevent\")\n",
        "    print(\"continent-spanning 'mega-fires' caused by temporal chunking.\")\n",
        "    print()\n",
        "\n",
        "    clustering = GeographicFireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    year = 2004\n",
        "    fire_events = clustering.process_year_geographic(year=year)\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        stats = clustering.analyze_fire_events(fire_events)\n",
        "\n",
        "        print(f\"\\nGEOGRAPHIC CLUSTERING RESULTS:\")\n",
        "        print(f\"Fire events found: {len(stats)}\")\n",
        "        print(f\"Average duration: {stats['duration_days'].mean():.1f} days\")\n",
        "        print(f\"Average spatial extent: {stats['spatial_extent_km'].mean():.1f} km\")\n",
        "        print(f\"Max spatial extent: {stats['spatial_extent_km'].max():.1f} km\")\n",
        "\n",
        "        large_events = stats[stats['spatial_extent_km'] > 500]\n",
        "        very_large_events = stats[stats['spatial_extent_km'] > 1000]\n",
        "\n",
        "        print(f\"\\nQUALITY CHECK:\")\n",
        "        print(f\"Events >500km: {len(large_events)} (target: <5)\")\n",
        "        print(f\"Events >1000km: {len(very_large_events)} (target: 0)\")\n",
        "\n",
        "        if len(large_events) > 0:\n",
        "            print(f\"\\nRemaining large events by region:\")\n",
        "            large_by_region = large_events.groupby('region')['fire_event_id'].count()\n",
        "            print(large_by_region)\n",
        "\n",
        "        clustering.save_results_to_csv(fire_events, f\"fire_events_{year}_final.csv\")\n",
        "        clustering.save_results_to_csv(stats, f\"fire_event_stats_{year}_final.csv\")\n",
        "\n",
        "        print(f\"\\nResults saved:\")\n",
        "        print(f\"- fire_events_{year}_final.csv\")\n",
        "        print(f\"- fire_event_stats_{year}_final.csv\")\n",
        "\n",
        "        print(f\"\\nResults by region:\")\n",
        "        region_summary = stats.groupby('region').agg({\n",
        "            'fire_event_id': 'count',\n",
        "            'spatial_extent_km': 'mean',\n",
        "            'duration_days': 'mean'\n",
        "        }).round(1)\n",
        "        print(region_summary)\n",
        "\n",
        "    else:\n",
        "        print(\"No fire events found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DvJ_d3ILcHWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from google.cloud import bigquery\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GeographicFireEventClustering:\n",
        "    def __init__(self, project_id, dataset_id):\n",
        "        self.client = bigquery.Client(project=project_id)\n",
        "        self.dataset_id = dataset_id\n",
        "        self.project_id = project_id\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calculate haversine distance between two points in kilometers\"\"\"\n",
        "        R = 6371\n",
        "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        return R * c\n",
        "\n",
        "    def define_geographic_regions(self):\n",
        "        \"\"\"Define strictly non-overlapping geographic regions with clear boundaries\"\"\"\n",
        "        regions = {\n",
        "            'Pacific_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -125.0, 'lon_max': -115.0},\n",
        "            'Mountain_West': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -115.0, 'lon_max': -105.0},\n",
        "            'Great_Plains': {'lat_min': 32.0, 'lat_max': 49.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Texas': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -105.0, 'lon_max': -95.0},\n",
        "            'South_Central': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Midwest': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -95.0, 'lon_max': -85.0},\n",
        "            'Southeast': {'lat_min': 25.0, 'lat_max': 37.0, 'lon_min': -85.0, 'lon_max': -75.0},\n",
        "            'Northeast': {'lat_min': 37.0, 'lat_max': 49.0, 'lon_min': -85.0, 'lon_max': -67.0},\n",
        "            'Florida': {'lat_min': 25.0, 'lat_max': 32.0, 'lon_min': -85.0, 'lon_max': -80.0}\n",
        "        }\n",
        "        return regions\n",
        "\n",
        "    def get_region_parameters(self, region_name):\n",
        "        \"\"\"Get optimized parameters for each region based on fire patterns\"\"\"\n",
        "        region_params = {\n",
        "            'Pacific_West': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Mountain_West': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Great_Plains': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'South_Texas': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'South_Central': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5},  # tighter\n",
        "            'Midwest': {'spatial_eps': 0.003, 'temporal_days': 2, 'min_samples': 5},\n",
        "            'Southeast': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5},\n",
        "            'Northeast': {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5},\n",
        "            'Florida': {'spatial_eps': 0.002, 'temporal_days': 1, 'min_samples': 5}\n",
        "        }\n",
        "        return region_params.get(region_name, {'spatial_eps': 0.005, 'temporal_days': 3, 'min_samples': 5})\n",
        "\n",
        "    def process_geographic_region(self, year, region_name, region_bounds):\n",
        "        \"\"\"Process fire events for a specific geographic region with region-optimized parameters\"\"\"\n",
        "\n",
        "        params = self.get_region_parameters(region_name)\n",
        "        spatial_eps = params['spatial_eps']\n",
        "        temporal_days = params['temporal_days']\n",
        "        min_samples = params['min_samples']\n",
        "\n",
        "        table_name = f\"emission_{year}\"\n",
        "\n",
        "        print(f\"\\nProcessing region: {region_name}\")\n",
        "        print(f\"   Bounds: {region_bounds}\")\n",
        "        print(f\"   Parameters: eps={spatial_eps}° (~{spatial_eps*111:.1f}km), temporal={temporal_days}d, min_samples={min_samples}\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            id, year, doy, longitude, latitude, fire_date,\n",
        "            grid10k, covertype, fuelcode, area_burned,\n",
        "            consumed_fuel, ECO2, burn_source, burnday_source\n",
        "        FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        AND latitude >= {region_bounds['lat_min']}\n",
        "        AND latitude < {region_bounds['lat_max']}\n",
        "        AND longitude >= {region_bounds['lon_min']}\n",
        "        AND longitude < {region_bounds['lon_max']}\n",
        "        ORDER BY fire_date, longitude, latitude\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.client.query(query).to_dataframe()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(f\"   No data found for {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"   Found {len(df):,} records\")\n",
        "\n",
        "        df['fire_date'] = pd.to_datetime(df['fire_date'])\n",
        "\n",
        "        clustered_df = self._cluster_region_data(df, spatial_eps, temporal_days, min_samples)\n",
        "\n",
        "        if len(clustered_df) > 0:\n",
        "            clustered_df['region'] = region_name\n",
        "            unique_events = len(clustered_df['fire_event_id'].unique())\n",
        "            print(f\"   Found {unique_events} fire events in {region_name}\")\n",
        "            return clustered_df\n",
        "        else:\n",
        "            print(f\"   No fire events found in {region_name}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _cluster_region_data(self, df, spatial_eps, temporal_days, min_samples):\n",
        "        \"\"\"Apply DBSCAN clustering to data from a single geographic region\"\"\"\n",
        "\n",
        "        coords = df[['longitude', 'latitude']].values\n",
        "        coords_rad = np.radians(coords)\n",
        "\n",
        "        dbscan = DBSCAN(eps=spatial_eps, min_samples=min_samples, metric='haversine')\n",
        "        spatial_labels = dbscan.fit_predict(coords_rad)\n",
        "        df['spatial_cluster'] = spatial_labels\n",
        "\n",
        "        fire_events = []\n",
        "        event_id = 0\n",
        "\n",
        "        for spatial_cluster in df['spatial_cluster'].unique():\n",
        "            if spatial_cluster == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_data = df[df['spatial_cluster'] == spatial_cluster].copy()\n",
        "            cluster_data = cluster_data.sort_values('fire_date')\n",
        "\n",
        "            cluster_data['temp_group'] = 0\n",
        "            current_group = 0\n",
        "\n",
        "            for i in range(1, len(cluster_data)):\n",
        "                prev_date = cluster_data.iloc[i-1]['fire_date']\n",
        "                curr_date = cluster_data.iloc[i]['fire_date']\n",
        "\n",
        "                if (curr_date - prev_date).days > temporal_days:\n",
        "                    current_group += 1\n",
        "\n",
        "                cluster_data.iloc[i, cluster_data.columns.get_loc('temp_group')] = current_group\n",
        "\n",
        "            for temp_group in cluster_data['temp_group'].unique():\n",
        "                group_data = cluster_data[cluster_data['temp_group'] == temp_group].copy()\n",
        "                group_data['fire_event_id'] = event_id\n",
        "                fire_events.append(group_data)\n",
        "                event_id += 1\n",
        "\n",
        "        if fire_events:\n",
        "            result_df = pd.concat(fire_events, ignore_index=True)\n",
        "            result_df = result_df.drop(['spatial_cluster', 'temp_group'], axis=1)\n",
        "            return result_df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def process_year_geographic(self, year):\n",
        "        \"\"\"Process entire year using geographic chunking approach with region-specific parameters\"\"\"\n",
        "\n",
        "        print(f\"GEOGRAPHIC FIRE CLUSTERING for {year}\")\n",
        "        print(\"Using region-specific optimized parameters:\")\n",
        "\n",
        "        regions = self.define_geographic_regions()\n",
        "\n",
        "        for region_name in regions.keys():\n",
        "            params = self.get_region_parameters(region_name)\n",
        "            print(f\"  {region_name}: eps={params['spatial_eps']}° (~{params['spatial_eps']*111:.1f}km), temporal={params['temporal_days']}d\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        total_query = f\"\"\"\n",
        "        SELECT COUNT(*) as total_count\n",
        "        FROM `{self.project_id}.{self.dataset_id}.emission_{year}`\n",
        "        WHERE longitude IS NOT NULL\n",
        "        AND latitude IS NOT NULL\n",
        "        AND fire_date IS NOT NULL\n",
        "        \"\"\"\n",
        "\n",
        "        total_result = self.client.query(total_query).to_dataframe()\n",
        "        original_count = total_result['total_count'].iloc[0]\n",
        "\n",
        "        all_regional_events = []\n",
        "        global_event_id = 0\n",
        "\n",
        "        for region_name, region_bounds in regions.items():\n",
        "            regional_events = self.process_geographic_region(\n",
        "                year, region_name, region_bounds\n",
        "            )\n",
        "\n",
        "            if len(regional_events) > 0:\n",
        "                max_regional_id = regional_events['fire_event_id'].max()\n",
        "                regional_events['fire_event_id'] += global_event_id\n",
        "                global_event_id += max_regional_id + 1\n",
        "\n",
        "                all_regional_events.append(regional_events)\n",
        "\n",
        "        if all_regional_events:\n",
        "            combined_events = pd.concat(all_regional_events, ignore_index=True)\n",
        "\n",
        "            initial_count = len(combined_events)\n",
        "            combined_events = combined_events.drop_duplicates(subset=['id'], keep='first')\n",
        "            final_count = len(combined_events)\n",
        "\n",
        "            if initial_count != final_count:\n",
        "                print(f\"WARNING: Removed {initial_count - final_count} duplicate records\")\n",
        "\n",
        "            print(f\"\\nFINAL RESULTS:\")\n",
        "            print(f\"Total fire events: {len(combined_events['fire_event_id'].unique())}\")\n",
        "            print(f\"Total data points: {len(combined_events):,}\")\n",
        "            print(f\"Original data points: {original_count:,}\")\n",
        "            print(f\"Coverage: {len(combined_events)/original_count*100:.1f}% of original data\")\n",
        "\n",
        "            if len(combined_events) > original_count:\n",
        "                print(f\"WARNING: More data points than original - investigating...\")\n",
        "\n",
        "                print(\"Regional data point totals:\")\n",
        "                for events in all_regional_events:\n",
        "                    region = events['region'].iloc[0]\n",
        "                    print(f\"  {region}: {len(events):,} points\")\n",
        "\n",
        "            return combined_events\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def analyze_fire_events(self, fire_events_df):\n",
        "        \"\"\"Analyze the identified fire events\"\"\"\n",
        "        if len(fire_events_df) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        event_stats = fire_events_df.groupby('fire_event_id').agg({\n",
        "            'id': 'count',\n",
        "            'fire_date': ['min', 'max'],\n",
        "            'longitude': ['min', 'max'],\n",
        "            'latitude': ['min', 'max'],\n",
        "            'area_burned': 'sum',\n",
        "            'consumed_fuel': 'sum',\n",
        "            'ECO2': 'sum',\n",
        "            'region': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        event_stats.columns = ['fire_event_id', 'num_points', 'start_date', 'end_date',\n",
        "                              'min_lon', 'max_lon', 'min_lat', 'max_lat',\n",
        "                              'total_area_burned', 'total_consumed_fuel', 'total_ECO2', 'region']\n",
        "\n",
        "        event_stats['duration_days'] = (event_stats['end_date'] - event_stats['start_date']).dt.days + 1\n",
        "        event_stats['spatial_extent_km'] = event_stats.apply(\n",
        "            lambda row: self.haversine_distance(row['min_lat'], row['min_lon'],\n",
        "                                              row['max_lat'], row['max_lon']), axis=1\n",
        "        )\n",
        "\n",
        "        return event_stats\n",
        "\n",
        "    def save_results_to_csv(self, fire_events_df, filename):\n",
        "        \"\"\"Save results to CSV file\"\"\"\n",
        "        fire_events_df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"GEOGRAPHIC FIRE CLUSTERING)\n",
        "    print(\"This approach processes data by geographic regions to prevent\")\n",
        "    print(\"continent-spanning 'mega-fires' caused by temporal chunking.\")\n",
        "    print()\n",
        "\n",
        "    clustering = GeographicFireEventClustering(\n",
        "        project_id=\"code-for-planet\",\n",
        "        dataset_id=\"emission_db\"\n",
        "    )\n",
        "\n",
        "    year = 2004\n",
        "    fire_events = clustering.process_year_geographic(year=year)\n",
        "\n",
        "    if len(fire_events) > 0:\n",
        "        stats = clustering.analyze_fire_events(fire_events)\n",
        "\n",
        "        print(f\"\\nGEOGRAPHIC CLUSTERING RESULTS:\")\n",
        "        print(f\"Fire events found: {len(stats)}\")\n",
        "        print(f\"Average duration: {stats['duration_days'].mean():.1f} days\")\n",
        "        print(f\"Average spatial extent: {stats['spatial_extent_km'].mean():.1f} km\")\n",
        "        print(f\"Max spatial extent: {stats['spatial_extent_km'].max():.1f} km\")\n",
        "\n",
        "        large_events = stats[stats['spatial_extent_km'] > 500]\n",
        "        very_large_events = stats[stats['spatial_extent_km'] > 1000]\n",
        "\n",
        "        print(f\"\\nQUALITY CHECK:\")\n",
        "        print(f\"Events >500km: {len(large_events)} (target: <5)\")\n",
        "        print(f\"Events >1000km: {len(very_large_events)} (target: 0)\")\n",
        "\n",
        "        if len(large_events) > 0:\n",
        "            print(f\"\\nRemaining large events by region:\")\n",
        "            large_by_region = large_events.groupby('region')['fire_event_id'].count()\n",
        "            print(large_by_region)\n",
        "\n",
        "        clustering.save_results_to_csv(fire_events, f\"fire_events_{year}_final.csv\")\n",
        "        clustering.save_results_to_csv(stats, f\"fire_event_stats_{year}_final.csv\")\n",
        "\n",
        "        print(f\"\\nResults saved:\")\n",
        "        print(f\"- fire_events_{year}_final.csv\")\n",
        "        print(f\"- fire_event_stats_{year}_final.csv\")\n",
        "\n",
        "        print(f\"\\nResults by region:\")\n",
        "        region_summary = stats.groupby('region').agg({\n",
        "            'fire_event_id': 'count',\n",
        "            'spatial_extent_km': 'mean',\n",
        "            'duration_days': 'mean'\n",
        "        }).round(1)\n",
        "        print(region_summary)\n",
        "\n",
        "    else:\n",
        "        print(\"No fire events found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Wu6k2Vj0dYYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}